{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L3QUcVx4z4Yd"
   },
   "source": [
    "# Introduction\n",
    "This notebook will test the performance of a 2-layer MLP (multi-layer perceptron) in performing a character-wise prediction task on \n",
    "* MNIST\n",
    "* the play *Hamlet*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "l5jJ291f9H2t",
    "outputId": "b1dcaf32-0433-4422-99a1-310a6a475487"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from tqdm import tnrange\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "%matplotlib inline  \n",
    "\n",
    "# alphabet\n",
    "import string\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xuaNMQ011CNj"
   },
   "source": [
    "# MLP code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hN9Ss_xO20Eo"
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "smKbyWrLz2NB"
   },
   "outputs": [],
   "source": [
    "def get_accuracy(logit, target):\n",
    "    batch_size = len(target)\n",
    "    corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
    "    accuracy = 100.0 * corrects/batch_size\n",
    "    return accuracy.item()\n",
    "\n",
    "def nparam(ninputs,nhidden,noutputs):\n",
    "    return ninputs*(nhidden+1) + nhidden*(nhidden+1)+nhidden*(noutputs+1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4G-YeFOa22ap"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "myxC_JCB24BI"
   },
   "outputs": [],
   "source": [
    "# a prototype MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden_neurons, n_output,  device):\n",
    "        super(MLP, self).__init__()\n",
    "        self.n_inputs = n_inputs # set the number of neurons in the input layer\n",
    "        self.n_hidden_neurons = n_hidden_neurons # how many neurons are in each hidden layer\n",
    "        #self.n_hidden_layers = n_hidden_layers # how many hidden layers are we using in our model\n",
    "        self.n_output = n_output # set the number of neurons in the output layer\n",
    "        self.dt = nn.Parameter(torch.Tensor([1])) # set the change in parameter update\n",
    "        self.a = nn.Parameter(torch.Tensor([1])) # set the bias\n",
    "        self.sig = nn.Sigmoid() # set the activation function \n",
    "        self.n_hidden = n_hidden_neurons\n",
    "        self.decoder = nn.Linear(n_hidden_neurons, n_output) # decode output\n",
    "        self.recurrent = nn.Linear(n_hidden_neurons,n_hidden_neurons)\n",
    "        self.encoder = nn.Linear(n_inputs, n_hidden_neurons) # encode input\n",
    "        \n",
    "    # the main operation for the MLP is to update each hidden layer with the state of the previous hidden layer\n",
    "    # so, if you need to update the hidden layers, make sure you update each layer with state of the previous layer\n",
    "    \n",
    "#     def update_hidden_layer(self):\n",
    "#         # update the neurons in the current hidden layer with the state of the inputted \"previous\" layer\n",
    "#         for i in range(1,self.n_hidden_layers):\n",
    "#             # update each node in the h1_current\n",
    "#             self.h1 = self.hidden\n",
    "# #             self.hidden = nn.Linear(self.h1,self.hidden) # update the stored hidden layer state as many times as there are hidden layers specified # BUG\n",
    "#             # the nn.Linear should take in the shape...\n",
    "#             self.hidden = nn.Linear(self.n_hidden_neurons,self.n_hidden_neurons) \n",
    "#         self.h1 = self.hidden\n",
    "#         return(self.h1)\n",
    "    \n",
    "    def forward(self, x0):\n",
    "        #x0=x0.permute(1,0,2) # permute the tensor\n",
    "        # save the hidden layers as a tensor\n",
    "        \n",
    "        # initialize self.h1\n",
    "        self.hidden1 = self.sig(self.encoder(x0))#torch.zeros(self.n_hidden_layers,BATCH_SIZE,self.n_hidden).to(device) # initialize the hidden layer\n",
    "        self.hidden2 = self.sig(self.recurrent(self.hidden1))\n",
    "        self.output = self.decoder(self.hidden2)\n",
    "        \n",
    "        \n",
    "#         # set the hidden layer value\n",
    "#         for i in range(x0.size(0)):\n",
    "#             self.hidden[0,:,:] = self.sig(self.dt)*self.a*torch.tanh(self.encoder(x0[i,:,:]))\n",
    "        \n",
    "#         self.hidden = torch.zeros(self.n_hidden_layers,BATCH_SIZE,self.n_hidden).to(device) # initialize the hidden layer\n",
    "        # the first layer will consist of the encoded inputs\n",
    "#         for i in range(x0.size(0)):\n",
    "#             self.hlayers[0,:,:] = self.sig(self.dt)*self.a*torch.tanh(self.encoder(x0[i,:,:])) # (1-self.sig(self.dt))*self.h1+self.sig(self.dt)*self.a*torch.tanh(self.encoder(x0[i,:,:])+self.recurrent(self.h1))\n",
    "#         # update the additional hidden layers\n",
    "        #self.hidden = self.update_hidden_layer()\n",
    "\n",
    "        #self.y1 = self.decoder(self.n_hidden_neurons,self.n_output)\n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pLXkw4RVq4jw"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "id": "lw_mPYUqEcFE",
    "outputId": "5b1af598-65f2-42b9-8e1d-82c251bc6d66"
   },
   "outputs": [],
   "source": [
    "# MNIST\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# list all transformations\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "# download and load training dataset\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "# download and load testing dataset\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tS0JlvNs9HFA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | Loss: 286.1397 | Train Accuracy: 85.11\n",
      "Epoch:  1 | Loss: 83.2102 | Train Accuracy: 96.04\n",
      "Epoch:  2 | Loss: 57.3114 | Train Accuracy: 97.32\n",
      "Epoch:  3 | Loss: 43.6119 | Train Accuracy: 97.97\n",
      "Epoch:  4 | Loss: 34.4946 | Train Accuracy: 98.40\n",
      "Epoch:  5 | Loss: 27.4446 | Train Accuracy: 98.76\n",
      "Epoch:  6 | Loss: 22.0844 | Train Accuracy: 99.01\n",
      "Epoch:  7 | Loss: 16.5204 | Train Accuracy: 99.36\n",
      "Epoch:  8 | Loss: 13.3614 | Train Accuracy: 99.59\n",
      "Epoch:  9 | Loss: 11.0588 | Train Accuracy: 99.66\n"
     ]
    }
   ],
   "source": [
    "# training on MNIST\n",
    "# parameters \n",
    "N_INPUTS = 784\n",
    "N_HIDDEN_NEURONS = 100\n",
    "N_OUTPUTS = 10\n",
    "N_EPHOCS = 10\n",
    "\n",
    "model = MLP(N_INPUTS,N_HIDDEN_NEURONS,N_OUTPUTS,device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_running_loss = np.zeros(N_EPHOCS)\n",
    "train_acc = np.zeros(N_EPHOCS)\n",
    "nparams = 0\n",
    "for param in model.parameters(): \n",
    "    if param.requires_grad:\n",
    "        nparams += param.data.numpy().size\n",
    "Phist = np.zeros((nparams,N_EPHOCS))\n",
    "\n",
    "for epoch in range(N_EPHOCS):\n",
    "    nps = 0\n",
    "    running_loss=0\n",
    "    running_acc=0\n",
    "    for p,param in enumerate(model.parameters()):\n",
    "        if param.requires_grad:\n",
    "            plist = param.data.numpy().flatten()\n",
    "            for j in range(plist.size):\n",
    "                Phist[nps,epoch]=plist[j]\n",
    "                nps+=1\n",
    "  \n",
    "    for i, data in enumerate(trainloader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        x, y_tar = data\n",
    "        y_pred = model(x.view(BATCH_SIZE,N_INPUTS))\n",
    "        loss = criterion(y_pred.view(BATCH_SIZE,N_OUTPUTS),y_tar)\n",
    "                \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss+=loss.item()\n",
    "        running_acc+=get_accuracy(y_pred.view(BATCH_SIZE,N_OUTPUTS), y_tar)\n",
    "    train_running_loss[epoch] = running_loss\n",
    "    train_acc[epoch] = running_acc/i\n",
    "    \n",
    "    print('Epoch:  %d | Loss: %.4f | Train Accuracy: %.2f' %(epoch, train_running_loss[epoch], train_acc[epoch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 6, 4, 1, 3, 3, 2, 9, 9, 0, 1, 4, 3, 9, 5, 8, 9, 8, 2, 5, 3, 4, 6, 6,\n",
       "        7, 6, 7, 4, 4, 5, 5, 9, 0, 6, 7, 1, 2, 9, 5, 7, 4, 1, 4, 2, 4, 8, 6, 8,\n",
       "        7, 7, 7, 2, 9, 0, 0, 4, 8, 4, 9, 9, 2, 7, 6, 4, 3, 2, 8, 1, 3, 7, 9, 0,\n",
       "        1, 3, 0, 9, 4, 8, 8, 6, 8, 8, 3, 3, 8, 6, 1, 7, 8, 3, 8, 3, 0, 5, 0, 6,\n",
       "        8, 3, 6, 5])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(y_pred,1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 9, 5, 8, 0, 8, 4, 8, 6, 1, 5, 6, 8, 1, 2, 3, 0, 1, 9, 4, 3, 0, 4, 5,\n",
       "        3, 4, 1, 1, 8, 1, 8, 3, 3, 3, 2, 0, 2, 0, 5, 9, 4, 8, 3, 0, 5, 9, 8, 9,\n",
       "        2, 4, 4, 6, 7, 2, 5, 3, 4, 3, 1, 6, 3, 3, 4, 3, 4, 9, 2, 0, 8, 2, 8, 6,\n",
       "        4, 8, 5, 5, 6, 0, 0, 1, 8, 1, 7, 6, 9, 8, 6, 6, 6, 6, 6, 3, 5, 5, 1, 9,\n",
       "        6, 1, 7, 2])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters \n",
    "N_INPUTS = 784\n",
    "N_HIDDEN_NEURONS = 100\n",
    "N_OUTPUTS = 10\n",
    "N_EPHOCS = 10\n",
    "N_REPS = 15\n",
    "N_PARAMS = nparam(N_INPUTS,N_HIDDEN_NEURONS,N_OUTPUTS)\n",
    "\n",
    "# regularization parameters\n",
    "lambdas = np.arange(0,5.5,0.5,dtype=np.float)\n",
    "N_LAMBDA = len(lambdas)\n",
    "gidx = int(N_HIDDEN_NEURONS/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fef7d7b229a4c0caba36a643005080b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=15), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ffd6a44cab94404a1fc15a1f7c4983b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb8dc8a39db4c5eba84d82de9f9bd1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# regularizing digonal blocks of the partitioned RNN\n",
    "train_loss_P = np.zeros((N_EPHOCS,N_LAMBDA,N_REPS))\n",
    "train_acc_P = np.zeros((N_EPHOCS,N_LAMBDA,N_REPS))\n",
    "test_loss_P = np.zeros((N_EPHOCS,N_LAMBDA,N_REPS))\n",
    "test_acc_P = np.zeros((N_EPHOCS,N_LAMBDA,N_REPS))\n",
    "Phist_P = np.zeros((N_PARAMS,N_EPHOCS,N_LAMBDA,N_REPS))\n",
    "\n",
    "model_P = [None]*N_LAMBDA*N_REPS\n",
    "regval_P = []\n",
    "for r in tnrange(N_REPS):\n",
    "    for k in tnrange(N_LAMBDA):\n",
    "        reg_lambda = lambdas[k]\n",
    "        # model_path = 'D:\\chongguo\\git\\AM221pset\\Project\\Final Project\\History\\Lamb\\model_P_rep_{}_lambda_{:d}_10.pt'.format(r,int(reg_lambda*10))\n",
    "        model_path = '/Users/vinayakvsv/am221/AM221pset/Project/vv_sandbox/mlp_experiments/model_P_rep_{}_lambda_{:d}_10.pt'.format(r,int(reg_lambda*10))\n",
    "        # model_P[k+r*N_LAMBDA] = PRNN(N_INPUTS,N_HIDDEN,N_OUTPUTS,1,device).to(device)\n",
    "        model_P[k+r*N_LAMBDA] = MLP(N_INPUTS,N_HIDDEN_NEURONS,N_OUTPUTS,device)\n",
    "        l2_reg = torch.Tensor(1,device=device)\n",
    "        optimizer = torch.optim.SGD(model_P[k+r*N_LAMBDA].parameters(), lr=1e-2, momentum=0.9)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        for epoch in range(N_EPHOCS):\n",
    "            nps = 0\n",
    "            running_train_loss=0\n",
    "            running_train_acc=0\n",
    "            model_P[k+r*N_LAMBDA].train()\n",
    "            for p, param in enumerate(model_P[k+r*N_LAMBDA].parameters()):\n",
    "                if param.requires_grad:\n",
    "                    plist = torch.flatten(param.data)\n",
    "                    for j in range(plist.size(0)):\n",
    "                        Phist_P[nps,epoch,k,r]=plist[j].item()\n",
    "                        nps+=1\n",
    "\n",
    "            for i, (x, y_tar) in enumerate(trainloader):\n",
    "                l2_reg = 0\n",
    "                x, y_tar = x.to(device), y_tar.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = model_P[k+r*N_LAMBDA](x.view(BATCH_SIZE,N_INPUTS))\n",
    "                loss = criterion(y_pred.view(BATCH_SIZE,N_OUTPUTS),y_tar)\n",
    "                for p,param in enumerate(model_P[k+r*N_LAMBDA].parameters()):\n",
    "                    if param.requires_grad and len(param.shape)==2:\n",
    "                        if param.shape[0]==N_HIDDEN_NEURONS and param.shape[1]==N_HIDDEN_NEURONS:\n",
    "                            l2_reg = l2_reg + param[:gidx,:gidx].norm(p=1)\n",
    "                            l2_reg = l2_reg + param[gidx:,gidx:].norm(p=1)\n",
    "                        elif param.shape[1]==N_HIDDEN_NEURONS:\n",
    "                            l2_reg = l2_reg + param[:,gidx:].norm(p=1)\n",
    "                        elif param.shape[0]==N_HIDDEN_NEURONS:\n",
    "                            l2_reg = l2_reg + param[:gidx,:].norm(p=1)\n",
    "                regval_P.append(l2_reg.item())\n",
    "                loss = loss + l2_reg*reg_lambda/BATCH_SIZE\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_train_loss+=loss.item()\n",
    "                running_train_acc+=get_accuracy(y_pred.view(BATCH_SIZE,N_OUTPUTS), y_tar)\n",
    "            \n",
    "            running_test_acc=0\n",
    "            running_test_loss=0\n",
    "            model_P[k+r*N_LAMBDA].eval()\n",
    "            for i,(x_test, y_test_tar) in enumerate(testloader):\n",
    "                x_test, y_test_tar = x_test.to(device), y_test_tar.to(device)\n",
    "                y_test_pred = model_P[k+r*N_LAMBDA](x_test.view(BATCH_SIZE,N_INPUTS))\n",
    "                loss = criterion(y_test_pred.view(BATCH_SIZE,N_OUTPUTS),y_test_tar)\n",
    "                running_test_loss+=loss.item()\n",
    "                running_test_acc+=get_accuracy(y_test_pred.view(BATCH_SIZE,N_OUTPUTS), y_test_tar)\n",
    "                \n",
    "            train_loss_P[epoch,k,r] = running_train_loss/len(trainloader)\n",
    "            train_acc_P[epoch,k,r] = running_train_acc/len(trainloader)\n",
    "            test_loss_P[epoch,k,r] = running_test_loss/len(testloader)\n",
    "            test_acc_P[epoch,k,r] = running_test_acc/len(testloader)\n",
    "            \n",
    "        # save the model and free the memory  \n",
    "        torch.save(model_P[k+r*N_LAMBDA].state_dict(), model_path)\n",
    "        model_P[k+r*N_LAMBDA] = [None]\n",
    "        del(l2_reg,loss,optimizer,criterion,plist,param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MLP_hamlet",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
