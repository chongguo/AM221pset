{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aRpyAKGvvXlr"
   },
   "source": [
    "March 23, 2019\n",
    "\n",
    "Chong coded this RNN, which works. Today, we will be doing the following\n",
    "* Raising the learning rate\n",
    "  * Rate increased by using (1) LSTM,and (2) \n",
    "* Have the RNN complete a classification task with at least 30% accuracy\n",
    "* Speculate on a \"smarter\" RNN structure\n",
    "\n",
    "Hold-over from March 21, 2019\n",
    "* encode `tanh` activation functions and learn weights for the functions in the partitioned and unpartitioned models\n",
    "* Try out `dropout`, $L_2$, and `weight decay` regularization methods for the partitioned and unpartitioned models  \n",
    "\n",
    "Next steps\n",
    "* Permuted Sequential MNIST (scramble images and train the RNN one pixel at a time)\n",
    "* Regularization for the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xs3E-zMxw2Sx"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "KmeqVDIox5mU",
    "outputId": "c3d389c9-e430-4be2-bfb0-e3d4a4d15db0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9920512it [00:01, 6750653.32it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32768it [00:00, 308096.23it/s]                           \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1654784it [00:00, 5183923.58it/s]                           \n",
      "8192it [00:00, 126528.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# BATCH_SIZE = 128\n",
    "\n",
    "# # generate permutation control\n",
    "# # suggested from https://discuss.pytorch.org/t/permutate-mnist-help-needed/22901 \n",
    "# rng_permute = np.random.RandomState(92916) # set random number generator\n",
    "# idx_permute = torch.from_numpy(rng_permute.permutation(784)) # creates a tensor from a numpy array # create a random ordering of 28x28 = 784 pixels (i.e. generate a random image)\n",
    "# transform = transforms.Compose(\n",
    "#     [transforms.ToTensor(),\n",
    "#      transforms.Normalize((0.1307,), (0.3081,)),\n",
    "#      transforms.Lambda(lambda x: x.view(-1)[idx_permute].view(1, 28, 28) )]) # generate a transform such that any image put through the transformation will be \"scrambled\"\n",
    "\n",
    "\n",
    "# # download and load training dataset\n",
    "# trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "#                                         download=True, transform=transform)\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
    "#                                           shuffle=True, num_workers=2)\n",
    "\n",
    "# # download and load testing dataset\n",
    "# testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "#                                        download=True, transform=transform)\n",
    "# testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n",
    "#                                          shuffle=False, num_workers=2)\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# list all transformations\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "# download and load training dataset\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "# download and load testing dataset\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n",
    "                                         shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SA2-gSUI76_L"
   },
   "outputs": [],
   "source": [
    "# build the RNN\n",
    "# class PRNN(nn.Module):\n",
    "#     def __init__(self, n_inputs, n_hidden,n_output,mr):\n",
    "#         super(PRNN, self).__init__()\n",
    "#         self.n_inputs = n_inputs\n",
    "#         self.n_hidden = n_hidden\n",
    "#         self.n_output = n_output\n",
    "#         self.mr = mr\n",
    "#         self.encoder = nn.Linear(n_inputs,n_hidden)\n",
    "#         self.recurrent = nn.Linear(n_hidden,n_hidden)\n",
    "#         self.decoder = nn.Linear(n_hidden, n_output)\n",
    "#         # self.lstm = nn.LSTM(n_inputs, n_output, batch_first=True)\n",
    "        \n",
    "#     def forward(self, x0):\n",
    "#         T = int(x0.shape[2]/2)\n",
    "#         #self.h1 = Variable(torch.zeros(self.n_hidden))\n",
    "#         self.h1 = Variable(torch.zeros(x0.size(0), self.n_hidden))\n",
    "#         #for t in range(T):\n",
    "#            #self.h1 = self.mr*self.h1+(1-self.mr)*torch.relu(self.encoder(x0[:,t+7,:])+self.recurrent(self.h1))\n",
    "#            # self.h1,_ = self.lstm(x0[:,:,:],self.h1)\n",
    "#         self.y1 = self.decoder(self.h1)\n",
    "        \n",
    "#         return self.y1\n",
    "\n",
    "class PRNN(nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden,n_output,mr):\n",
    "        super(PRNN, self).__init__()\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        self.mr = mr\n",
    "        self.encoder = nn.Linear(n_inputs,n_hidden)\n",
    "        self.recurrent = nn.Linear(n_hidden,n_hidden)\n",
    "        self.decoder = nn.Linear(n_hidden, n_output)\n",
    "        self.rnn = nn.RNN(n_inputs, n_hidden)\n",
    "        \n",
    "    def forward(self, x0):\n",
    "        x0=x0.permute(1,0,2)\n",
    "        self.h1 = torch.zeros(1,BATCH_SIZE,self.n_hidden)\n",
    "        #self.h1 = Variable(torch.zeros(x0.size(0), self.n_hidden))\n",
    "        #for t in range(T):\n",
    "           #self.h1 = self.mr*self.h1+(1-self.mr)*torch.relu(self.encoder(x0[:,t+7,:])+self.recurrent(self.h1))\n",
    "        self.y0, self.h1 = self.rnn(x0,self.h1)\n",
    "        self.y1 = self.decoder(self.h1[0])\n",
    "        \n",
    "        return self.y1\n",
    "# can I modify this to set an arbitrary number of layers?\n",
    "        \n",
    "# RNN LSTM  \n",
    "class PRNN_LSTM(nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden,n_output,mr):\n",
    "        super(PRNN_LSTM, self).__init__()\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        self.mr = mr\n",
    "#         self.encoder = nn.Linear(n_inputs,n_hidden)\n",
    "#         self.recurrent = nn.Linear(n_hidden,n_hidden)\n",
    "        self.decoder = nn.Linear(n_hidden, n_output)\n",
    "        self.lstm = nn.LSTM(n_inputs, n_hidden)\n",
    "        \n",
    "    def forward(self, x0):\n",
    "        x0=x0.permute(1,0,2)\n",
    "        self.h1 = (torch.zeros(1,BATCH_SIZE,self.n_hidden),torch.zeros(1,BATCH_SIZE,self.n_hidden))\n",
    "        #self.h1 = Variable(torch.zeros(x0.size(0), self.n_hidden))\n",
    "        #for t in range(T):\n",
    "           #self.h1 = self.mr*self.h1+(1-self.mr)*torch.relu(self.encoder(x0[:,t+7,:])+self.recurrent(self.h1))\n",
    "        self.y0, self.h1 = self.lstm(x0,self.h1)\n",
    "        self.y1 = self.decoder(self.h1[0])\n",
    "        \n",
    "        return self.y1\n",
    "# can I modify this to set an arbitrary number of layers?\n",
    "\n",
    "# accuracy and one-hot encoding functions\n",
    "def onehotTensor(category,n_categories):\n",
    "    tensor = torch.zeros(1, n_categories,dtype=torch.long)\n",
    "    tensor[0][category] = 1\n",
    "    return tensor\n",
    "        \n",
    "def get_accuracy(logit, target, batch_size):\n",
    "#     corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data)#.sum()\n",
    "#     print(corrects)\n",
    "    corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
    "    accuracy = 100.0 * corrects/batch_size\n",
    "    return accuracy.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "D1WCgnwWRuM9",
    "outputId": "2365d997-45d3-4de7-dcab-ecbaf7e2e394"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | Loss: 347.8351 | Train Accuracy: 80.01\n",
      "Epoch:  1 | Loss: 66.4622 | Train Accuracy: 96.80\n",
      "Epoch:  2 | Loss: 45.5988 | Train Accuracy: 97.88\n",
      "Epoch:  3 | Loss: 34.3857 | Train Accuracy: 98.44\n",
      "Epoch:  4 | Loss: 28.5616 | Train Accuracy: 98.72\n",
      "Epoch:  5 | Loss: 24.1230 | Train Accuracy: 98.90\n",
      "Epoch:  6 | Loss: 21.4008 | Train Accuracy: 99.03\n",
      "Epoch:  7 | Loss: 17.8757 | Train Accuracy: 99.20\n",
      "Epoch:  8 | Loss: 18.0717 | Train Accuracy: 99.18\n",
      "Epoch:  9 | Loss: 15.9395 | Train Accuracy: 99.32\n"
     ]
    }
   ],
   "source": [
    "# run RNN-LSTM\n",
    "# parameters \n",
    "N_STEPS = 28\n",
    "N_INPUTS = 28\n",
    "N_HIDDEN = 100\n",
    "N_OUTPUTS = 10\n",
    "N_EPHOCS = 10\n",
    "\n",
    "model = PRNN_LSTM(N_INPUTS,N_HIDDEN,N_OUTPUTS,0.1)\n",
    "# model = MyModel()\n",
    "# model.cuda()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_running_loss = np.zeros(N_EPHOCS)\n",
    "train_acc = np.zeros(N_EPHOCS)\n",
    "nparams = 0\n",
    "for param in model.parameters(): \n",
    "  if param.requires_grad:\n",
    "    nparams += param.data.numpy().size\n",
    "Phist = np.zeros((nparams,N_EPHOCS))\n",
    "\n",
    "for epoch in range(N_EPHOCS):\n",
    "    nps = 0\n",
    "    running_loss=0\n",
    "    running_acc=0\n",
    "    for p,param in enumerate(model.parameters()):\n",
    "        if param.requires_grad:\n",
    "            plist = param.data.numpy().flatten()\n",
    "            for j in range(plist.size):\n",
    "                Phist[nps,epoch]=plist[j]\n",
    "                nps+=1\n",
    "  \n",
    "    for i, data in enumerate(trainloader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        x, y_tar = data\n",
    "        y_pred = model(x.view(BATCH_SIZE,N_STEPS,N_INPUTS))\n",
    "        loss = criterion(y_pred.view(BATCH_SIZE,N_OUTPUTS),y_tar)\n",
    "        \n",
    "        # print shapes\n",
    "#         print(\"shape of the label:\"),\n",
    "#         print(y_tar.shape)\n",
    "#         print(\"shape of the machine-generated label\"),\n",
    "#         print(y_pred.shape)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss+=loss.item()\n",
    "        running_acc+=get_accuracy(y_pred.view(BATCH_SIZE,N_OUTPUTS), y_tar, BATCH_SIZE)\n",
    "    train_running_loss[epoch] = running_loss\n",
    "    train_acc[epoch] = running_acc/i\n",
    "    \n",
    "    print('Epoch:  %d | Loss: %.4f | Train Accuracy: %.2f' %(epoch, train_running_loss[epoch], train_acc[epoch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Im58iGzjSJLt"
   },
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.plot(Phist[::1000,:].T)\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(train_running_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0HiajXKjRvqK"
   },
   "outputs": [],
   "source": [
    "# run Vanilla RNN\n",
    "# parameters \n",
    "N_STEPS = 28\n",
    "N_INPUTS = 28\n",
    "N_HIDDEN = 100\n",
    "N_OUTPUTS = 10\n",
    "N_EPHOCS = 10\n",
    "\n",
    "model = PRNN(N_INPUTS,N_HIDDEN,N_OUTPUTS,0.1)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_running_loss = np.zeros(N_EPHOCS)\n",
    "train_acc = np.zeros(N_EPHOCS)\n",
    "nparams = 0\n",
    "for param in model.parameters(): \n",
    "  if param.requires_grad:\n",
    "    nparams += param.data.numpy().size\n",
    "Phist = np.zeros((nparams,N_EPHOCS))\n",
    "\n",
    "for epoch in range(N_EPHOCS):\n",
    "    nps = 0\n",
    "    running_loss=0\n",
    "    running_acc=0\n",
    "    for p,param in enumerate(model.parameters()):\n",
    "        if param.requires_grad:\n",
    "            plist = param.data.numpy().flatten()\n",
    "            for j in range(plist.size):\n",
    "                Phist[nps,epoch]=plist[j]\n",
    "                nps+=1\n",
    "  \n",
    "    for i, data in enumerate(trainloader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        x, y_tar = data\n",
    "        y_pred = model(x.view(BATCH_SIZE,N_STEPS,N_INPUTS))\n",
    "        loss = criterion(y_pred.view(BATCH_SIZE,N_OUTPUTS),y_tar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss+=loss.item()\n",
    "        running_acc+=get_accuracy(y_pred.view(BATCH_SIZE,N_OUTPUTS), y_tar, BATCH_SIZE)\n",
    "    train_running_loss[epoch] = running_loss\n",
    "    train_acc[epoch] = running_acc/i\n",
    "    \n",
    "    print('Epoch:  %d | Loss: %.4f | Train Accuracy: %.2f' %(epoch, train_running_loss[epoch], train_acc[epoch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aws_G2K0Rjq4"
   },
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.plot(Phist[::1000,:].T)\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(train_running_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bFIiVS-pRl4k"
   },
   "outputs": [],
   "source": [
    "# # weights\n",
    "# l2_reg = Variable(torch.FloatTensor(1), requires_grad=True)\n",
    "# for W in mdl.parameters():\n",
    "#   l2_reg = l2_reg + W.norm(2)\n",
    "# batch_loss = (1/N_train)*(y_pred - batch_ys).pow(2).sum() + l2_reg * reg_lambda\n",
    "# batch_loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AIzv61MrYMTV"
   },
   "source": [
    "\n",
    "The trainloader consists of a series of images. We need to funnel each image one pixel at a time as opposed to one row at a time. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C_6FfFVnS7PD"
   },
   "outputs": [],
   "source": [
    "# Sparse Sequential MNIST -- feed in the dataset one pixel at a time and every other pixel. \n",
    "# Use many different neurons in the hidden layer as well, since the dynamic range of out\n",
    "\n",
    "# Round 1: feed in the sparser image row by row. \n",
    "# parameters \n",
    "N_STEPS_SPARSE = 14 # since we fed in every other entry, we now have a 14x14 matrix \n",
    "N_INPUTS = 14 #14 # since the model has to take in one value -- i.e. one pixel -- at a time # and subsequently, 1 input at a time\n",
    "N_HIDDEN = 400 # let's raise the number of hidden neurons\n",
    "N_OUTPUTS = 50 # 50 output values\n",
    "N_EPHOCS = 30\n",
    "\n",
    "model_1 = PRNN(N_INPUTS,N_HIDDEN,N_OUTPUTS,0.1) # vanilla RNN\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_running_loss = np.zeros(N_EPHOCS)\n",
    "train_acc = np.zeros(N_EPHOCS)\n",
    "nparams = 0\n",
    "for param in model.parameters(): \n",
    "  if param.requires_grad:\n",
    "    nparams += param.data.numpy().size\n",
    "Phist = np.zeros((nparams,N_EPHOCS))\n",
    "\n",
    "for epoch in range(N_EPHOCS):\n",
    "    nps = 0\n",
    "    running_loss=0\n",
    "    running_acc=0\n",
    "    for p,param in enumerate(model.parameters()):\n",
    "        if param.requires_grad:\n",
    "            plist = param.data.numpy().flatten()\n",
    "            for j in range(plist.size):\n",
    "                Phist[nps,epoch]=plist[j]\n",
    "                nps+=1\n",
    "  \n",
    "    for i, data in enumerate(trainloader):\n",
    "#         x, y_tar = data # get the data: x is the string of pixels, and y is the class\n",
    "#        print(x)\n",
    "#         print(x[0]) # 100 images at a time\n",
    "        optimizer.zero_grad() # set the gradients for the optimizer function at 0\n",
    "        x, y_tar = data # get the data: x is the string of pixels, and y is the class\n",
    "#        print(x.shape)\n",
    "        x = x[:,:,::2,::2]\n",
    "        x = x.contiguous()\n",
    "        # print(x.shape)\n",
    "        # print(x.view)\n",
    "        y_pred = model(x.view(BATCH_SIZE,N_STEPS_SPARSE,N_INPUTS)) # make the prediction\n",
    "        loss = criterion(y_pred.view(BATCH_SIZE,N_OUTPUTS),y_tar) # calculate the loss\n",
    "        loss.backward() # backpropogate the loss\n",
    "        optimizer.step() # set the next step in the weights using the optimization function\n",
    "        running_loss+=loss.item() # track the running loss\n",
    "        running_acc+=get_accuracy(y_pred.view(BATCH_SIZE,N_OUTPUTS), y_tar, BATCH_SIZE) # compute the accuracy of the prediction\n",
    "    train_running_loss[epoch] = running_loss # compute this epoch's losss\n",
    "    train_acc[epoch] = running_acc/i # compute this epoch's accuracy\n",
    "    \n",
    "    print('Epoch:  %d | Loss: %.4f | Train Accuracy: %.2f' %(epoch, train_running_loss[epoch], train_acc[epoch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sAWtPBjlOc1V"
   },
   "outputs": [],
   "source": [
    "# Sequential MNIST -- feed in the dataset pixel by pixel\n",
    "# parameters \n",
    "N_STEPS = 28 # NSTEPs is the number of traversals through each datapoint that the RNN must make\n",
    "N_STEPS_SPARSE = 1# feed the vector in all at once rather than one at a time \n",
    "N_INPUTS = 14*14#feed the vector in at once. No real recurrent property\n",
    "N_HIDDEN = 400 # let's raise the number of hidden neurons\n",
    "N_OUTPUTS = 50 # 50 output values\n",
    "N_EPHOCS = 30\n",
    "\n",
    "model = PRNN(N_INPUTS,N_HIDDEN,N_OUTPUTS,0.1)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_running_loss = np.zeros(N_EPHOCS)\n",
    "train_acc = np.zeros(N_EPHOCS)\n",
    "nparams = 0\n",
    "for param in model.parameters(): \n",
    "  if param.requires_grad:\n",
    "    nparams += param.data.numpy().size\n",
    "Phist = np.zeros((nparams,N_EPHOCS))\n",
    "\n",
    "for epoch in range(N_EPHOCS):\n",
    "    nps = 0\n",
    "    running_loss=0\n",
    "    running_acc=0\n",
    "    for p,param in enumerate(model.parameters()):\n",
    "        if param.requires_grad:\n",
    "            plist = param.data.numpy().flatten()\n",
    "            for j in range(plist.size):\n",
    "                Phist[nps,epoch]=plist[j]\n",
    "                nps+=1\n",
    "  \n",
    "    for i, data in enumerate(trainloader):\n",
    "#         x, y_tar = data # get the data: x is the string of pixels, and y is the class\n",
    "#        print(x)\n",
    "#         print(x[0]) # 100 images at a time\n",
    "        optimizer.zero_grad() # set the gradients for the optimizer function at 0\n",
    "        x, y_tar = data # get the data: x is the string of pixels, and y is the class\n",
    "#        print(x.shape)\n",
    "        x = x[:,:,::2,::2]\n",
    "        x = x.contiguous()\n",
    "        # print(x.shape)\n",
    "        # print(x.view)\n",
    "        y_pred = model(x.view(BATCH_SIZE,N_STEPS_SPARSE,N_INPUTS)) # make the prediction\n",
    "        loss = criterion(y_pred.view(BATCH_SIZE,N_OUTPUTS),y_tar) # calculate the loss\n",
    "        loss.backward() # backpropogate the loss\n",
    "        optimizer.step() # set the next step in the weights using the optimization function\n",
    "        running_loss+=loss.item() # track the running loss\n",
    "        running_acc+=get_accuracy(y_pred.view(BATCH_SIZE,N_OUTPUTS), y_tar, BATCH_SIZE) # compute the accuracy of the prediction\n",
    "    train_running_loss[epoch] = running_loss # compute this epoch's losss\n",
    "    train_acc[epoch] = running_acc/i # compute this epoch's accuracy\n",
    "    \n",
    "    print('Epoch:  %d | Loss: %.4f | Train Accuracy: %.2f' %(epoch, train_running_loss[epoch], train_acc[epoch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "24WTqZU0Oz4h"
   },
   "outputs": [],
   "source": [
    "# Sequential MNIST -- Feed in the data four pixels at a time\n",
    "# parameters \n",
    "N_STEPS = 28 # NSTEPs is the number of traversals through each datapoint that the RNN must make\n",
    "N_STEPS_SPARSE = 7*7# feed the vector in all at once rather than one at a time \n",
    "N_INPUTS = 4#feed the image in four pixels at a time. No real recurrent property\n",
    "N_HIDDEN = 400 # let's raise the number of hidden neurons\n",
    "N_OUTPUTS = 50 # 50 output values\n",
    "N_EPHOCS = 30\n",
    "\n",
    "model = PRNN(N_INPUTS,N_HIDDEN,N_OUTPUTS,0.1)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_running_loss = np.zeros(N_EPHOCS)\n",
    "train_acc = np.zeros(N_EPHOCS)\n",
    "nparams = 0\n",
    "for param in model.parameters(): \n",
    "  if param.requires_grad:\n",
    "    nparams += param.data.numpy().size\n",
    "Phist = np.zeros((nparams,N_EPHOCS))\n",
    "\n",
    "for epoch in range(N_EPHOCS):\n",
    "    nps = 0\n",
    "    running_loss=0\n",
    "    running_acc=0\n",
    "    for p,param in enumerate(model.parameters()):\n",
    "        if param.requires_grad:\n",
    "            plist = param.data.numpy().flatten()\n",
    "            for j in range(plist.size):\n",
    "                Phist[nps,epoch]=plist[j]\n",
    "                nps+=1\n",
    "  \n",
    "    for i, data in enumerate(trainloader):\n",
    "#         x, y_tar = data # get the data: x is the string of pixels, and y is the class\n",
    "#        print(x)\n",
    "#         print(x[0]) # 100 images at a time\n",
    "        optimizer.zero_grad() # set the gradients for the optimizer function at 0\n",
    "        x, y_tar = data # get the data: x is the string of pixels, and y is the class\n",
    "#        print(x.shape)\n",
    "        x = x[:,:,::2,::2]\n",
    "        x = x.contiguous()\n",
    "        # print(x.shape)\n",
    "        # print(x.view)\n",
    "        y_pred = model(x.view(BATCH_SIZE,N_STEPS_SPARSE,N_INPUTS)) # make the prediction\n",
    "        loss = criterion(y_pred.view(BATCH_SIZE,N_OUTPUTS),y_tar) # calculate the loss\n",
    "        loss.backward() # backpropogate the loss\n",
    "        optimizer.step() # set the next step in the weights using the optimization function\n",
    "        running_loss+=loss.item() # track the running loss\n",
    "        running_acc+=get_accuracy(y_pred.view(BATCH_SIZE,N_OUTPUTS), y_tar, BATCH_SIZE) # compute the accuracy of the prediction\n",
    "    train_running_loss[epoch] = running_loss # compute this epoch's losss\n",
    "    train_acc[epoch] = running_acc/i # compute this epoch's accuracy\n",
    "    \n",
    "    print('Epoch:  %d | Loss: %.4f | Train Accuracy: %.2f' %(epoch, train_running_loss[epoch], train_acc[epoch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jTOzpSgAO6CG"
   },
   "outputs": [],
   "source": [
    "# Sequential MNIST -- feed in the dataset pixel by pixel\n",
    "# parameters \n",
    "N_STEPS = 28 # NSTEPs is the number of traversals through each datapoint that the RNN must make\n",
    "N_STEPS_SPARSE = 14*14# feed the vector in all at once rather than one at a time \n",
    "N_INPUTS = 1#feed the vector in at once. No real recurrent property\n",
    "N_HIDDEN = 400 # let's raise the number of hidden neurons\n",
    "N_OUTPUTS = 50 # 50 output values\n",
    "N_EPHOCS = 40\n",
    "\n",
    "model = PRNN(N_INPUTS,N_HIDDEN,N_OUTPUTS,0.1)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_running_loss = np.zeros(N_EPHOCS)\n",
    "train_acc = np.zeros(N_EPHOCS)\n",
    "nparams = 0\n",
    "for param in model.parameters(): \n",
    "  if param.requires_grad:\n",
    "    nparams += param.data.numpy().size\n",
    "Phist = np.zeros((nparams,N_EPHOCS))\n",
    "\n",
    "for epoch in range(N_EPHOCS):\n",
    "    nps = 0\n",
    "    running_loss=0\n",
    "    running_acc=0\n",
    "    for p,param in enumerate(model.parameters()):\n",
    "        if param.requires_grad:\n",
    "            plist = param.data.numpy().flatten()\n",
    "            for j in range(plist.size):\n",
    "                Phist[nps,epoch]=plist[j]\n",
    "                nps+=1\n",
    "  \n",
    "    for i, data in enumerate(trainloader):\n",
    "#         x, y_tar = data # get the data: x is the string of pixels, and y is the class\n",
    "#        print(x)\n",
    "#         print(x[0]) # 100 images at a time\n",
    "        optimizer.zero_grad() # set the gradients for the optimizer function at 0\n",
    "        x, y_tar = data # get the data: x is the string of pixels, and y is the class\n",
    "#        print(x.shape)\n",
    "        x = x[:,:,::2,::2]\n",
    "        x = x.contiguous()\n",
    "        # print(x.shape)\n",
    "        # print(x.view)\n",
    "        y_pred = model(x.view(BATCH_SIZE,N_STEPS_SPARSE,N_INPUTS)) # make the prediction\n",
    "        loss = criterion(y_pred.view(BATCH_SIZE,N_OUTPUTS),y_tar) # calculate the loss\n",
    "        loss.backward() # backpropogate the loss\n",
    "        optimizer.step() # set the next step in the weights using the optimization function\n",
    "        running_loss+=loss.item() # track the running loss\n",
    "        running_acc+=get_accuracy(y_pred.view(BATCH_SIZE,N_OUTPUTS), y_tar, BATCH_SIZE) # compute the accuracy of the prediction\n",
    "    train_running_loss[epoch] = running_loss # compute this epoch's losss\n",
    "    train_acc[epoch] = running_acc/i # compute this epoch's accuracy\n",
    "    \n",
    "    print('Epoch:  %d | Loss: %.4f | Train Accuracy: %.2f' %(epoch, train_running_loss[epoch], train_acc[epoch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zWd25yDFEY--"
   },
   "outputs": [],
   "source": [
    "## Sequential-Sparse MNIST with RNN-LSTM\n",
    "# parameters: same task as the 4-pixels-at-a-time task\n",
    "N_STEPS_SPARSE = 7*7 # the number of traversals through each dataset that the RNN must make \n",
    "N_INPUTS = 4 # feed the image in four pixels at a time. No real recurrent property\n",
    "N_HIDDEN = 400 # let's raise the number of hidden neurons\n",
    "N_OUTPUTS = 50 # 50 output values\n",
    "N_EPHOCS = 30\n",
    "\n",
    "model = PRNN_LSTM(N_INPUTS,N_HIDDEN,N_OUTPUTS,0.1)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_running_loss = np.zeros(N_EPHOCS)\n",
    "train_acc = np.zeros(N_EPHOCS)\n",
    "nparams = 0\n",
    "for param in model.parameters(): \n",
    "  if param.requires_grad:\n",
    "    nparams += param.data.numpy().size\n",
    "Phist = np.zeros((nparams,N_EPHOCS))\n",
    "\n",
    "for epoch in range(N_EPHOCS):\n",
    "    nps = 0\n",
    "    running_loss=0\n",
    "    running_acc=0\n",
    "    for p,param in enumerate(model.parameters()):\n",
    "        if param.requires_grad:\n",
    "            plist = param.data.numpy().flatten()\n",
    "            for j in range(plist.size):\n",
    "                Phist[nps,epoch]=plist[j]\n",
    "                nps+=1\n",
    "  \n",
    "    for i, data in enumerate(trainloader):\n",
    "#         x, y_tar = data # get the data: x is the string of pixels, and y is the class\n",
    "#        print(x)\n",
    "#         print(x[0]) # 100 images at a time\n",
    "        optimizer.zero_grad() # set the gradients for the optimizer function at 0\n",
    "        x, y_tar = data # get the data: x is the string of pixels, and y is the class\n",
    "#        print(x.shape)\n",
    "        x = x[:,:,::2,::2]\n",
    "        x = x.contiguous()\n",
    "        # print(x.shape)\n",
    "        # print(x.view)\n",
    "        y_pred = model(x.view(BATCH_SIZE,N_STEPS_SPARSE,N_INPUTS)) # make the prediction\n",
    "        loss = criterion(y_pred.view(BATCH_SIZE,N_OUTPUTS),y_tar) # calculate the loss\n",
    "        loss.backward() # backpropogate the loss\n",
    "        optimizer.step() # set the next step in the weights using the optimization function\n",
    "        running_loss+=loss.item() # track the running loss\n",
    "        running_acc+=get_accuracy(y_pred.view(BATCH_SIZE,N_OUTPUTS), y_tar, BATCH_SIZE) # compute the accuracy of the prediction\n",
    "    train_running_loss[epoch] = running_loss # compute this epoch's losss\n",
    "    train_acc[epoch] = running_acc/i # compute this epoch's accuracy\n",
    "    \n",
    "    print('Epoch:  %d | Loss: %.4f | Train Accuracy: %.2f' %(epoch, train_running_loss[epoch], train_acc[epoch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "loYP0shHj7VV"
   },
   "outputs": [],
   "source": [
    "for data in trainloader:\n",
    "  x, y_tar = data\n",
    "  print(x.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SHcq2A8n1Een"
   },
   "source": [
    "The training takes too long. We will feed in every other image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fwGd6FbIRaAp"
   },
   "source": [
    "# OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DBfU2ho2n5xj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kBcFhoLDO2_N"
   },
   "outputs": [],
   "source": [
    "# # parameters \n",
    "# N_STEPS = 28\n",
    "# N_INPUTS = 28\n",
    "# N_HIDDEN = 128\n",
    "# N_OUTPUTS = 10\n",
    "# N_EPHOCS = 10\n",
    "\n",
    "# model = PRNN(N_INPUTS,N_HIDDEN,N_OUTPUTS,0.1)\n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-1, momentum=0.9)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# train_running_loss = np.zeros(N_EPHOCS)\n",
    "# train_acc = np.zeros(N_EPHOCS)\n",
    "# nparams = 0\n",
    "# for param in model.parameters(): \n",
    "#   if param.requires_grad:\n",
    "#     nparams += param.data.numpy().size\n",
    "# Phist = np.zeros((nparams,N_EPHOCS))\n",
    "\n",
    "# for epoch in range(N_EPHOCS):\n",
    "#     nps = 0\n",
    "#     running_loss=0\n",
    "#     running_acc=0\n",
    "#     for p,param in enumerate(model.parameters()):\n",
    "#         if param.requires_grad:\n",
    "#             plist = param.data.numpy().flatten()\n",
    "#             for j in range(plist.size):\n",
    "#                 Phist[nps,epoch]=plist[j]\n",
    "#                 nps+=1\n",
    "  \n",
    "#     for i, data in enumerate(trainloader):\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         x, y_tar = data\n",
    "#         y_pred = model(x.view(BATCH_SIZE,N_STEPS,N_INPUTS))\n",
    "#         loss = criterion(y_pred.view(BATCH_SIZE,N_OUTPUTS),y_tar)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         running_loss+=loss.item()\n",
    "#         running_acc+=get_accuracy(y_pred.view(BATCH_SIZE,N_OUTPUTS), y_tar, BATCH_SIZE)\n",
    "#     train_running_loss[epoch] = running_loss\n",
    "#     train_acc[epoch] = running_acc/i\n",
    "    \n",
    "#     print('Epoch:  %d | Loss: %.4f | Train Accuracy: %.2f' %(epoch, train_running_loss[epoch], train_acc[epoch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5jnZLdkJoL2A"
   },
   "outputs": [],
   "source": [
    "# plt.plot(Phist[::1000,:].T)\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(train_running_loss)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6viVdlkV7sxh"
   },
   "outputs": [],
   "source": [
    "# l2_reg = Variable(torch.FloatTensor(1), requires_grad=True)\n",
    "# for W in mdl.parameters():\n",
    "#   l2_reg = l2_reg + W.norm(2)\n",
    "# batch_loss = (1/N_train)*(y_pred - batch_ys).pow(2).sum() + l2_reg * reg_lambda\n",
    "# batch_loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D9p1DW2v8Wug"
   },
   "source": [
    "Strategies we can use to improve the learning rate for a Vanilla RNN\n",
    "* Better gradient descent methods (best not to play with )\n",
    "* Online optimization methods--can we get the network to try out different orders of the data (\"training regimes\"), find which training regime produces the fastest rate, and determine \n",
    "  * Read the curriculum learning paper: https://ronan.collobert.com/pub/matos/2009_curriculum_icml.pdf \n",
    "\n",
    "\n",
    "More \n",
    "* play with initialization of parameters\n",
    "* spit out the hidden state of the RNN over time\n",
    "* Dropout for regularization\n",
    "* add more hidden layers\n",
    "\n",
    "\n",
    "Let's play with the above:\n",
    "1. Try different parameter initializations\n",
    "2. Monitor the hidden state of the RNN over time and monitor the backpropogation values\n",
    "3. Add more hidden layers\n",
    "4. Initial curriculum learning simulations\n",
    "5. Try out the partitioning of a hidden layer into separate memory units. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BjPnYdrbDvpz"
   },
   "outputs": [],
   "source": [
    "# # Add more hidden layers -- parameterize the number of hidden layers\n",
    "\n",
    "# # build the RNN\n",
    "# class PRNN2(nn.Module):\n",
    "#     def __init__(self, n_inputs, n_hidden,n_output,mr, n_hidden_layers):\n",
    "#         super(PRNN, self).__init__()\n",
    "#         self.n_inputs = n_inputs\n",
    "#         self.n_hidden = n_hidden # number of hidden neurons\n",
    "#         self.n_hidden_layers = n_hidden_layers\n",
    "#         self.n_output = n_output\n",
    "#         self.mr = mr\n",
    "#         self.encoder = nn.Linear(n_inputs,n_hidden)\n",
    "#         # add a function here that will \n",
    "#         self.recurrent = nn.Linear(n_hidden,n_hidden)\n",
    "#         self.decoder = nn.Linear(n_hidden, n_output)\n",
    "#         # self.lstm = nn.LSTM(n_inputs, n_output, batch_first=True)\n",
    "        \n",
    "#     def forward(self, x0):\n",
    "#         T = int(x0.shape[2]/2)\n",
    "#         #self.h1 = Variable(torch.zeros(self.n_hidden))\n",
    "#         self.h1 = Variable(torch.zeros(x0.size(0), self.n_hidden))\n",
    "#         #for t in range(T):\n",
    "#            #self.h1 = self.mr*self.h1+(1-self.mr)*torch.relu(self.encoder(x0[:,t+7,:])+self.recurrent(self.h1))\n",
    "#            # self.h1,_ = self.lstm(x0[:,:,:],self.h1)\n",
    "#         self.y1 = self.decoder(self.h1)\n",
    "        \n",
    "#         return self.y1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tPB7MIBJJoYs"
   },
   "source": [
    "# RNN with LSTM unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rgR5ex3KJnta"
   },
   "outputs": [],
   "source": [
    "# class PRNN(nn.Module):\n",
    "#     def __init__(self, n_inputs, n_hidden,n_output,mr):\n",
    "#         super(PRNN, self).__init__()\n",
    "#         self.n_inputs = n_inputs\n",
    "#         self.n_hidden = n_hidden\n",
    "#         self.n_output = n_output\n",
    "#         self.mr = mr\n",
    "#         self.encoder = nn.Linear(n_inputs,n_hidden)\n",
    "#         self.recurrent = nn.Linear(n_hidden,n_hidden)\n",
    "#         self.decoder = nn.Linear(n_hidden, n_output)\n",
    "#         self.lstm = nn.LSTM(n_inputs, n_hidden)\n",
    "        \n",
    "#     def forward(self, x0):\n",
    "#         x0=x0.permute(1,0,2)\n",
    "#         self.h1 = (torch.zeros(1,BATCH_SIZE,self.n_hidden),torch.zeros(1,BATCH_SIZE,self.n_hidden))\n",
    "#         self.y0, self.h1 = self.lstm(x0,self.h1)\n",
    "#         self.y1 = self.decoder(self.h1[0])\n",
    "        \n",
    "#         return self.y1\n",
    "      \n",
    "# def get_accuracy(logit, target, batch_size):\n",
    "#     corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
    "#     accuracy = 100.0 * corrects/batch_size\n",
    "#     return accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MPzWevbNJuL4"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # parameters \n",
    "# N_STEPS = 28\n",
    "# N_INPUTS = 28\n",
    "# N_HIDDEN = 100\n",
    "# N_OUTPUTS = 10\n",
    "# N_EPHOCS = 10\n",
    "\n",
    "# model = PRNN_LSTM(N_INPUTS,N_HIDDEN,N_OUTPUTS,0.1)\n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-1, momentum=0.9)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# train_running_loss = np.zeros(N_EPHOCS)\n",
    "# train_acc = np.zeros(N_EPHOCS)\n",
    "# nparams = 0\n",
    "# for param in model.parameters(): \n",
    "#   if param.requires_grad:\n",
    "#     nparams += param.data.numpy().size\n",
    "# Phist = np.zeros((nparams,N_EPHOCS))\n",
    "\n",
    "# for epoch in range(N_EPHOCS):\n",
    "#     nps = 0\n",
    "#     running_loss=0\n",
    "#     running_acc=0\n",
    "#     for p,param in enumerate(model.parameters()):\n",
    "#         if param.requires_grad:\n",
    "#             plist = param.data.numpy().flatten()\n",
    "#             for j in range(plist.size):\n",
    "#                 Phist[nps,epoch]=plist[j]\n",
    "#                 nps+=1\n",
    "  \n",
    "#     for i, data in enumerate(trainloader):\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         x, y_tar = data\n",
    "#         y_pred = model(x.view(BATCH_SIZE,N_STEPS,N_INPUTS))\n",
    "#         loss = criterion(y_pred.view(BATCH_SIZE,N_OUTPUTS),y_tar)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         running_loss+=loss.item()\n",
    "#         running_acc+=get_accuracy(y_pred.view(BATCH_SIZE,N_OUTPUTS), y_tar, BATCH_SIZE)\n",
    "#     train_running_loss[epoch] = running_loss\n",
    "#     train_acc[epoch] = running_acc/i\n",
    "    \n",
    "#     print('Epoch:  %d | Loss: %.4f | Train Accuracy: %.2f' %(epoch, train_running_loss[epoch], train_acc[epoch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HCNh7aVYLJOG"
   },
   "outputs": [],
   "source": [
    "# # test the trained RNN-LSTM on the testloader data\n",
    "# for epoch in range(N_EPHOCS):\n",
    "#     nps = 0\n",
    "#     running_loss=0\n",
    "#     running_acc=0\n",
    "#     for p,param in enumerate(model.parameters()):\n",
    "#         if param.requires_grad:\n",
    "#             plist = param.data.numpy().flatten()\n",
    "#             for j in range(plist.size):\n",
    "#                 Phist[nps,epoch]=plist[j]\n",
    "#                 nps+=1\n",
    "  \n",
    "#     for i, data, j, tdata in enumerate(testloader):\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         x, y_tar = data\n",
    "#         y_pred = model(x.view(BATCH_SIZE,N_STEPS,N_INPUTS))\n",
    "#         loss = criterion(y_pred.view(BATCH_SIZE,N_OUTPUTS),y_tar)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         running_loss+=loss.item()\n",
    "#         running_acc+=get_accuracy(y_pred.view(BATCH_SIZE,N_OUTPUTS), y_tar, BATCH_SIZE)\n",
    "#     train_running_loss[epoch] = running_loss\n",
    "#     train_acc[epoch] = running_acc/i\n",
    "    \n",
    "#     print('Epoch:  %d | Loss: %.4f | Train Accuracy: %.2f' %(epoch, train_running_loss[epoch], train_acc[epoch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4YTFzHebJwJo"
   },
   "outputs": [],
   "source": [
    "# plt.plot(Phist[::1000,:].T)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wM_AocsEJx47"
   },
   "outputs": [],
   "source": [
    "# l2_reg = Variable(torch.FloatTensor(1), requires_grad=True)\n",
    "# for W in mdl.parameters():\n",
    "#   l2_reg = l2_reg + W.norm(2)\n",
    "# batch_loss = (1/N_train)*(y_pred - batch_ys).pow(2).sum() + l2_reg * reg_lambda\n",
    "# batch_loss.backward()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Working PRNN",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
