{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MLP_hamlet","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"L3QUcVx4z4Yd","colab_type":"text"},"cell_type":"markdown","source":["# Introduction\n","This notebook will test the performance of a 2-layer MLP (multi-layer perceptron) in performing a character-wise prediction task on \n","* MNIST\n","* the play *Hamlet*. "]},{"metadata":{"id":"l5jJ291f9H2t","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"b1dcaf32-0433-4422-99a1-310a6a475487","executionInfo":{"status":"ok","timestamp":1555360260749,"user_tz":240,"elapsed":4408,"user":{"displayName":"Vinay Viswanadham","photoUrl":"","userId":"17038328966780750763"}}},"cell_type":"code","source":["# imports\n","from tqdm import tnrange\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.autograd import Variable\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import gc\n","import os\n","%matplotlib inline  \n","\n","# alphabet\n","import string\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["cpu\n"],"name":"stdout"}]},{"metadata":{"id":"xuaNMQ011CNj","colab_type":"text"},"cell_type":"markdown","source":["# MLP code"]},{"metadata":{"id":"hN9Ss_xO20Eo","colab_type":"text"},"cell_type":"markdown","source":["## Data preparation"]},{"metadata":{"id":"smKbyWrLz2NB","colab_type":"code","colab":{}},"cell_type":"code","source":["def onehotTensor(category,n_categories):\n","    tensor = torch.zeros(1, n_categories,dtype=torch.long)\n","    tensor[0][category] = 1\n","    return tensor\n","        \n","def get_accuracy(logit, target):\n","    batch_size = len(target)\n","    corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n","    accuracy = 100.0 * corrects/batch_size\n","    return accuracy.item()\n","\n","def nparam(ninputs,nhidden,noutputs):\n","    return ninputs*(nhidden+1) + nhidden*(nhidden+1)+nhidden*(noutputs+1)\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4G-YeFOa22ap","colab_type":"text"},"cell_type":"markdown","source":["## Model"]},{"metadata":{"id":"myxC_JCB24BI","colab_type":"code","colab":{}},"cell_type":"code","source":["# a prototype MLP\n","class MLP(nn.Module):\n","    def __init__(self, n_inputs, n_hidden_neurons, n_hidden_layers, n_output, dt, device):\n","        super(MLP, self).__init__()\n","        self.n_inputs = n_inputs # set the number of neurons in the input layer\n","        self.n_hidden_neurons = n_hidden_neurons # how many neurons are in each hidden layer\n","        self.n_hidden_layers = n_hidden_layers # how many hidden layers are we using in our model\n","        self.n_output = n_output # set the number of neurons in the output layer\n","        self.dt = nn.Parameter(torch.Tensor([dt])) # set the change in parameter update\n","        self.a = nn.Parameter(torch.Tensor([1])) # set the bias\n","        self.sig = nn.Sigmoid() # set the activation function \n","        self.decoder = nn.Linear(n_hidden, n_output) # decode output\n","        self.encoder = nn.Linear(n_inputs, n_hidden) # encode input\n","        \n","    # the main operation for the MLP is to update each hidden layer with the state of the previous hidden layer\n","    # so, if you need to update the hidden layers, make sure you update each layer with state of the previous layer\n","    \n","    def update_hidden_layer(self,h1_prev):\n","      # update the neurons in the current hidden layer with the state of the inputted \"previous\" layer\n","      for i in range(self.n_hidden_layers):\n","        # update each node in the h1_current\n","        self.h1 = nn.Linear(self.h1,self.h1) # update the stored hidden layer state as many times as there are hidden layers specified\n","      return(h1_current)\n","    \n","    def forward(self, x0):\n","        x0=x0.permute(1,0,2) # permute the tensor\n","        # save the hidden layers as a tensor\n","        \n","        self.hlayers = torch.zeros(self.n_hidden_layers,BATCH_SIZE,self.n_hidden).to(device) # initialize the hidden layer\n","        # the first layer will consist of the encoded inputs\n","        for i in range(x0.size(0)):\n","            self.hlayers[0,:,:] = self.sig(self.dt)*self.a*torch.tanh(self.encoder(x0[i,:,:])) # (1-self.sig(self.dt))*self.h1+self.sig(self.dt)*self.a*torch.tanh(self.encoder(x0[i,:,:])+self.recurrent(self.h1))\n","        # update the additional hidden layers\n","        ind_hidden = 1\n","        while ind_hidden < self.n_hidden_layers:\n","          self.hlayers[ind_hidden,:,:] = self.update_hidden_layer(self.hlayers[ind_hidden - 1,:,:])\n","          ind_hidden += 1\n","\n","        self.y1 = self.decoder(self.hlayers[-1,:,:])\n","        return self.y1 "],"execution_count":0,"outputs":[]},{"metadata":{"id":"pLXkw4RVq4jw","colab_type":"text"},"cell_type":"markdown","source":["# Load data"]},{"metadata":{"id":"lw_mPYUqEcFE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":275},"outputId":"5b1af598-65f2-42b9-8e1d-82c251bc6d66","executionInfo":{"status":"ok","timestamp":1555360278329,"user_tz":240,"elapsed":10968,"user":{"displayName":"Vinay Viswanadham","photoUrl":"","userId":"17038328966780750763"}}},"cell_type":"code","source":["# MNIST\n","\n","BATCH_SIZE = 100\n","\n","# list all transformations\n","transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.1307,), (0.3081,))])\n","\n","# download and load training dataset\n","trainset = torchvision.datasets.MNIST(root='./data', train=True,\n","                                        download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n","                                          shuffle=True, num_workers=2)\n","\n","# download and load testing dataset\n","testset = torchvision.datasets.MNIST(root='./data', train=False,\n","                                       download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n","                                         shuffle=False, num_workers=2)\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["  0%|          | 0/9912422 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["9920512it [00:06, 1622994.43it/s]                            \n"],"name":"stderr"},{"output_type":"stream","text":["Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["32768it [00:00, 239709.21it/s]                           \n","0it [00:00, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n","Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["1654784it [00:03, 446144.75it/s]                             \n","8192it [00:00, 168642.55it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n","Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n","Processing...\n","Done!\n"],"name":"stdout"}]},{"metadata":{"id":"tS0JlvNs9HFA","colab_type":"code","colab":{}},"cell_type":"code","source":["# training\n"],"execution_count":0,"outputs":[]}]}