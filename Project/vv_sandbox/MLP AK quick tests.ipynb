{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from tqdm import tnrange\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "%matplotlib inline  \n",
    "import datetime\n",
    "\n",
    "# alphabet\n",
    "import string\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(logit, target):\n",
    "    batch_size = len(target)\n",
    "    corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
    "    accuracy = 100.0 * corrects/batch_size\n",
    "    return accuracy.item()\n",
    "\n",
    "def nparam(ninputs,nhidden,noutputs):\n",
    "    return ninputs*(nhidden+1) + nhidden*(nhidden+1)+nhidden*(noutputs+1)\n",
    "\n",
    "# define the nnumber of parameters we need\n",
    "def nparam_MLP(N_INPUTS,N_HIDDEN,N_OUTPUTS):\n",
    "    input_to_hidden1 = (N_INPUTS+1)*N_HIDDEN #+1 for bias\n",
    "    hidden1_to_hidden2 = (N_HIDDEN + 1)*N_HIDDEN\n",
    "    hidden2_to_output = (N_OUTPUTS)*(N_HIDDEN+1)\n",
    "    return(sum([input_to_hidden1,hidden1_to_hidden2,hidden2_to_output]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a prototype MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden_neurons, n_output,  device):\n",
    "        super(MLP, self).__init__()\n",
    "        self.n_inputs = n_inputs # set the number of neurons in the input layer\n",
    "        self.n_hidden_neurons = n_hidden_neurons # how many neurons are in each hidden layer\n",
    "        self.n_output = n_output # set the number of neurons in the output layer\n",
    "        self.sig = nn.Sigmoid() # set the activation function \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.n_hidden = n_hidden_neurons\n",
    "        self.encoder = nn.Linear(n_inputs, n_hidden_neurons) # encode input\n",
    "        self.recurrent = nn.Linear(n_hidden_neurons,n_hidden_neurons) # recurrent connections\n",
    "        self.decoder = nn.Linear(n_hidden_neurons, n_output) # decode output\n",
    "                \n",
    "    def forward(self, x):\n",
    "        self.hidden1 = self.tanh(self.encoder(x))\n",
    "        self.hidden2 = self.tanh(self.recurrent(self.hidden1))\n",
    "        self.output = self.decoder(self.hidden2)\n",
    "        return self.output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.   0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09]\n"
     ]
    }
   ],
   "source": [
    "# Test MLP on Anna Karenina\n",
    "# Load Anna Karenina\n",
    "from torch.utils.data import DataLoader # dataloader \n",
    "import sys\n",
    "sys.path.insert(0,'../final_project/Data/')\n",
    "from AnnaDataset_MLP import AnnaDataset, InvertAnna # import AK dataset\n",
    "# from AnnaDataset import AnnaDataset, InvertAnna # import AK dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# params\n",
    "BATCH_SIZE = 500 # how many batches we are running\n",
    "N_STEPS = 10 # How many characters are we inputting into the list at a time\n",
    "N_HIDDEN_NEURONS = 512 # how many neurons per hidden layer\n",
    "N_LAYERS = 2 # 2 hidden layers\n",
    "N_EPOCHS = 50 # how many training epocs\n",
    "learning_rates = np.asarray([2]) # learning rates\n",
    "N_REPS = 1 # len(learning_rates) # the number of learning repetitions\n",
    "gidx = int(N_HIDDEN_NEURONS/2)\n",
    "\n",
    "# regularization parameters\n",
    "# lambdas = np.arange(0,1e-2,3e-3,dtype=np.float)\n",
    "lambdas = np.arange(0,1e-1,1e-2,dtype=np.float) # full sweep\n",
    "# lambdas = np.arange(0.4,1,0.45) # short sweep\n",
    "print(lambdas)\n",
    "# lambdas = np.arange(0,1,1e-1,dtype=np.float) # full sweep\n",
    "N_LAMBDA = len(lambdas)\n",
    "\n",
    "# load data\n",
    "# list all transformations\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Normalize((0,), (0.3,))])\n",
    "\n",
    "dataset = AnnaDataset(N_STEPS) # load the dataset\n",
    "\n",
    "N_INPUTS = len(dataset.categories)*N_STEPS\n",
    "N_OUTPUTS = len(dataset.categories)\n",
    "N_PARAMS = nparam_MLP(N_INPUTS,N_HIDDEN_NEURONS,N_OUTPUTS)\n",
    "\n",
    "\n",
    "# trainloader = DataLoader(dataset, batch_size=BATCH_SIZE,\n",
    "#                         shuffle=False, num_workers=4) # create a DataLoader. We want a batch of BATCH_SIZE entries\n",
    "# testloader = DataLoader(dataset, batch_size=BATCH_SIZE,\n",
    "#                         shuffle=False, num_workers=4) # create a DataLoader. We want a batch of BATCH_SIZE entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566579\n",
      "1566579\n",
      "391645\n"
     ]
    }
   ],
   "source": [
    "# test the test-train-validate sampler\n",
    "# # test_split = torch.split(dataset.onehot_encoded,20,dim=0)\n",
    "# # print(len(test_split))\n",
    "# print(dataset.onehot_encoded.shape)\n",
    "# print(test_split[0].shape)\n",
    "# print(test_split[1].shape)\n",
    "\n",
    "# train-test-split\n",
    "# train_fraction,test_fraction,valid_fraction = (0.8,0.1,0.1)\n",
    "train_fraction = 0.8\n",
    "\n",
    "random_seed = 0\n",
    "shuffle_dataset = True\n",
    "\n",
    "# from https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets \n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset.onehot_encoded)\n",
    "indices = list(range(dataset_size))\n",
    "train_split = int(np.floor(train_fraction * dataset_size))\n",
    "print(train_split)\n",
    "# valid_split = train_split + int(np.floor(valid_fraction * dataset_size))\n",
    "# test_split = valid_split d+ int(np.floor(test_fraction * dataset_size))\n",
    "\n",
    "# print(train_split,valid_split,test_split,dataset_size)\n",
    "\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "# train_indices, val_indices, test_indices = indices[:train_split], indices[train_split:valid_split], indices[valid_split:]\n",
    "train_indices = indices[:train_split]\n",
    "# val_indices = indices[train_split:valid_split]\n",
    "# test_indices = indices[valid_split:]\n",
    "test_indices = indices[train_split:]\n",
    "\n",
    "print(len(train_indices))\n",
    "# print(len(val_indices))\n",
    "print(len(test_indices))\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = torch.utils.data.SequentialSampler(train_indices)\n",
    "test_sampler = torch.utils.data.SequentialSampler(test_indices)\n",
    "# valid_sampler = torch.utils.data.SubsetRandomSampler(val_indices)\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "#                                            sampler=train_sampler)\n",
    "# validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "#                                                 sampler=valid_sampler)\n",
    "\n",
    "\n",
    "# dataset_train,dataset_test = train_test_split(dataset,test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test-validate split\n",
    "\n",
    "trainloader = DataLoader(dataset, batch_size=BATCH_SIZE,\n",
    "                         shuffle=False, num_workers=4,\n",
    "                        sampler = train_sampler) # create a DataLoader. We want a batch of BATCH_SIZE entries\n",
    "# validloader = DataLoader(dataset, batch_size=BATCH_SIZE,\n",
    "#                         sampler = valid_sampler) # create a DataLoader. We want a batch of BATCH_SIZE entries\n",
    "testloader = DataLoader(dataset, batch_size=BATCH_SIZE,\n",
    "                        shuffle=False, num_workers=4,\n",
    "                        sampler = test_sampler) # create a DataLoader. We want a batch of BATCH_SIZE entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-28T16:35:45.389754\n"
     ]
    }
   ],
   "source": [
    "# modelkey = ''.join([str(np.random.randint(0,9)) for i in range(10)])\n",
    "\n",
    "runnow = datetime.datetime.now()\n",
    "modelkey = str(runnow.isoformat())\n",
    "print(modelkey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "009f11bb48eb40a8a064966ba208f7b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b781e6ebfd34a83b995216c3f81e5a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0; rep 0; lambda 0.000000; train accuracy 30.566050; train loss 2.504377; test accuracy 35.289541; test loss 2.263334; reg val 3199.556152; time 2019-04-28 16:39:17.661750\n",
      "Epoch 1; rep 0; lambda 0.000000; train accuracy 37.293235; train loss 2.177907; test accuracy 37.821429; test loss 2.148207; reg val 3368.358887; time 2019-04-28 16:40:07.945759\n",
      "Epoch 2; rep 0; lambda 0.000000; train accuracy 39.848756; train loss 2.067166; test accuracy 40.102041; test loss 2.050313; reg val 3565.511230; time 2019-04-28 16:40:59.819310\n",
      "Epoch 3; rep 0; lambda 0.000000; train accuracy 42.459796; train loss 1.962239; test accuracy 42.568878; test loss 1.950831; reg val 3763.987793; time 2019-04-28 16:41:51.932330\n",
      "Epoch 4; rep 0; lambda 0.000000; train accuracy 44.548819; train loss 1.877771; test accuracy 45.035714; test loss 1.868702; reg val 3950.877441; time 2019-04-28 16:42:44.193018\n",
      "Epoch 5; rep 0; lambda 0.000000; train accuracy 46.308232; train loss 1.811234; test accuracy 46.455357; test loss 1.816248; reg val 4135.092773; time 2019-04-28 16:43:36.943182\n",
      "Epoch 6; rep 0; lambda 0.000000; train accuracy 47.863752; train loss 1.755219; test accuracy 47.271684; test loss 1.775554; reg val 4319.676758; time 2019-04-28 16:44:29.228901\n",
      "Epoch 7; rep 0; lambda 0.000000; train accuracy 49.188258; train loss 1.706252; test accuracy 48.514031; test loss 1.731877; reg val 4502.860352; time 2019-04-28 16:45:21.515161\n",
      "Epoch 8; rep 0; lambda 0.000000; train accuracy 50.374920; train loss 1.663090; test accuracy 49.636480; test loss 1.693634; reg val 4681.506836; time 2019-04-28 16:46:13.605756\n",
      "Epoch 9; rep 0; lambda 0.000000; train accuracy 51.404276; train loss 1.624982; test accuracy 50.588010; test loss 1.660103; reg val 4856.083008; time 2019-04-28 16:47:05.822644\n",
      "Epoch 10; rep 0; lambda 0.000000; train accuracy 52.381621; train loss 1.591079; test accuracy 51.595663; test loss 1.627536; reg val 5027.505859; time 2019-04-28 16:47:57.488341\n",
      "Epoch 11; rep 0; lambda 0.000000; train accuracy 53.208360; train loss 1.560907; test accuracy 52.543367; test loss 1.597227; reg val 5195.146484; time 2019-04-28 16:48:49.583229\n",
      "Epoch 12; rep 0; lambda 0.000000; train accuracy 53.947033; train loss 1.534037; test accuracy 53.256378; test loss 1.570235; reg val 5357.702148; time 2019-04-28 16:49:42.052567\n",
      "Epoch 13; rep 0; lambda 0.000000; train accuracy 54.587747; train loss 1.510011; test accuracy 53.877551; test loss 1.546601; reg val 5514.406250; time 2019-04-28 16:50:35.278902\n",
      "Epoch 14; rep 0; lambda 0.000000; train accuracy 55.154754; train loss 1.488461; test accuracy 54.387755; test loss 1.526187; reg val 5664.931152; time 2019-04-28 16:51:28.236457\n",
      "Epoch 15; rep 0; lambda 0.000000; train accuracy 55.685067; train loss 1.469034; test accuracy 54.863520; test loss 1.508407; reg val 5810.186523; time 2019-04-28 16:52:20.919379\n",
      "Epoch 16; rep 0; lambda 0.000000; train accuracy 56.140077; train loss 1.451406; test accuracy 55.246173; test loss 1.492678; reg val 5950.512695; time 2019-04-28 16:53:13.669533\n",
      "Epoch 17; rep 0; lambda 0.000000; train accuracy 56.566050; train loss 1.435325; test accuracy 55.559949; test loss 1.478593; reg val 6086.037109; time 2019-04-28 16:54:06.003163\n",
      "Epoch 18; rep 0; lambda 0.000000; train accuracy 56.948947; train loss 1.420594; test accuracy 55.880102; test loss 1.465919; reg val 6217.119141; time 2019-04-28 16:54:58.134891\n",
      "Epoch 19; rep 0; lambda 0.000000; train accuracy 57.320038; train loss 1.407041; test accuracy 56.117347; test loss 1.454508; reg val 6344.198730; time 2019-04-28 16:55:49.861924\n",
      "Epoch 20; rep 0; lambda 0.000000; train accuracy 57.634014; train loss 1.394506; test accuracy 56.363520; test loss 1.444181; reg val 6467.684570; time 2019-04-28 16:56:41.934945\n",
      "Epoch 21; rep 0; lambda 0.000000; train accuracy 57.926930; train loss 1.382854; test accuracy 56.604592; test loss 1.434718; reg val 6588.125977; time 2019-04-28 16:57:34.176360\n",
      "Epoch 22; rep 0; lambda 0.000000; train accuracy 58.219847; train loss 1.371976; test accuracy 56.794643; test loss 1.425917; reg val 6705.400391; time 2019-04-28 16:58:26.600855\n",
      "Epoch 23; rep 0; lambda 0.000000; train accuracy 58.488513; train loss 1.361790; test accuracy 56.993622; test loss 1.417637; reg val 6820.169922; time 2019-04-28 16:59:18.918902\n",
      "Epoch 24; rep 0; lambda 0.000000; train accuracy 58.735482; train loss 1.352227; test accuracy 57.224490; test loss 1.409781; reg val 6932.362305; time 2019-04-28 17:00:10.918744\n",
      "Epoch 25; rep 0; lambda 0.000000; train accuracy 58.974154; train loss 1.343230; test accuracy 57.422194; test loss 1.402277; reg val 7041.694336; time 2019-04-28 17:01:03.099480\n",
      "Epoch 26; rep 0; lambda 0.000000; train accuracy 59.214742; train loss 1.334750; test accuracy 57.633929; test loss 1.395078; reg val 7148.493164; time 2019-04-28 17:01:54.805221\n",
      "Epoch 27; rep 0; lambda 0.000000; train accuracy 59.446075; train loss 1.326741; test accuracy 57.794643; test loss 1.388147; reg val 7252.971680; time 2019-04-28 17:02:46.464601\n",
      "Epoch 28; rep 0; lambda 0.000000; train accuracy 59.644544; train loss 1.319166; test accuracy 57.945153; test loss 1.381451; reg val 7354.926270; time 2019-04-28 17:03:38.814740\n",
      "Epoch 29; rep 0; lambda 0.000000; train accuracy 59.827058; train loss 1.311988; test accuracy 58.116071; test loss 1.374954; reg val 7454.632812; time 2019-04-28 17:04:30.870196\n",
      "Epoch 30; rep 0; lambda 0.000000; train accuracy 60.026165; train loss 1.305174; test accuracy 58.271684; test loss 1.368611; reg val 7552.163086; time 2019-04-28 17:05:22.823948\n",
      "Epoch 31; rep 0; lambda 0.000000; train accuracy 60.189853; train loss 1.298694; test accuracy 58.429847; test loss 1.362377; reg val 7647.500977; time 2019-04-28 17:06:15.629633\n",
      "Epoch 32; rep 0; lambda 0.000000; train accuracy 60.352265; train loss 1.292518; test accuracy 58.582908; test loss 1.356214; reg val 7740.857422; time 2019-04-28 17:07:08.227195\n",
      "Epoch 33; rep 0; lambda 0.000000; train accuracy 60.509892; train loss 1.286619; test accuracy 58.738520; test loss 1.350099; reg val 7832.340332; time 2019-04-28 17:08:00.493259\n",
      "Epoch 34; rep 0; lambda 0.000000; train accuracy 60.655712; train loss 1.280973; test accuracy 58.892857; test loss 1.344022; reg val 7921.979492; time 2019-04-28 17:08:53.315504\n",
      "Epoch 35; rep 0; lambda 0.000000; train accuracy 60.808232; train loss 1.275560; test accuracy 59.080357; test loss 1.337990; reg val 8009.831055; time 2019-04-28 17:09:45.846429\n",
      "Epoch 36; rep 0; lambda 0.000000; train accuracy 60.936184; train loss 1.270363; test accuracy 59.229592; test loss 1.332017; reg val 8096.241211; time 2019-04-28 17:10:38.242744\n",
      "Epoch 37; rep 0; lambda 0.000000; train accuracy 61.086471; train loss 1.265364; test accuracy 59.377551; test loss 1.326125; reg val 8181.121094; time 2019-04-28 17:11:30.787147\n",
      "Epoch 38; rep 0; lambda 0.000000; train accuracy 61.222719; train loss 1.260550; test accuracy 59.536990; test loss 1.320342; reg val 8264.574219; time 2019-04-28 17:12:23.258482\n",
      "Epoch 39; rep 0; lambda 0.000000; train accuracy 61.343012; train loss 1.255908; test accuracy 59.660714; test loss 1.314694; reg val 8346.833008; time 2019-04-28 17:13:15.938704\n",
      "Epoch 40; rep 0; lambda 0.000000; train accuracy 61.467773; train loss 1.251426; test accuracy 59.827806; test loss 1.309203; reg val 8427.911133; time 2019-04-28 17:14:08.216214\n",
      "Epoch 41; rep 0; lambda 0.000000; train accuracy 61.577218; train loss 1.247094; test accuracy 59.974490; test loss 1.303891; reg val 8507.824219; time 2019-04-28 17:15:00.710463\n",
      "Epoch 42; rep 0; lambda 0.000000; train accuracy 61.686343; train loss 1.242900; test accuracy 60.108418; test loss 1.298769; reg val 8586.671875; time 2019-04-28 17:15:53.834537\n",
      "Epoch 43; rep 0; lambda 0.000000; train accuracy 61.796426; train loss 1.238836; test accuracy 60.227041; test loss 1.293847; reg val 8664.394531; time 2019-04-28 17:16:47.100617\n",
      "Epoch 44; rep 0; lambda 0.000000; train accuracy 61.905871; train loss 1.234894; test accuracy 60.359694; test loss 1.289129; reg val 8741.248047; time 2019-04-28 17:17:40.315229\n",
      "Epoch 45; rep 0; lambda 0.000000; train accuracy 62.003191; train loss 1.231067; test accuracy 60.484694; test loss 1.284615; reg val 8817.320312; time 2019-04-28 17:18:33.425260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46; rep 0; lambda 0.000000; train accuracy 62.092534; train loss 1.227348; test accuracy 60.608418; test loss 1.280301; reg val 8892.532227; time 2019-04-28 17:19:25.940198\n",
      "Epoch 47; rep 0; lambda 0.000000; train accuracy 62.196554; train loss 1.223733; test accuracy 60.709184; test loss 1.276182; reg val 8966.767578; time 2019-04-28 17:20:18.561309\n",
      "Epoch 48; rep 0; lambda 0.000000; train accuracy 62.292916; train loss 1.220214; test accuracy 60.822704; test loss 1.272249; reg val 9040.200195; time 2019-04-28 17:21:11.388584\n",
      "Epoch 49; rep 0; lambda 0.000000; train accuracy 62.375239; train loss 1.216789; test accuracy 60.946429; test loss 1.268493; reg val 9112.832031; time 2019-04-28 17:22:04.027264\n",
      "Epoch 0; rep 0; lambda 0.010000; train accuracy 30.485003; train loss 2.560340; test accuracy 35.079082; test loss 2.265384; reg val 2463.884277; time 2019-04-28 17:22:56.744428\n",
      "Epoch 1; rep 0; lambda 0.010000; train accuracy 37.114231; train loss 2.230585; test accuracy 37.830357; test loss 2.155591; reg val 1995.364014; time 2019-04-28 17:23:49.878004\n",
      "Epoch 2; rep 0; lambda 0.010000; train accuracy 39.285897; train loss 2.125444; test accuracy 39.649235; test loss 2.072559; reg val 1661.502197; time 2019-04-28 17:24:42.150280\n",
      "Epoch 3; rep 0; lambda 0.010000; train accuracy 41.547224; train loss 2.024410; test accuracy 42.043367; test loss 1.972991; reg val 1425.068115; time 2019-04-28 17:25:34.309624\n",
      "Epoch 4; rep 0; lambda 0.010000; train accuracy 43.684429; train loss 1.938139; test accuracy 44.357143; test loss 1.898620; reg val 1249.254150; time 2019-04-28 17:26:25.153960\n",
      "Epoch 5; rep 0; lambda 0.010000; train accuracy 45.447352; train loss 1.868493; test accuracy 45.872449; test loss 1.845040; reg val 1122.199951; time 2019-04-28 17:27:16.029216\n",
      "Epoch 6; rep 0; lambda 0.010000; train accuracy 46.986280; train loss 1.811163; test accuracy 46.677296; test loss 1.805143; reg val 1035.165039; time 2019-04-28 17:28:06.890289\n",
      "Epoch 7; rep 0; lambda 0.010000; train accuracy 48.309828; train loss 1.762225; test accuracy 47.556122; test loss 1.769826; reg val 979.653381; time 2019-04-28 17:28:57.893268\n",
      "Epoch 8; rep 0; lambda 0.010000; train accuracy 49.434269; train loss 1.719925; test accuracy 48.531888; test loss 1.736788; reg val 945.143433; time 2019-04-28 17:29:48.947546\n",
      "Epoch 9; rep 0; lambda 0.010000; train accuracy 50.417677; train loss 1.683183; test accuracy 49.447704; test loss 1.705520; reg val 923.651001; time 2019-04-28 17:30:40.012376\n",
      "Epoch 10; rep 0; lambda 0.010000; train accuracy 51.297064; train loss 1.651036; test accuracy 50.155612; test loss 1.675922; reg val 910.158813; time 2019-04-28 17:31:31.170716\n",
      "Epoch 11; rep 0; lambda 0.010000; train accuracy 52.059987; train loss 1.622624; test accuracy 50.876276; test loss 1.649337; reg val 900.852295; time 2019-04-28 17:32:22.259934\n",
      "Epoch 12; rep 0; lambda 0.010000; train accuracy 52.769304; train loss 1.597205; test accuracy 51.482143; test loss 1.626117; reg val 893.727295; time 2019-04-28 17:33:12.968145\n",
      "Epoch 13; rep 0; lambda 0.010000; train accuracy 53.370772; train loss 1.574385; test accuracy 52.006378; test loss 1.605726; reg val 887.768677; time 2019-04-28 17:34:04.188729\n",
      "Epoch 14; rep 0; lambda 0.010000; train accuracy 53.941927; train loss 1.553875; test accuracy 52.468112; test loss 1.587236; reg val 882.407898; time 2019-04-28 17:34:55.205495\n",
      "Epoch 15; rep 0; lambda 0.010000; train accuracy 54.424697; train loss 1.535374; test accuracy 52.932398; test loss 1.569914; reg val 877.179810; time 2019-04-28 17:35:46.355030\n",
      "Epoch 16; rep 0; lambda 0.010000; train accuracy 54.842693; train loss 1.518619; test accuracy 53.381378; test loss 1.553725; reg val 871.660400; time 2019-04-28 17:36:37.732429\n",
      "Epoch 17; rep 0; lambda 0.010000; train accuracy 55.222080; train loss 1.503355; test accuracy 53.792092; test loss 1.538744; reg val 865.812073; time 2019-04-28 17:37:29.276342\n",
      "Epoch 18; rep 0; lambda 0.010000; train accuracy 55.607211; train loss 1.489361; test accuracy 54.160714; test loss 1.524898; reg val 859.564514; time 2019-04-28 17:38:20.620451\n",
      "Epoch 19; rep 0; lambda 0.010000; train accuracy 55.932993; train loss 1.476448; test accuracy 54.482143; test loss 1.511969; reg val 852.935425; time 2019-04-28 17:39:11.536036\n",
      "Epoch 20; rep 0; lambda 0.010000; train accuracy 56.257498; train loss 1.464461; test accuracy 54.797194; test loss 1.499740; reg val 845.930908; time 2019-04-28 17:40:02.327687\n",
      "Epoch 21; rep 0; lambda 0.010000; train accuracy 56.548819; train loss 1.453269; test accuracy 55.109694; test loss 1.488100; reg val 838.647339; time 2019-04-28 17:40:53.236943\n",
      "Epoch 22; rep 0; lambda 0.010000; train accuracy 56.812380; train loss 1.442767; test accuracy 55.367347; test loss 1.476967; reg val 831.072754; time 2019-04-28 17:41:44.490496\n",
      "Epoch 23; rep 0; lambda 0.010000; train accuracy 57.052648; train loss 1.432863; test accuracy 55.621173; test loss 1.466331; reg val 823.290161; time 2019-04-28 17:42:35.351148\n",
      "Epoch 24; rep 0; lambda 0.010000; train accuracy 57.298341; train loss 1.423489; test accuracy 55.890306; test loss 1.456266; reg val 815.529968; time 2019-04-28 17:43:26.266716\n",
      "Epoch 25; rep 0; lambda 0.010000; train accuracy 57.514359; train loss 1.414586; test accuracy 56.158163; test loss 1.446845; reg val 807.706543; time 2019-04-28 17:44:17.028677\n",
      "Epoch 26; rep 0; lambda 0.010000; train accuracy 57.731015; train loss 1.406109; test accuracy 56.341837; test loss 1.438089; reg val 799.801453; time 2019-04-28 17:45:07.819070\n",
      "Epoch 27; rep 0; lambda 0.010000; train accuracy 57.933631; train loss 1.398021; test accuracy 56.542092; test loss 1.429976; reg val 791.825562; time 2019-04-28 17:45:58.538647\n",
      "Epoch 28; rep 0; lambda 0.010000; train accuracy 58.152521; train loss 1.390288; test accuracy 56.765306; test loss 1.422436; reg val 783.675659; time 2019-04-28 17:46:50.601520\n",
      "Epoch 29; rep 0; lambda 0.010000; train accuracy 58.335992; train loss 1.382882; test accuracy 56.978316; test loss 1.415359; reg val 775.607971; time 2019-04-28 17:47:43.535159\n",
      "Epoch 30; rep 0; lambda 0.010000; train accuracy 58.536375; train loss 1.375785; test accuracy 57.163265; test loss 1.408682; reg val 767.712036; time 2019-04-28 17:48:36.202555\n",
      "Epoch 31; rep 0; lambda 0.010000; train accuracy 58.700383; train loss 1.368980; test accuracy 57.329082; test loss 1.402346; reg val 759.965759; time 2019-04-28 17:49:29.186003\n",
      "Epoch 32; rep 0; lambda 0.010000; train accuracy 58.880664; train loss 1.362447; test accuracy 57.507653; test loss 1.396298; reg val 752.245117; time 2019-04-28 17:50:22.000478\n",
      "Epoch 33; rep 0; lambda 0.010000; train accuracy 59.037332; train loss 1.356170; test accuracy 57.654337; test loss 1.390512; reg val 744.659790; time 2019-04-28 17:51:14.928377\n",
      "Epoch 34; rep 0; lambda 0.010000; train accuracy 59.202297; train loss 1.350132; test accuracy 57.821429; test loss 1.384931; reg val 737.331909; time 2019-04-28 17:52:07.235626\n",
      "Epoch 35; rep 0; lambda 0.010000; train accuracy 59.354818; train loss 1.344322; test accuracy 57.951531; test loss 1.379523; reg val 730.207764; time 2019-04-28 17:52:59.777112\n",
      "Epoch 36; rep 0; lambda 0.010000; train accuracy 59.487556; train loss 1.338723; test accuracy 58.089286; test loss 1.374218; reg val 723.228088; time 2019-04-28 17:53:52.762120\n",
      "Epoch 37; rep 0; lambda 0.010000; train accuracy 59.614869; train loss 1.333320; test accuracy 58.250000; test loss 1.369019; reg val 716.260681; time 2019-04-28 17:54:45.378225\n",
      "Epoch 38; rep 0; lambda 0.010000; train accuracy 59.753669; train loss 1.328099; test accuracy 58.343112; test loss 1.363931; reg val 709.346069; time 2019-04-28 17:55:37.708883\n",
      "Epoch 39; rep 0; lambda 0.010000; train accuracy 59.878111; train loss 1.323045; test accuracy 58.517857; test loss 1.358953; reg val 702.528320; time 2019-04-28 17:56:30.304098\n",
      "Epoch 40; rep 0; lambda 0.010000; train accuracy 60.018188; train loss 1.318148; test accuracy 58.619898; test loss 1.354123; reg val 695.803528; time 2019-04-28 17:57:22.830931\n",
      "Epoch 41; rep 0; lambda 0.010000; train accuracy 60.133057; train loss 1.313397; test accuracy 58.758929; test loss 1.349420; reg val 689.270020; time 2019-04-28 17:58:15.028112\n",
      "Epoch 42; rep 0; lambda 0.010000; train accuracy 60.250160; train loss 1.308785; test accuracy 58.909439; test loss 1.344865; reg val 682.869690; time 2019-04-28 17:59:06.983366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43; rep 0; lambda 0.010000; train accuracy 60.372049; train loss 1.304305; test accuracy 59.008929; test loss 1.340481; reg val 676.550781; time 2019-04-28 17:59:59.586757\n",
      "Epoch 44; rep 0; lambda 0.010000; train accuracy 60.469687; train loss 1.299949; test accuracy 59.079082; test loss 1.336268; reg val 670.352661; time 2019-04-28 18:00:51.697043\n",
      "Epoch 45; rep 0; lambda 0.010000; train accuracy 60.586471; train loss 1.295711; test accuracy 59.195153; test loss 1.332202; reg val 664.225220; time 2019-04-28 18:01:42.955226\n",
      "Epoch 46; rep 0; lambda 0.010000; train accuracy 60.695278; train loss 1.291583; test accuracy 59.315051; test loss 1.328292; reg val 658.214539; time 2019-04-28 18:02:34.295873\n",
      "Epoch 47; rep 0; lambda 0.010000; train accuracy 60.798660; train loss 1.287564; test accuracy 59.395408; test loss 1.324531; reg val 652.291260; time 2019-04-28 18:03:27.043108\n",
      "Epoch 48; rep 0; lambda 0.010000; train accuracy 60.890236; train loss 1.283649; test accuracy 59.480867; test loss 1.320915; reg val 646.486328; time 2019-04-28 18:04:19.250132\n",
      "Epoch 49; rep 0; lambda 0.010000; train accuracy 60.998086; train loss 1.279833; test accuracy 59.565051; test loss 1.317436; reg val 640.780457; time 2019-04-28 18:05:11.701244\n",
      "Epoch 0; rep 0; lambda 0.020000; train accuracy 30.436822; train loss 2.604266; test accuracy 35.192602; test loss 2.266575; reg val 1816.923584; time 2019-04-28 18:06:03.548116\n",
      "Epoch 1; rep 0; lambda 0.020000; train accuracy 37.001276; train loss 2.247669; test accuracy 37.692602; test loss 2.165776; reg val 1025.935181; time 2019-04-28 18:06:55.935760\n",
      "Epoch 2; rep 0; lambda 0.020000; train accuracy 39.190810; train loss 2.128943; test accuracy 39.815051; test loss 2.076276; reg val 607.747070; time 2019-04-28 18:07:48.449667\n",
      "Epoch 3; rep 0; lambda 0.020000; train accuracy 41.557754; train loss 2.021424; test accuracy 41.752551; test loss 1.992840; reg val 409.147888; time 2019-04-28 18:08:40.899068\n",
      "Epoch 4; rep 0; lambda 0.020000; train accuracy 43.386088; train loss 1.939550; test accuracy 43.443878; test loss 1.927185; reg val 323.964203; time 2019-04-28 18:09:33.969948\n",
      "Epoch 5; rep 0; lambda 0.020000; train accuracy 45.021698; train loss 1.874699; test accuracy 45.073980; test loss 1.870402; reg val 292.829407; time 2019-04-28 18:10:26.319626\n",
      "Epoch 6; rep 0; lambda 0.020000; train accuracy 46.462668; train loss 1.819947; test accuracy 46.454082; test loss 1.819631; reg val 282.555328; time 2019-04-28 18:11:19.364346\n",
      "Epoch 7; rep 0; lambda 0.020000; train accuracy 47.726228; train loss 1.773725; test accuracy 47.485969; test loss 1.779407; reg val 279.935303; time 2019-04-28 18:12:11.822338\n",
      "Epoch 8; rep 0; lambda 0.020000; train accuracy 48.869177; train loss 1.734010; test accuracy 48.223214; test loss 1.748993; reg val 279.212585; time 2019-04-28 18:13:04.234969\n",
      "Epoch 9; rep 0; lambda 0.020000; train accuracy 49.842693; train loss 1.699140; test accuracy 49.005102; test loss 1.721943; reg val 278.223816; time 2019-04-28 18:13:56.228431\n",
      "Epoch 10; rep 0; lambda 0.020000; train accuracy 50.684429; train loss 1.668158; test accuracy 49.863520; test loss 1.691881; reg val 276.574371; time 2019-04-28 18:14:49.061775\n",
      "Epoch 11; rep 0; lambda 0.020000; train accuracy 51.431717; train loss 1.640430; test accuracy 50.571429; test loss 1.663835; reg val 275.006256; time 2019-04-28 18:15:41.808674\n",
      "Epoch 12; rep 0; lambda 0.020000; train accuracy 52.125718; train loss 1.615358; test accuracy 51.186224; test loss 1.639948; reg val 273.194672; time 2019-04-28 18:16:34.847704\n",
      "Epoch 13; rep 0; lambda 0.020000; train accuracy 52.738034; train loss 1.592449; test accuracy 51.751276; test loss 1.619241; reg val 270.701874; time 2019-04-28 18:17:27.970207\n",
      "Epoch 14; rep 0; lambda 0.020000; train accuracy 53.291002; train loss 1.571392; test accuracy 52.266582; test loss 1.600919; reg val 268.011017; time 2019-04-28 18:18:20.621206\n",
      "Epoch 15; rep 0; lambda 0.020000; train accuracy 53.786854; train loss 1.551971; test accuracy 52.682398; test loss 1.584156; reg val 265.380127; time 2019-04-28 18:19:13.366306\n",
      "Epoch 16; rep 0; lambda 0.020000; train accuracy 54.257817; train loss 1.534005; test accuracy 53.089286; test loss 1.568488; reg val 262.806702; time 2019-04-28 18:20:06.108251\n",
      "Epoch 17; rep 0; lambda 0.020000; train accuracy 54.699107; train loss 1.517356; test accuracy 53.463010; test loss 1.553767; reg val 259.805725; time 2019-04-28 18:20:58.536557\n",
      "Epoch 18; rep 0; lambda 0.020000; train accuracy 55.093172; train loss 1.501891; test accuracy 53.781888; test loss 1.539956; reg val 256.495911; time 2019-04-28 18:21:52.207491\n",
      "Epoch 19; rep 0; lambda 0.020000; train accuracy 55.466816; train loss 1.487492; test accuracy 54.068878; test loss 1.526875; reg val 252.837402; time 2019-04-28 18:22:45.724034\n",
      "Epoch 20; rep 0; lambda 0.020000; train accuracy 55.825782; train loss 1.474038; test accuracy 54.423469; test loss 1.514236; reg val 248.948593; time 2019-04-28 18:23:38.502896\n",
      "Epoch 21; rep 0; lambda 0.020000; train accuracy 56.159860; train loss 1.461422; test accuracy 54.739796; test loss 1.501929; reg val 244.893158; time 2019-04-28 18:24:31.205891\n",
      "Epoch 22; rep 0; lambda 0.020000; train accuracy 56.449904; train loss 1.449566; test accuracy 55.044643; test loss 1.490185; reg val 240.790375; time 2019-04-28 18:25:23.926255\n",
      "Epoch 23; rep 0; lambda 0.020000; train accuracy 56.720485; train loss 1.438413; test accuracy 55.368622; test loss 1.479282; reg val 236.595703; time 2019-04-28 18:26:17.172297\n",
      "Epoch 24; rep 0; lambda 0.020000; train accuracy 56.992980; train loss 1.427902; test accuracy 55.590561; test loss 1.469285; reg val 232.320587; time 2019-04-28 18:27:10.425872\n",
      "Epoch 25; rep 0; lambda 0.020000; train accuracy 57.224952; train loss 1.417982; test accuracy 55.830357; test loss 1.460214; reg val 228.085861; time 2019-04-28 18:28:03.374968\n",
      "Epoch 26; rep 0; lambda 0.020000; train accuracy 57.473197; train loss 1.408606; test accuracy 56.073980; test loss 1.451875; reg val 223.976624; time 2019-04-28 18:28:56.081491\n",
      "Epoch 27; rep 0; lambda 0.020000; train accuracy 57.691768; train loss 1.399733; test accuracy 56.260204; test loss 1.444112; reg val 220.014786; time 2019-04-28 18:29:48.757149\n",
      "Epoch 28; rep 0; lambda 0.020000; train accuracy 57.920230; train loss 1.391318; test accuracy 56.461735; test loss 1.436753; reg val 216.210495; time 2019-04-28 18:30:41.332451\n",
      "Epoch 29; rep 0; lambda 0.020000; train accuracy 58.127951; train loss 1.383325; test accuracy 56.664541; test loss 1.429667; reg val 212.534393; time 2019-04-28 18:31:33.861175\n",
      "Epoch 30; rep 0; lambda 0.020000; train accuracy 58.326420; train loss 1.375712; test accuracy 56.852041; test loss 1.422783; reg val 209.008759; time 2019-04-28 18:32:24.725196\n",
      "Epoch 31; rep 0; lambda 0.020000; train accuracy 58.506063; train loss 1.368447; test accuracy 57.043367; test loss 1.416128; reg val 205.710342; time 2019-04-28 18:33:16.452627\n",
      "Epoch 32; rep 0; lambda 0.020000; train accuracy 58.708360; train loss 1.361504; test accuracy 57.213010; test loss 1.409683; reg val 202.580704; time 2019-04-28 18:34:07.610879\n",
      "Epoch 33; rep 0; lambda 0.020000; train accuracy 58.867581; train loss 1.354856; test accuracy 57.339286; test loss 1.403445; reg val 199.518219; time 2019-04-28 18:34:58.727683\n",
      "Epoch 34; rep 0; lambda 0.020000; train accuracy 59.042119; train loss 1.348481; test accuracy 57.516582; test loss 1.397425; reg val 196.565552; time 2019-04-28 18:35:50.224456\n",
      "Epoch 35; rep 0; lambda 0.020000; train accuracy 59.201659; train loss 1.342356; test accuracy 57.696429; test loss 1.391633; reg val 193.729187; time 2019-04-28 18:36:41.449371\n",
      "Epoch 36; rep 0; lambda 0.020000; train accuracy 59.343969; train loss 1.336465; test accuracy 57.802296; test loss 1.386052; reg val 191.024628; time 2019-04-28 18:37:32.757107\n",
      "Epoch 37; rep 0; lambda 0.020000; train accuracy 59.493937; train loss 1.330795; test accuracy 57.941327; test loss 1.380676; reg val 188.506897; time 2019-04-28 18:38:24.172741\n",
      "Epoch 38; rep 0; lambda 0.020000; train accuracy 59.631461; train loss 1.325334; test accuracy 58.077806; test loss 1.375465; reg val 186.201538; time 2019-04-28 18:39:15.579985\n",
      "Epoch 39; rep 0; lambda 0.020000; train accuracy 59.780791; train loss 1.320067; test accuracy 58.204082; test loss 1.370411; reg val 184.047302; time 2019-04-28 18:40:06.964597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40; rep 0; lambda 0.020000; train accuracy 59.904276; train loss 1.314982; test accuracy 58.336735; test loss 1.365480; reg val 182.070770; time 2019-04-28 18:40:58.251506\n",
      "Epoch 41; rep 0; lambda 0.020000; train accuracy 60.035099; train loss 1.310068; test accuracy 58.459184; test loss 1.360657; reg val 180.159912; time 2019-04-28 18:41:49.971394\n",
      "Epoch 42; rep 0; lambda 0.020000; train accuracy 60.168794; train loss 1.305315; test accuracy 58.570153; test loss 1.355917; reg val 178.353851; time 2019-04-28 18:42:41.339263\n",
      "Epoch 43; rep 0; lambda 0.020000; train accuracy 60.289726; train loss 1.300715; test accuracy 58.667092; test loss 1.351276; reg val 176.657196; time 2019-04-28 18:43:32.686933\n",
      "Epoch 44; rep 0; lambda 0.020000; train accuracy 60.401085; train loss 1.296259; test accuracy 58.794643; test loss 1.346737; reg val 175.064972; time 2019-04-28 18:44:23.998543\n",
      "Epoch 45; rep 0; lambda 0.020000; train accuracy 60.514359; train loss 1.291938; test accuracy 58.918367; test loss 1.342319; reg val 173.578873; time 2019-04-28 18:45:15.739784\n",
      "Epoch 46; rep 0; lambda 0.020000; train accuracy 60.611040; train loss 1.287745; test accuracy 59.003827; test loss 1.338027; reg val 172.226593; time 2019-04-28 18:46:06.582446\n",
      "Epoch 47; rep 0; lambda 0.020000; train accuracy 60.725909; train loss 1.283674; test accuracy 59.095663; test loss 1.333846; reg val 170.962433; time 2019-04-28 18:46:57.925302\n",
      "Epoch 48; rep 0; lambda 0.020000; train accuracy 60.828334; train loss 1.279716; test accuracy 59.191327; test loss 1.329781; reg val 169.779999; time 2019-04-28 18:47:49.681305\n",
      "Epoch 49; rep 0; lambda 0.020000; train accuracy 60.927888; train loss 1.275867; test accuracy 59.325255; test loss 1.325835; reg val 168.663437; time 2019-04-28 18:48:40.575200\n",
      "Epoch 0; rep 0; lambda 0.030000; train accuracy 30.265475; train loss 2.640034; test accuracy 35.068878; test loss 2.271098; reg val 1286.659790; time 2019-04-28 18:49:32.048540\n",
      "Epoch 1; rep 0; lambda 0.030000; train accuracy 36.861838; train loss 2.249896; test accuracy 37.667092; test loss 2.176497; reg val 457.080444; time 2019-04-28 18:50:24.195583\n",
      "Epoch 2; rep 0; lambda 0.030000; train accuracy 38.836950; train loss 2.131993; test accuracy 39.386480; test loss 2.097062; reg val 193.498199; time 2019-04-28 18:51:17.689241\n",
      "Epoch 3; rep 0; lambda 0.030000; train accuracy 41.120294; train loss 2.029595; test accuracy 41.544643; test loss 1.999760; reg val 142.936234; time 2019-04-28 18:52:10.804931\n",
      "Epoch 4; rep 0; lambda 0.030000; train accuracy 43.137524; train loss 1.944010; test accuracy 43.349490; test loss 1.928352; reg val 133.428436; time 2019-04-28 18:53:03.479637\n",
      "Epoch 5; rep 0; lambda 0.030000; train accuracy 44.753031; train loss 1.879442; test accuracy 44.855867; test loss 1.873087; reg val 132.585205; time 2019-04-28 18:53:55.940148\n",
      "Epoch 6; rep 0; lambda 0.030000; train accuracy 46.164327; train loss 1.826075; test accuracy 46.086735; test loss 1.827000; reg val 132.509781; time 2019-04-28 18:54:48.719267\n",
      "Epoch 7; rep 0; lambda 0.030000; train accuracy 47.478941; train loss 1.780026; test accuracy 47.113520; test loss 1.788381; reg val 132.652557; time 2019-04-28 18:55:42.323664\n",
      "Epoch 8; rep 0; lambda 0.030000; train accuracy 48.616146; train loss 1.739395; test accuracy 47.977041; test loss 1.754340; reg val 133.041672; time 2019-04-28 18:56:35.082573\n",
      "Epoch 9; rep 0; lambda 0.030000; train accuracy 49.625399; train loss 1.703236; test accuracy 48.747449; test loss 1.723908; reg val 133.394653; time 2019-04-28 18:57:28.326174\n",
      "Epoch 10; rep 0; lambda 0.030000; train accuracy 50.520102; train loss 1.671140; test accuracy 49.548469; test loss 1.695792; reg val 133.264114; time 2019-04-28 18:58:21.092769\n",
      "Epoch 11; rep 0; lambda 0.030000; train accuracy 51.323548; train loss 1.642539; test accuracy 50.334184; test loss 1.669349; reg val 132.555206; time 2019-04-28 18:59:15.339385\n",
      "Epoch 12; rep 0; lambda 0.030000; train accuracy 52.022017; train loss 1.616874; test accuracy 51.068878; test loss 1.644916; reg val 131.668930; time 2019-04-28 19:00:10.434516\n",
      "Epoch 13; rep 0; lambda 0.030000; train accuracy 52.630504; train loss 1.593725; test accuracy 51.665816; test loss 1.623224; reg val 130.639008; time 2019-04-28 19:01:04.013131\n",
      "Epoch 14; rep 0; lambda 0.030000; train accuracy 53.196554; train loss 1.572760; test accuracy 52.246173; test loss 1.604409; reg val 129.437408; time 2019-04-28 19:02:01.925367\n",
      "Epoch 15; rep 0; lambda 0.030000; train accuracy 53.701978; train loss 1.553665; test accuracy 52.725765; test loss 1.587950; reg val 128.061066; time 2019-04-28 19:02:54.327805\n",
      "Epoch 16; rep 0; lambda 0.030000; train accuracy 54.172623; train loss 1.536164; test accuracy 53.079082; test loss 1.573316; reg val 126.660278; time 2019-04-28 19:03:47.112080\n",
      "Epoch 17; rep 0; lambda 0.030000; train accuracy 54.594767; train loss 1.519985; test accuracy 53.441327; test loss 1.559903; reg val 125.183083; time 2019-04-28 19:04:39.790153\n",
      "Epoch 18; rep 0; lambda 0.030000; train accuracy 55.002234; train loss 1.504899; test accuracy 53.735969; test loss 1.547377; reg val 123.677597; time 2019-04-28 19:05:32.699185\n",
      "Epoch 19; rep 0; lambda 0.030000; train accuracy 55.353223; train loss 1.490763; test accuracy 54.073980; test loss 1.535589; reg val 122.202850; time 2019-04-28 19:06:25.392082\n",
      "Epoch 20; rep 0; lambda 0.030000; train accuracy 55.694320; train loss 1.477482; test accuracy 54.352041; test loss 1.524502; reg val 120.819244; time 2019-04-28 19:07:18.063904\n",
      "Epoch 21; rep 0; lambda 0.030000; train accuracy 55.989470; train loss 1.464974; test accuracy 54.630102; test loss 1.513904; reg val 119.552963; time 2019-04-28 19:08:12.481413\n",
      "Epoch 22; rep 0; lambda 0.030000; train accuracy 56.298979; train loss 1.453163; test accuracy 54.876276; test loss 1.503679; reg val 118.394295; time 2019-04-28 19:09:05.542933\n",
      "Epoch 23; rep 0; lambda 0.030000; train accuracy 56.575303; train loss 1.441981; test accuracy 55.117347; test loss 1.493807; reg val 117.208427; time 2019-04-28 19:09:58.015967\n",
      "Epoch 24; rep 0; lambda 0.030000; train accuracy 56.841417; train loss 1.431373; test accuracy 55.360969; test loss 1.484327; reg val 116.214058; time 2019-04-28 19:10:51.103831\n",
      "Epoch 25; rep 0; lambda 0.030000; train accuracy 57.101468; train loss 1.421303; test accuracy 55.603316; test loss 1.475297; reg val 115.365883; time 2019-04-28 19:11:44.529796\n",
      "Epoch 26; rep 0; lambda 0.030000; train accuracy 57.360243; train loss 1.411733; test accuracy 55.835459; test loss 1.466657; reg val 114.636398; time 2019-04-28 19:12:39.216940\n",
      "Epoch 27; rep 0; lambda 0.030000; train accuracy 57.605616; train loss 1.402626; test accuracy 56.057398; test loss 1.458295; reg val 113.961693; time 2019-04-28 19:13:32.268682\n",
      "Epoch 28; rep 0; lambda 0.030000; train accuracy 57.824825; train loss 1.393951; test accuracy 56.267857; test loss 1.450167; reg val 113.359512; time 2019-04-28 19:14:25.528449\n",
      "Epoch 29; rep 0; lambda 0.030000; train accuracy 58.037332; train loss 1.385679; test accuracy 56.488520; test loss 1.442256; reg val 112.796257; time 2019-04-28 19:15:18.905917\n",
      "Epoch 30; rep 0; lambda 0.030000; train accuracy 58.263880; train loss 1.377789; test accuracy 56.672194; test loss 1.434576; reg val 112.254044; time 2019-04-28 19:16:12.115500\n",
      "Epoch 31; rep 0; lambda 0.030000; train accuracy 58.485003; train loss 1.370253; test accuracy 56.866071; test loss 1.427140; reg val 111.755608; time 2019-04-28 19:17:05.232701\n",
      "Epoch 32; rep 0; lambda 0.030000; train accuracy 58.678047; train loss 1.363052; test accuracy 57.099490; test loss 1.419966; reg val 111.285408; time 2019-04-28 19:17:57.729487\n",
      "Epoch 33; rep 0; lambda 0.030000; train accuracy 58.847160; train loss 1.356161; test accuracy 57.286990; test loss 1.413026; reg val 110.837418; time 2019-04-28 19:18:50.551519\n",
      "Epoch 34; rep 0; lambda 0.030000; train accuracy 59.035099; train loss 1.349559; test accuracy 57.446429; test loss 1.406282; reg val 110.459503; time 2019-04-28 19:19:43.337923\n",
      "Epoch 35; rep 0; lambda 0.030000; train accuracy 59.197830; train loss 1.343228; test accuracy 57.585459; test loss 1.399718; reg val 110.179764; time 2019-04-28 19:20:36.117718\n",
      "Epoch 36; rep 0; lambda 0.030000; train accuracy 59.345246; train loss 1.337147; test accuracy 57.770408; test loss 1.393317; reg val 109.929749; time 2019-04-28 19:21:28.717048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37; rep 0; lambda 0.030000; train accuracy 59.508296; train loss 1.331299; test accuracy 57.931122; test loss 1.387093; reg val 109.702286; time 2019-04-28 19:22:20.941207\n",
      "Epoch 38; rep 0; lambda 0.030000; train accuracy 59.650925; train loss 1.325667; test accuracy 58.067602; test loss 1.381049; reg val 109.504440; time 2019-04-28 19:23:13.543910\n",
      "Epoch 39; rep 0; lambda 0.030000; train accuracy 59.773772; train loss 1.320237; test accuracy 58.244898; test loss 1.375188; reg val 109.337654; time 2019-04-28 19:24:05.742590\n",
      "Epoch 40; rep 0; lambda 0.030000; train accuracy 59.925654; train loss 1.314997; test accuracy 58.381378; test loss 1.369506; reg val 109.224983; time 2019-04-28 19:24:57.605128\n",
      "Epoch 41; rep 0; lambda 0.030000; train accuracy 60.060944; train loss 1.309936; test accuracy 58.511480; test loss 1.364006; reg val 109.144958; time 2019-04-28 19:25:50.112977\n",
      "Epoch 42; rep 0; lambda 0.030000; train accuracy 60.185386; train loss 1.305044; test accuracy 58.672194; test loss 1.358686; reg val 109.086884; time 2019-04-28 19:26:42.890208\n",
      "Epoch 43; rep 0; lambda 0.030000; train accuracy 60.307275; train loss 1.300311; test accuracy 58.799745; test loss 1.353544; reg val 109.074036; time 2019-04-28 19:27:35.481669\n",
      "Epoch 44; rep 0; lambda 0.030000; train accuracy 60.438736; train loss 1.295728; test accuracy 58.968112; test loss 1.348563; reg val 109.094467; time 2019-04-28 19:28:28.852709\n",
      "Epoch 45; rep 0; lambda 0.030000; train accuracy 60.549138; train loss 1.291286; test accuracy 59.105867; test loss 1.343736; reg val 109.167282; time 2019-04-28 19:29:21.635887\n",
      "Epoch 46; rep 0; lambda 0.030000; train accuracy 60.662412; train loss 1.286978; test accuracy 59.195153; test loss 1.339058; reg val 109.279633; time 2019-04-28 19:30:14.099046\n",
      "Epoch 47; rep 0; lambda 0.030000; train accuracy 60.785578; train loss 1.282798; test accuracy 59.334184; test loss 1.334522; reg val 109.418304; time 2019-04-28 19:31:07.353543\n",
      "Epoch 48; rep 0; lambda 0.030000; train accuracy 60.884812; train loss 1.278738; test accuracy 59.446429; test loss 1.330125; reg val 109.577652; time 2019-04-28 19:31:58.236991\n",
      "Epoch 49; rep 0; lambda 0.030000; train accuracy 61.000957; train loss 1.274792; test accuracy 59.551020; test loss 1.325869; reg val 109.747993; time 2019-04-28 19:32:50.566785\n",
      "Epoch 0; rep 0; lambda 0.040000; train accuracy 30.228781; train loss 2.659154; test accuracy 35.040816; test loss 2.275302; reg val 862.523438; time 2019-04-28 19:33:39.495808\n",
      "Epoch 1; rep 0; lambda 0.040000; train accuracy 36.768028; train loss 2.240549; test accuracy 37.549745; test loss 2.183885; reg val 182.873688; time 2019-04-28 19:34:26.281285\n",
      "Epoch 2; rep 0; lambda 0.040000; train accuracy 38.863114; train loss 2.119280; test accuracy 39.485969; test loss 2.087361; reg val 76.733536; time 2019-04-28 19:35:13.156303\n",
      "Epoch 3; rep 0; lambda 0.040000; train accuracy 41.100830; train loss 2.024715; test accuracy 41.186224; test loss 2.009481; reg val 69.104271; time 2019-04-28 19:35:59.863656\n",
      "Epoch 4; rep 0; lambda 0.040000; train accuracy 42.927888; train loss 1.949870; test accuracy 43.020408; test loss 1.939498; reg val 70.200760; time 2019-04-28 19:36:46.653523\n",
      "Epoch 5; rep 0; lambda 0.040000; train accuracy 44.604659; train loss 1.886224; test accuracy 44.659439; test loss 1.882256; reg val 69.902550; time 2019-04-28 19:37:33.715151\n",
      "Epoch 6; rep 0; lambda 0.040000; train accuracy 46.043076; train loss 1.832835; test accuracy 45.947704; test loss 1.836298; reg val 70.291901; time 2019-04-28 19:38:20.395876\n",
      "Epoch 7; rep 0; lambda 0.040000; train accuracy 47.307594; train loss 1.786537; test accuracy 46.864796; test loss 1.800996; reg val 70.589828; time 2019-04-28 19:39:07.187876\n",
      "Epoch 8; rep 0; lambda 0.040000; train accuracy 48.431078; train loss 1.745282; test accuracy 47.873724; test loss 1.766695; reg val 70.566544; time 2019-04-28 19:39:54.185590\n",
      "Epoch 9; rep 0; lambda 0.040000; train accuracy 49.423421; train loss 1.708091; test accuracy 48.911990; test loss 1.732335; reg val 70.472885; time 2019-04-28 19:40:40.808986\n",
      "Epoch 10; rep 0; lambda 0.040000; train accuracy 50.350032; train loss 1.674643; test accuracy 49.702806; test loss 1.701752; reg val 70.423935; time 2019-04-28 19:41:27.828035\n",
      "Epoch 11; rep 0; lambda 0.040000; train accuracy 51.141672; train loss 1.644805; test accuracy 50.477041; test loss 1.674693; reg val 70.389198; time 2019-04-28 19:42:14.525517\n",
      "Epoch 12; rep 0; lambda 0.040000; train accuracy 51.843650; train loss 1.618211; test accuracy 51.082908; test loss 1.650797; reg val 70.479752; time 2019-04-28 19:43:01.396963\n",
      "Epoch 13; rep 0; lambda 0.040000; train accuracy 52.459796; train loss 1.594318; test accuracy 51.593112; test loss 1.630277; reg val 70.672516; time 2019-04-28 19:43:48.251765\n",
      "Epoch 14; rep 0; lambda 0.040000; train accuracy 53.045629; train loss 1.572596; test accuracy 52.012755; test loss 1.611967; reg val 71.105179; time 2019-04-28 19:44:35.036961\n",
      "Epoch 15; rep 0; lambda 0.040000; train accuracy 53.589343; train loss 1.552582; test accuracy 52.410714; test loss 1.594677; reg val 71.727043; time 2019-04-28 19:45:21.887770\n",
      "Epoch 16; rep 0; lambda 0.040000; train accuracy 54.063178; train loss 1.533951; test accuracy 52.864796; test loss 1.578051; reg val 72.503609; time 2019-04-28 19:46:08.475868\n",
      "Epoch 17; rep 0; lambda 0.040000; train accuracy 54.528398; train loss 1.516529; test accuracy 53.253827; test loss 1.562109; reg val 73.183472; time 2019-04-28 19:46:55.254932\n",
      "Epoch 18; rep 0; lambda 0.040000; train accuracy 54.975112; train loss 1.500252; test accuracy 53.649235; test loss 1.547010; reg val 73.712791; time 2019-04-28 19:47:42.335607\n",
      "Epoch 19; rep 0; lambda 0.040000; train accuracy 55.388960; train loss 1.485091; test accuracy 53.968112; test loss 1.532911; reg val 74.156403; time 2019-04-28 19:48:29.215999\n",
      "Epoch 20; rep 0; lambda 0.040000; train accuracy 55.751755; train loss 1.470992; test accuracy 54.289541; test loss 1.519809; reg val 74.566154; time 2019-04-28 19:49:16.302581\n",
      "Epoch 21; rep 0; lambda 0.040000; train accuracy 56.115507; train loss 1.457865; test accuracy 54.602041; test loss 1.507646; reg val 74.925995; time 2019-04-28 19:50:02.915664\n",
      "Epoch 22; rep 0; lambda 0.040000; train accuracy 56.439694; train loss 1.445606; test accuracy 54.859694; test loss 1.496318; reg val 75.247246; time 2019-04-28 19:50:49.731737\n",
      "Epoch 23; rep 0; lambda 0.040000; train accuracy 56.742502; train loss 1.434118; test accuracy 55.095663; test loss 1.485729; reg val 75.540031; time 2019-04-28 19:51:36.733255\n",
      "Epoch 24; rep 0; lambda 0.040000; train accuracy 57.029675; train loss 1.423321; test accuracy 55.390306; test loss 1.475787; reg val 75.904320; time 2019-04-28 19:52:23.604597\n",
      "Epoch 25; rep 0; lambda 0.040000; train accuracy 57.300893; train loss 1.413148; test accuracy 55.640306; test loss 1.466398; reg val 76.379356; time 2019-04-28 19:53:10.652474\n",
      "Epoch 26; rep 0; lambda 0.040000; train accuracy 57.546586; train loss 1.403541; test accuracy 55.923469; test loss 1.457520; reg val 76.853607; time 2019-04-28 19:53:57.889777\n",
      "Epoch 27; rep 0; lambda 0.040000; train accuracy 57.792916; train loss 1.394447; test accuracy 56.160714; test loss 1.449055; reg val 77.358475; time 2019-04-28 19:54:44.902094\n",
      "Epoch 28; rep 0; lambda 0.040000; train accuracy 58.007339; train loss 1.385823; test accuracy 56.382653; test loss 1.440933; reg val 77.871567; time 2019-04-28 19:55:32.134757\n",
      "Epoch 29; rep 0; lambda 0.040000; train accuracy 58.245054; train loss 1.377630; test accuracy 56.566327; test loss 1.433157; reg val 78.369232; time 2019-04-28 19:56:18.848062\n",
      "Epoch 30; rep 0; lambda 0.040000; train accuracy 58.466816; train loss 1.369835; test accuracy 56.761480; test loss 1.425714; reg val 78.894257; time 2019-04-28 19:57:05.956035\n",
      "Epoch 31; rep 0; lambda 0.040000; train accuracy 58.662412; train loss 1.362409; test accuracy 56.934949; test loss 1.418607; reg val 79.423187; time 2019-04-28 19:57:52.643799\n",
      "Epoch 32; rep 0; lambda 0.040000; train accuracy 58.851627; train loss 1.355327; test accuracy 57.112245; test loss 1.411825; reg val 79.948204; time 2019-04-28 19:58:39.407684\n",
      "Epoch 33; rep 0; lambda 0.040000; train accuracy 59.026484; train loss 1.348565; test accuracy 57.271684; test loss 1.405347; reg val 80.484383; time 2019-04-28 19:59:26.376753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34; rep 0; lambda 0.040000; train accuracy 59.216656; train loss 1.342100; test accuracy 57.448980; test loss 1.399135; reg val 81.037079; time 2019-04-28 20:00:13.217122\n",
      "Epoch 35; rep 0; lambda 0.040000; train accuracy 59.378430; train loss 1.335914; test accuracy 57.600765; test loss 1.393168; reg val 81.607048; time 2019-04-28 20:00:59.981564\n",
      "Epoch 36; rep 0; lambda 0.040000; train accuracy 59.535099; train loss 1.329989; test accuracy 57.766582; test loss 1.387405; reg val 82.194473; time 2019-04-28 20:01:46.977369\n",
      "Epoch 37; rep 0; lambda 0.040000; train accuracy 59.697192; train loss 1.324306; test accuracy 57.883929; test loss 1.381839; reg val 82.776169; time 2019-04-28 20:02:33.759570\n",
      "Epoch 38; rep 0; lambda 0.040000; train accuracy 59.834716; train loss 1.318849; test accuracy 58.008929; test loss 1.376446; reg val 83.368111; time 2019-04-28 20:03:20.904578\n",
      "Epoch 39; rep 0; lambda 0.040000; train accuracy 59.985960; train loss 1.313604; test accuracy 58.173469; test loss 1.371212; reg val 83.985085; time 2019-04-28 20:04:07.786593\n",
      "Epoch 40; rep 0; lambda 0.040000; train accuracy 60.105935; train loss 1.308555; test accuracy 58.320153; test loss 1.366124; reg val 84.585861; time 2019-04-28 20:04:54.706313\n",
      "Epoch 41; rep 0; lambda 0.040000; train accuracy 60.227505; train loss 1.303688; test accuracy 58.440051; test loss 1.361167; reg val 85.167816; time 2019-04-28 20:05:41.468584\n",
      "Epoch 42; rep 0; lambda 0.040000; train accuracy 60.345246; train loss 1.298991; test accuracy 58.507653; test loss 1.356343; reg val 85.752823; time 2019-04-28 20:06:28.306650\n",
      "Epoch 43; rep 0; lambda 0.040000; train accuracy 60.466496; train loss 1.294452; test accuracy 58.631378; test loss 1.351645; reg val 86.300049; time 2019-04-28 20:07:15.338829\n",
      "Epoch 44; rep 0; lambda 0.040000; train accuracy 60.599872; train loss 1.290061; test accuracy 58.765306; test loss 1.347071; reg val 86.846680; time 2019-04-28 20:08:02.133547\n",
      "Epoch 45; rep 0; lambda 0.040000; train accuracy 60.709955; train loss 1.285810; test accuracy 58.866071; test loss 1.342608; reg val 87.400299; time 2019-04-28 20:08:48.876189\n",
      "Epoch 46; rep 0; lambda 0.040000; train accuracy 60.826420; train loss 1.281690; test accuracy 58.979592; test loss 1.338256; reg val 87.940979; time 2019-04-28 20:09:35.785835\n",
      "Epoch 47; rep 0; lambda 0.040000; train accuracy 60.935865; train loss 1.277692; test accuracy 59.084184; test loss 1.334009; reg val 88.467178; time 2019-04-28 20:10:22.627423\n",
      "Epoch 48; rep 0; lambda 0.040000; train accuracy 61.031270; train loss 1.273810; test accuracy 59.201531; test loss 1.329866; reg val 88.990112; time 2019-04-28 20:11:09.478228\n",
      "Epoch 49; rep 0; lambda 0.040000; train accuracy 61.121251; train loss 1.270038; test accuracy 59.260204; test loss 1.325817; reg val 89.505875; time 2019-04-28 20:11:56.393092\n",
      "Epoch 0; rep 0; lambda 0.050000; train accuracy 30.119974; train loss 2.676091; test accuracy 34.965561; test loss 2.278059; reg val 538.888916; time 2019-04-28 20:12:43.308938\n",
      "Epoch 1; rep 0; lambda 0.050000; train accuracy 36.707084; train loss 2.230918; test accuracy 37.762755; test loss 2.177508; reg val 68.262794; time 2019-04-28 20:13:30.737655\n",
      "Epoch 2; rep 0; lambda 0.050000; train accuracy 38.873963; train loss 2.114820; test accuracy 39.401786; test loss 2.093613; reg val 37.232822; time 2019-04-28 20:14:17.951393\n",
      "Epoch 3; rep 0; lambda 0.050000; train accuracy 41.122208; train loss 2.021678; test accuracy 41.108418; test loss 2.014852; reg val 39.325623; time 2019-04-28 20:15:04.947456\n",
      "Epoch 4; rep 0; lambda 0.050000; train accuracy 43.021059; train loss 1.945336; test accuracy 42.994898; test loss 1.943516; reg val 40.991920; time 2019-04-28 20:15:51.761637\n",
      "Epoch 5; rep 0; lambda 0.050000; train accuracy 44.559030; train loss 1.884316; test accuracy 44.510204; test loss 1.887638; reg val 43.030914; time 2019-04-28 20:16:38.921836\n",
      "Epoch 6; rep 0; lambda 0.050000; train accuracy 45.921825; train loss 1.831693; test accuracy 45.644133; test loss 1.839841; reg val 44.210625; time 2019-04-28 20:17:26.173440\n",
      "Epoch 7; rep 0; lambda 0.050000; train accuracy 47.211551; train loss 1.785504; test accuracy 46.822704; test loss 1.795602; reg val 44.966370; time 2019-04-28 20:18:12.989186\n",
      "Epoch 8; rep 0; lambda 0.050000; train accuracy 48.373644; train loss 1.744845; test accuracy 47.785714; test loss 1.756252; reg val 45.808167; time 2019-04-28 20:18:59.821003\n",
      "Epoch 9; rep 0; lambda 0.050000; train accuracy 49.366943; train loss 1.708582; test accuracy 48.704082; test loss 1.721932; reg val 46.725258; time 2019-04-28 20:19:46.716862\n",
      "Epoch 10; rep 0; lambda 0.050000; train accuracy 50.250798; train loss 1.676074; test accuracy 49.589286; test loss 1.689939; reg val 47.645157; time 2019-04-28 20:20:33.531382\n",
      "Epoch 11; rep 0; lambda 0.050000; train accuracy 51.062540; train loss 1.646869; test accuracy 50.468112; test loss 1.662093; reg val 48.557217; time 2019-04-28 20:21:20.440643\n",
      "Epoch 12; rep 0; lambda 0.050000; train accuracy 51.796426; train loss 1.620439; test accuracy 51.123724; test loss 1.639452; reg val 49.570122; time 2019-04-28 20:22:07.487252\n",
      "Epoch 13; rep 0; lambda 0.050000; train accuracy 52.470645; train loss 1.596255; test accuracy 51.658163; test loss 1.620637; reg val 50.581345; time 2019-04-28 20:22:54.455224\n",
      "Epoch 14; rep 0; lambda 0.050000; train accuracy 53.055201; train loss 1.574026; test accuracy 52.178571; test loss 1.604297; reg val 51.536278; time 2019-04-28 20:23:41.383951\n",
      "Epoch 15; rep 0; lambda 0.050000; train accuracy 53.604659; train loss 1.553611; test accuracy 52.641582; test loss 1.589362; reg val 52.352264; time 2019-04-28 20:24:28.400795\n",
      "Epoch 16; rep 0; lambda 0.050000; train accuracy 54.098596; train loss 1.534823; test accuracy 53.057398; test loss 1.574988; reg val 53.126339; time 2019-04-28 20:25:15.668538\n",
      "Epoch 17; rep 0; lambda 0.050000; train accuracy 54.552967; train loss 1.517434; test accuracy 53.466837; test loss 1.560726; reg val 53.952160; time 2019-04-28 20:26:03.079631\n",
      "Epoch 18; rep 0; lambda 0.050000; train accuracy 54.988513; train loss 1.501264; test accuracy 53.801020; test loss 1.546621; reg val 54.781471; time 2019-04-28 20:26:50.207513\n",
      "Epoch 19; rep 0; lambda 0.050000; train accuracy 55.393427; train loss 1.486227; test accuracy 54.147959; test loss 1.532905; reg val 55.648796; time 2019-04-28 20:27:37.276937\n",
      "Epoch 20; rep 0; lambda 0.050000; train accuracy 55.743778; train loss 1.472266; test accuracy 54.471939; test loss 1.519810; reg val 56.573265; time 2019-04-28 20:28:24.069232\n",
      "Epoch 21; rep 0; lambda 0.050000; train accuracy 56.085833; train loss 1.459302; test accuracy 54.812500; test loss 1.507522; reg val 57.511002; time 2019-04-28 20:29:11.608101\n",
      "Epoch 22; rep 0; lambda 0.050000; train accuracy 56.402680; train loss 1.447239; test accuracy 55.102041; test loss 1.496163; reg val 58.444477; time 2019-04-28 20:29:58.415731\n",
      "Epoch 23; rep 0; lambda 0.050000; train accuracy 56.687620; train loss 1.435973; test accuracy 55.437500; test loss 1.485779; reg val 59.318260; time 2019-04-28 20:30:45.296943\n",
      "Epoch 24; rep 0; lambda 0.050000; train accuracy 56.970964; train loss 1.425401; test accuracy 55.663265; test loss 1.476350; reg val 60.131573; time 2019-04-28 20:31:32.414468\n",
      "Epoch 25; rep 0; lambda 0.050000; train accuracy 57.242502; train loss 1.415434; test accuracy 55.840561; test loss 1.467793; reg val 60.935036; time 2019-04-28 20:32:19.251169\n",
      "Epoch 26; rep 0; lambda 0.050000; train accuracy 57.474154; train loss 1.406001; test accuracy 56.019133; test loss 1.459986; reg val 61.683357; time 2019-04-28 20:33:06.294390\n",
      "Epoch 27; rep 0; lambda 0.050000; train accuracy 57.719528; train loss 1.397048; test accuracy 56.232143; test loss 1.452749; reg val 62.397781; time 2019-04-28 20:33:53.113612\n",
      "Epoch 28; rep 0; lambda 0.050000; train accuracy 57.941608; train loss 1.388538; test accuracy 56.405612; test loss 1.445897; reg val 63.124405; time 2019-04-28 20:34:40.006753\n",
      "Epoch 29; rep 0; lambda 0.050000; train accuracy 58.165922; train loss 1.380437; test accuracy 56.572704; test loss 1.439282; reg val 63.835697; time 2019-04-28 20:35:26.983220\n",
      "Epoch 30; rep 0; lambda 0.050000; train accuracy 58.371091; train loss 1.372719; test accuracy 56.723214; test loss 1.432807; reg val 64.517166; time 2019-04-28 20:36:13.980241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31; rep 0; lambda 0.050000; train accuracy 58.571474; train loss 1.365359; test accuracy 56.858418; test loss 1.426437; reg val 65.172691; time 2019-04-28 20:37:00.705913\n",
      "Epoch 32; rep 0; lambda 0.050000; train accuracy 58.756860; train loss 1.358335; test accuracy 56.997449; test loss 1.420133; reg val 65.813850; time 2019-04-28 20:37:47.602524\n",
      "Epoch 33; rep 0; lambda 0.050000; train accuracy 58.942565; train loss 1.351624; test accuracy 57.165816; test loss 1.413878; reg val 66.446503; time 2019-04-28 20:38:34.553863\n",
      "Epoch 34; rep 0; lambda 0.050000; train accuracy 59.126037; train loss 1.345203; test accuracy 57.354592; test loss 1.407655; reg val 67.030228; time 2019-04-28 20:39:21.712794\n",
      "Epoch 35; rep 0; lambda 0.050000; train accuracy 59.296426; train loss 1.339051; test accuracy 57.506378; test loss 1.401459; reg val 67.601967; time 2019-04-28 20:40:08.437527\n",
      "Epoch 36; rep 0; lambda 0.050000; train accuracy 59.439375; train loss 1.333146; test accuracy 57.642857; test loss 1.395285; reg val 68.164856; time 2019-04-28 20:40:55.358891\n",
      "Epoch 37; rep 0; lambda 0.050000; train accuracy 59.577218; train loss 1.327470; test accuracy 57.776786; test loss 1.389144; reg val 68.708389; time 2019-04-28 20:41:42.467719\n",
      "Epoch 38; rep 0; lambda 0.050000; train accuracy 59.734844; train loss 1.322007; test accuracy 57.936224; test loss 1.383045; reg val 69.245857; time 2019-04-28 20:42:29.258016\n",
      "Epoch 39; rep 0; lambda 0.050000; train accuracy 59.882897; train loss 1.316742; test accuracy 58.086735; test loss 1.377014; reg val 69.794098; time 2019-04-28 20:43:16.407340\n",
      "Epoch 40; rep 0; lambda 0.050000; train accuracy 60.016911; train loss 1.311663; test accuracy 58.250000; test loss 1.371075; reg val 70.338516; time 2019-04-28 20:44:03.389346\n",
      "Epoch 41; rep 0; lambda 0.050000; train accuracy 60.135609; train loss 1.306761; test accuracy 58.376276; test loss 1.365274; reg val 70.863739; time 2019-04-28 20:44:50.222910\n",
      "Epoch 42; rep 0; lambda 0.050000; train accuracy 60.254946; train loss 1.302026; test accuracy 58.530612; test loss 1.359646; reg val 71.379684; time 2019-04-28 20:45:37.359862\n",
      "Epoch 43; rep 0; lambda 0.050000; train accuracy 60.389598; train loss 1.297447; test accuracy 58.647959; test loss 1.354214; reg val 71.868866; time 2019-04-28 20:46:24.195656\n",
      "Epoch 44; rep 0; lambda 0.050000; train accuracy 60.509892; train loss 1.293019; test accuracy 58.770408; test loss 1.349011; reg val 72.362198; time 2019-04-28 20:47:10.926774\n",
      "Epoch 45; rep 0; lambda 0.050000; train accuracy 60.614231; train loss 1.288733; test accuracy 58.878827; test loss 1.344046; reg val 72.848419; time 2019-04-28 20:47:57.930710\n",
      "Epoch 46; rep 0; lambda 0.050000; train accuracy 60.737077; train loss 1.284583; test accuracy 58.992347; test loss 1.339329; reg val 73.312134; time 2019-04-28 20:48:44.721697\n",
      "Epoch 47; rep 0; lambda 0.050000; train accuracy 60.846522; train loss 1.280563; test accuracy 59.102041; test loss 1.334863; reg val 73.762497; time 2019-04-28 20:49:31.680492\n",
      "Epoch 48; rep 0; lambda 0.050000; train accuracy 60.957243; train loss 1.276665; test accuracy 59.205357; test loss 1.330652; reg val 74.185410; time 2019-04-28 20:50:18.605707\n",
      "Epoch 49; rep 0; lambda 0.050000; train accuracy 61.064454; train loss 1.272884; test accuracy 59.286990; test loss 1.326677; reg val 74.601181; time 2019-04-28 20:51:05.548742\n",
      "Epoch 0; rep 0; lambda 0.060000; train accuracy 29.921825; train loss 2.691810; test accuracy 34.876276; test loss 2.284462; reg val 324.536285; time 2019-04-28 20:51:52.122558\n",
      "Epoch 1; rep 0; lambda 0.060000; train accuracy 36.591895; train loss 2.227164; test accuracy 37.576531; test loss 2.187604; reg val 30.317644; time 2019-04-28 20:52:38.879880\n",
      "Epoch 2; rep 0; lambda 0.060000; train accuracy 38.579451; train loss 2.126842; test accuracy 38.993622; test loss 2.108786; reg val 25.745327; time 2019-04-28 20:53:25.907728\n",
      "Epoch 3; rep 0; lambda 0.060000; train accuracy 40.645182; train loss 2.037647; test accuracy 41.028061; test loss 2.019437; reg val 33.927330; time 2019-04-28 20:54:12.704032\n",
      "Epoch 4; rep 0; lambda 0.060000; train accuracy 42.738354; train loss 1.952220; test accuracy 42.835459; test loss 1.948973; reg val 35.563072; time 2019-04-28 20:54:59.638411\n",
      "Epoch 5; rep 0; lambda 0.060000; train accuracy 44.420230; train loss 1.885484; test accuracy 44.494898; test loss 1.886304; reg val 36.429722; time 2019-04-28 20:55:46.710105\n",
      "Epoch 6; rep 0; lambda 0.060000; train accuracy 45.959796; train loss 1.830874; test accuracy 45.966837; test loss 1.836145; reg val 38.081238; time 2019-04-28 20:56:33.489008\n",
      "Epoch 7; rep 0; lambda 0.060000; train accuracy 47.298341; train loss 1.783198; test accuracy 47.017857; test loss 1.794465; reg val 39.799210; time 2019-04-28 20:57:23.192564\n",
      "Epoch 8; rep 0; lambda 0.060000; train accuracy 48.476707; train loss 1.741007; test accuracy 48.034439; test loss 1.757492; reg val 41.057709; time 2019-04-28 20:58:14.962246\n",
      "Epoch 9; rep 0; lambda 0.060000; train accuracy 49.485641; train loss 1.703851; test accuracy 48.815051; test loss 1.725071; reg val 42.167404; time 2019-04-28 20:59:06.648893\n",
      "Epoch 10; rep 0; lambda 0.060000; train accuracy 50.334716; train loss 1.671108; test accuracy 49.631378; test loss 1.693299; reg val 43.166519; time 2019-04-28 20:59:58.865590\n",
      "Epoch 11; rep 0; lambda 0.060000; train accuracy 51.110721; train loss 1.641832; test accuracy 50.489796; test loss 1.662149; reg val 43.956291; time 2019-04-28 21:00:50.583392\n",
      "Epoch 12; rep 0; lambda 0.060000; train accuracy 51.785897; train loss 1.615465; test accuracy 51.290816; test loss 1.635127; reg val 44.692001; time 2019-04-28 21:01:42.865982\n",
      "Epoch 13; rep 0; lambda 0.060000; train accuracy 52.436184; train loss 1.591649; test accuracy 51.989796; test loss 1.612316; reg val 45.314156; time 2019-04-28 21:02:34.869401\n",
      "Epoch 14; rep 0; lambda 0.060000; train accuracy 53.056477; train loss 1.570033; test accuracy 52.577806; test loss 1.592767; reg val 45.911438; time 2019-04-28 21:03:27.023006\n",
      "Epoch 15; rep 0; lambda 0.060000; train accuracy 53.584556; train loss 1.550321; test accuracy 53.073980; test loss 1.575367; reg val 46.516243; time 2019-04-28 21:04:18.818479\n",
      "Epoch 16; rep 0; lambda 0.060000; train accuracy 54.095405; train loss 1.532228; test accuracy 53.535714; test loss 1.559051; reg val 47.175652; time 2019-04-28 21:05:10.709578\n",
      "Epoch 17; rep 0; lambda 0.060000; train accuracy 54.534142; train loss 1.515473; test accuracy 53.926020; test loss 1.543214; reg val 47.853455; time 2019-04-28 21:06:02.900510\n",
      "Epoch 18; rep 0; lambda 0.060000; train accuracy 54.941927; train loss 1.499869; test accuracy 54.323980; test loss 1.527938; reg val 48.567703; time 2019-04-28 21:06:54.632682\n",
      "Epoch 19; rep 0; lambda 0.060000; train accuracy 55.334397; train loss 1.485319; test accuracy 54.695153; test loss 1.513364; reg val 49.308968; time 2019-04-28 21:07:46.767944\n",
      "Epoch 20; rep 0; lambda 0.060000; train accuracy 55.704531; train loss 1.471742; test accuracy 54.961735; test loss 1.499540; reg val 50.076008; time 2019-04-28 21:08:38.785256\n",
      "Epoch 21; rep 0; lambda 0.060000; train accuracy 56.039885; train loss 1.459049; test accuracy 55.318878; test loss 1.486439; reg val 50.868237; time 2019-04-28 21:09:30.898612\n",
      "Epoch 22; rep 0; lambda 0.060000; train accuracy 56.363752; train loss 1.447149; test accuracy 55.613520; test loss 1.473992; reg val 51.684975; time 2019-04-28 21:10:22.738299\n",
      "Epoch 23; rep 0; lambda 0.060000; train accuracy 56.653478; train loss 1.435951; test accuracy 55.896684; test loss 1.462185; reg val 52.526108; time 2019-04-28 21:11:14.874462\n",
      "Epoch 24; rep 0; lambda 0.060000; train accuracy 56.921187; train loss 1.425381; test accuracy 56.132653; test loss 1.451068; reg val 53.412567; time 2019-04-28 21:12:07.171439\n",
      "Epoch 25; rep 0; lambda 0.060000; train accuracy 57.171985; train loss 1.415378; test accuracy 56.399235; test loss 1.440692; reg val 54.350883; time 2019-04-28 21:12:59.260592\n",
      "Epoch 26; rep 0; lambda 0.060000; train accuracy 57.441608; train loss 1.405891; test accuracy 56.639031; test loss 1.431104; reg val 55.305855; time 2019-04-28 21:13:51.905637\n",
      "Epoch 27; rep 0; lambda 0.060000; train accuracy 57.683153; train loss 1.396877; test accuracy 56.906888; test loss 1.422279; reg val 56.289909; time 2019-04-28 21:14:43.806074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28; rep 0; lambda 0.060000; train accuracy 57.902999; train loss 1.388301; test accuracy 57.114796; test loss 1.414150; reg val 57.244934; time 2019-04-28 21:15:35.581483\n",
      "Epoch 29; rep 0; lambda 0.060000; train accuracy 58.133695; train loss 1.380129; test accuracy 57.306122; test loss 1.406652; reg val 58.180878; time 2019-04-28 21:16:22.590017\n",
      "Epoch 30; rep 0; lambda 0.060000; train accuracy 58.339502; train loss 1.372335; test accuracy 57.503827; test loss 1.399690; reg val 59.109585; time 2019-04-28 21:17:09.588270\n",
      "Epoch 31; rep 0; lambda 0.060000; train accuracy 58.556158; train loss 1.364894; test accuracy 57.678571; test loss 1.393178; reg val 60.014732; time 2019-04-28 21:17:56.481905\n",
      "Epoch 32; rep 0; lambda 0.060000; train accuracy 58.755584; train loss 1.357784; test accuracy 57.857143; test loss 1.387060; reg val 60.867790; time 2019-04-28 21:18:43.381115\n",
      "Epoch 33; rep 0; lambda 0.060000; train accuracy 58.944480; train loss 1.350985; test accuracy 58.012755; test loss 1.381277; reg val 61.695015; time 2019-04-28 21:19:30.582116\n",
      "Epoch 34; rep 0; lambda 0.060000; train accuracy 59.099553; train loss 1.344476; test accuracy 58.122449; test loss 1.375788; reg val 62.503098; time 2019-04-28 21:20:17.565174\n",
      "Epoch 35; rep 0; lambda 0.060000; train accuracy 59.278558; train loss 1.338243; test accuracy 58.267857; test loss 1.370563; reg val 63.276768; time 2019-04-28 21:21:04.634754\n",
      "Epoch 36; rep 0; lambda 0.060000; train accuracy 59.440332; train loss 1.332267; test accuracy 58.387755; test loss 1.365579; reg val 63.999928; time 2019-04-28 21:21:51.543634\n",
      "Epoch 37; rep 0; lambda 0.060000; train accuracy 59.609764; train loss 1.326531; test accuracy 58.530612; test loss 1.360813; reg val 64.691536; time 2019-04-28 21:22:38.468181\n",
      "Epoch 38; rep 0; lambda 0.060000; train accuracy 59.753989; train loss 1.321018; test accuracy 58.618622; test loss 1.356244; reg val 65.390472; time 2019-04-28 21:23:25.509479\n",
      "Epoch 39; rep 0; lambda 0.060000; train accuracy 59.880664; train loss 1.315715; test accuracy 58.732143; test loss 1.351842; reg val 66.039139; time 2019-04-28 21:24:12.295416\n",
      "Epoch 40; rep 0; lambda 0.060000; train accuracy 60.031908; train loss 1.310606; test accuracy 58.857143; test loss 1.347579; reg val 66.667175; time 2019-04-28 21:24:59.059349\n",
      "Epoch 41; rep 0; lambda 0.060000; train accuracy 60.168794; train loss 1.305680; test accuracy 58.974490; test loss 1.343434; reg val 67.296280; time 2019-04-28 21:25:46.047516\n",
      "Epoch 42; rep 0; lambda 0.060000; train accuracy 60.301851; train loss 1.300923; test accuracy 59.067602; test loss 1.339382; reg val 67.888855; time 2019-04-28 21:26:32.831324\n",
      "Epoch 43; rep 0; lambda 0.060000; train accuracy 60.419272; train loss 1.296325; test accuracy 59.161990; test loss 1.335410; reg val 68.488419; time 2019-04-28 21:27:19.881273\n",
      "Epoch 44; rep 0; lambda 0.060000; train accuracy 60.538290; train loss 1.291875; test accuracy 59.292092; test loss 1.331502; reg val 69.053421; time 2019-04-28 21:28:06.750301\n",
      "Epoch 45; rep 0; lambda 0.060000; train accuracy 60.647096; train loss 1.287567; test accuracy 59.358418; test loss 1.327652; reg val 69.597733; time 2019-04-28 21:28:53.613967\n",
      "Epoch 46; rep 0; lambda 0.060000; train accuracy 60.754946; train loss 1.283390; test accuracy 59.448980; test loss 1.323856; reg val 70.127655; time 2019-04-28 21:29:40.484121\n",
      "Epoch 47; rep 0; lambda 0.060000; train accuracy 60.865029; train loss 1.279339; test accuracy 59.547194; test loss 1.320111; reg val 70.645546; time 2019-04-28 21:30:27.428986\n",
      "Epoch 48; rep 0; lambda 0.060000; train accuracy 60.968730; train loss 1.275407; test accuracy 59.627551; test loss 1.316419; reg val 71.124809; time 2019-04-28 21:31:14.279615\n",
      "Epoch 49; rep 0; lambda 0.060000; train accuracy 61.076260; train loss 1.271588; test accuracy 59.743622; test loss 1.312780; reg val 71.590012; time 2019-04-28 21:32:01.625517\n",
      "Epoch 0; rep 0; lambda 0.070000; train accuracy 29.942565; train loss 2.691421; test accuracy 34.878827; test loss 2.285351; reg val 184.556915; time 2019-04-28 21:32:48.320754\n",
      "Epoch 1; rep 0; lambda 0.070000; train accuracy 36.533823; train loss 2.220868; test accuracy 37.558673; test loss 2.182917; reg val 18.116890; time 2019-04-28 21:33:35.473251\n",
      "Epoch 2; rep 0; lambda 0.070000; train accuracy 38.664327; train loss 2.121883; test accuracy 38.969388; test loss 2.111714; reg val 19.141005; time 2019-04-28 21:34:22.305309\n",
      "Epoch 3; rep 0; lambda 0.070000; train accuracy 40.776643; train loss 2.033474; test accuracy 41.207908; test loss 2.013356; reg val 23.802633; time 2019-04-28 21:35:09.447442\n",
      "Epoch 4; rep 0; lambda 0.070000; train accuracy 42.722399; train loss 1.953522; test accuracy 43.095663; test loss 1.938069; reg val 25.939775; time 2019-04-28 21:35:56.320456\n",
      "Epoch 5; rep 0; lambda 0.070000; train accuracy 44.447033; train loss 1.887284; test accuracy 44.822704; test loss 1.877046; reg val 27.871466; time 2019-04-28 21:36:43.158033\n",
      "Epoch 6; rep 0; lambda 0.070000; train accuracy 45.965539; train loss 1.831972; test accuracy 46.048469; test loss 1.828318; reg val 29.600166; time 2019-04-28 21:37:30.267528\n",
      "Epoch 7; rep 0; lambda 0.070000; train accuracy 47.276005; train loss 1.785377; test accuracy 47.289541; test loss 1.782782; reg val 31.376301; time 2019-04-28 21:38:17.239486\n",
      "Epoch 8; rep 0; lambda 0.070000; train accuracy 48.373963; train loss 1.744123; test accuracy 48.431122; test loss 1.742769; reg val 33.377186; time 2019-04-28 21:39:04.130001\n",
      "Epoch 9; rep 0; lambda 0.070000; train accuracy 49.363114; train loss 1.706597; test accuracy 49.228316; test loss 1.708483; reg val 35.111214; time 2019-04-28 21:39:51.171121\n",
      "Epoch 10; rep 0; lambda 0.070000; train accuracy 50.292916; train loss 1.672671; test accuracy 49.920918; test loss 1.680496; reg val 36.501335; time 2019-04-28 21:40:38.028816\n",
      "Epoch 11; rep 0; lambda 0.070000; train accuracy 51.136248; train loss 1.642161; test accuracy 50.516582; test loss 1.657252; reg val 37.667561; time 2019-04-28 21:41:24.967589\n",
      "Epoch 12; rep 0; lambda 0.070000; train accuracy 51.878111; train loss 1.614754; test accuracy 51.221939; test loss 1.634470; reg val 38.864929; time 2019-04-28 21:42:11.853035\n",
      "Epoch 13; rep 0; lambda 0.070000; train accuracy 52.577856; train loss 1.590075; test accuracy 51.883929; test loss 1.610832; reg val 40.066113; time 2019-04-28 21:42:58.852030\n",
      "Epoch 14; rep 0; lambda 0.070000; train accuracy 53.210913; train loss 1.567744; test accuracy 52.485969; test loss 1.588718; reg val 41.144573; time 2019-04-28 21:43:46.000722\n",
      "Epoch 15; rep 0; lambda 0.070000; train accuracy 53.749202; train loss 1.547420; test accuracy 53.051020; test loss 1.569070; reg val 42.226822; time 2019-04-28 21:44:32.804780\n",
      "Epoch 16; rep 0; lambda 0.070000; train accuracy 54.242821; train loss 1.528808; test accuracy 53.526786; test loss 1.551550; reg val 43.345688; time 2019-04-28 21:45:19.913436\n",
      "Epoch 17; rep 0; lambda 0.070000; train accuracy 54.702616; train loss 1.511670; test accuracy 53.927296; test loss 1.535758; reg val 44.453487; time 2019-04-28 21:46:06.795468\n",
      "Epoch 18; rep 0; lambda 0.070000; train accuracy 55.104659; train loss 1.495815; test accuracy 54.323980; test loss 1.521419; reg val 45.517586; time 2019-04-28 21:46:53.499779\n",
      "Epoch 19; rep 0; lambda 0.070000; train accuracy 55.492980; train loss 1.481088; test accuracy 54.691327; test loss 1.508269; reg val 46.565918; time 2019-04-28 21:47:40.673180\n",
      "Epoch 20; rep 0; lambda 0.070000; train accuracy 55.890555; train loss 1.467364; test accuracy 54.966837; test loss 1.496070; reg val 47.575165; time 2019-04-28 21:48:27.573739\n",
      "Epoch 21; rep 0; lambda 0.070000; train accuracy 56.237077; train loss 1.454545; test accuracy 55.288265; test loss 1.484614; reg val 48.553787; time 2019-04-28 21:49:14.831616\n",
      "Epoch 22; rep 0; lambda 0.070000; train accuracy 56.546586; train loss 1.442542; test accuracy 55.524235; test loss 1.473705; reg val 49.514847; time 2019-04-28 21:50:01.839634\n",
      "Epoch 23; rep 0; lambda 0.070000; train accuracy 56.838864; train loss 1.431264; test accuracy 55.790816; test loss 1.463190; reg val 50.494377; time 2019-04-28 21:50:48.930503\n",
      "Epoch 24; rep 0; lambda 0.070000; train accuracy 57.131461; train loss 1.420636; test accuracy 56.044643; test loss 1.453111; reg val 51.427147; time 2019-04-28 21:51:35.914540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25; rep 0; lambda 0.070000; train accuracy 57.376835; train loss 1.410600; test accuracy 56.304847; test loss 1.443549; reg val 52.318031; time 2019-04-28 21:52:22.916211\n",
      "Epoch 26; rep 0; lambda 0.070000; train accuracy 57.629228; train loss 1.401115; test accuracy 56.525510; test loss 1.434585; reg val 53.183735; time 2019-04-28 21:53:09.866600\n",
      "Epoch 27; rep 0; lambda 0.070000; train accuracy 57.873963; train loss 1.392145; test accuracy 56.761480; test loss 1.426238; reg val 54.012604; time 2019-04-28 21:53:56.943369\n",
      "Epoch 28; rep 0; lambda 0.070000; train accuracy 58.125080; train loss 1.383656; test accuracy 56.923469; test loss 1.418487; reg val 54.821777; time 2019-04-28 21:54:43.883011\n",
      "Epoch 29; rep 0; lambda 0.070000; train accuracy 58.333440; train loss 1.375613; test accuracy 57.103316; test loss 1.411272; reg val 55.598255; time 2019-04-28 21:55:31.121886\n",
      "Epoch 30; rep 0; lambda 0.070000; train accuracy 58.529036; train loss 1.367980; test accuracy 57.336735; test loss 1.404480; reg val 56.375854; time 2019-04-28 21:56:18.020984\n",
      "Epoch 31; rep 0; lambda 0.070000; train accuracy 58.727824; train loss 1.360724; test accuracy 57.515306; test loss 1.397986; reg val 57.099075; time 2019-04-28 21:57:05.199898\n",
      "Epoch 32; rep 0; lambda 0.070000; train accuracy 58.921825; train loss 1.353815; test accuracy 57.660714; test loss 1.391710; reg val 57.797523; time 2019-04-28 21:57:51.828901\n",
      "Epoch 33; rep 0; lambda 0.070000; train accuracy 59.109764; train loss 1.347225; test accuracy 57.820153; test loss 1.385567; reg val 58.478363; time 2019-04-28 21:58:39.052645\n",
      "Epoch 34; rep 0; lambda 0.070000; train accuracy 59.286535; train loss 1.340928; test accuracy 57.983418; test loss 1.379541; reg val 59.111397; time 2019-04-28 21:59:26.133278\n",
      "Epoch 35; rep 0; lambda 0.070000; train accuracy 59.456605; train loss 1.334903; test accuracy 58.130102; test loss 1.373646; reg val 59.711395; time 2019-04-28 22:00:13.221330\n",
      "Epoch 36; rep 0; lambda 0.070000; train accuracy 59.612955; train loss 1.329129; test accuracy 58.265306; test loss 1.367907; reg val 60.287399; time 2019-04-28 22:01:00.203663\n",
      "Epoch 37; rep 0; lambda 0.070000; train accuracy 59.758456; train loss 1.323590; test accuracy 58.405612; test loss 1.362343; reg val 60.856701; time 2019-04-28 22:01:47.295724\n",
      "Epoch 38; rep 0; lambda 0.070000; train accuracy 59.904914; train loss 1.318268; test accuracy 58.558673; test loss 1.356986; reg val 61.400051; time 2019-04-28 22:02:34.465648\n",
      "Epoch 39; rep 0; lambda 0.070000; train accuracy 60.042119; train loss 1.313149; test accuracy 58.691327; test loss 1.351850; reg val 61.931648; time 2019-04-28 22:03:21.631117\n",
      "Epoch 40; rep 0; lambda 0.070000; train accuracy 60.166560; train loss 1.308220; test accuracy 58.845663; test loss 1.346955; reg val 62.432472; time 2019-04-28 22:04:08.424447\n",
      "Epoch 41; rep 0; lambda 0.070000; train accuracy 60.303765; train loss 1.303468; test accuracy 58.969388; test loss 1.342291; reg val 62.926769; time 2019-04-28 22:04:55.467694\n",
      "Epoch 42; rep 0; lambda 0.070000; train accuracy 60.425016; train loss 1.298880; test accuracy 59.104592; test loss 1.337864; reg val 63.395592; time 2019-04-28 22:05:42.502361\n",
      "Epoch 43; rep 0; lambda 0.070000; train accuracy 60.544033; train loss 1.294446; test accuracy 59.197704; test loss 1.333658; reg val 63.843834; time 2019-04-28 22:06:29.440038\n",
      "Epoch 44; rep 0; lambda 0.070000; train accuracy 60.653478; train loss 1.290155; test accuracy 59.325255; test loss 1.329665; reg val 64.291542; time 2019-04-28 22:07:16.314362\n",
      "Epoch 45; rep 0; lambda 0.070000; train accuracy 60.765795; train loss 1.285999; test accuracy 59.446429; test loss 1.325882; reg val 64.700439; time 2019-04-28 22:08:03.388575\n",
      "Epoch 46; rep 0; lambda 0.070000; train accuracy 60.880664; train loss 1.281969; test accuracy 59.563776; test loss 1.322297; reg val 65.121796; time 2019-04-28 22:08:50.286599\n",
      "Epoch 47; rep 0; lambda 0.070000; train accuracy 60.985322; train loss 1.278056; test accuracy 59.625000; test loss 1.318899; reg val 65.506256; time 2019-04-28 22:09:37.473882\n",
      "Epoch 48; rep 0; lambda 0.070000; train accuracy 61.097320; train loss 1.274254; test accuracy 59.715561; test loss 1.315674; reg val 65.892944; time 2019-04-28 22:10:24.323368\n",
      "Epoch 49; rep 0; lambda 0.070000; train accuracy 61.191768; train loss 1.270556; test accuracy 59.809949; test loss 1.312620; reg val 66.263428; time 2019-04-28 22:11:11.433890\n",
      "Epoch 0; rep 0; lambda 0.080000; train accuracy 29.932993; train loss 2.694315; test accuracy 35.033163; test loss 2.288930; reg val 96.883156; time 2019-04-28 22:11:58.221648\n",
      "Epoch 1; rep 0; lambda 0.080000; train accuracy 36.520102; train loss 2.218862; test accuracy 37.503827; test loss 2.182136; reg val 14.227283; time 2019-04-28 22:12:45.144444\n",
      "Epoch 2; rep 0; lambda 0.080000; train accuracy 38.791321; train loss 2.116490; test accuracy 39.051020; test loss 2.099574; reg val 18.577658; time 2019-04-28 22:13:32.196378\n",
      "Epoch 3; rep 0; lambda 0.080000; train accuracy 40.925335; train loss 2.027108; test accuracy 41.125000; test loss 2.017360; reg val 21.146862; time 2019-04-28 22:14:19.228782\n",
      "Epoch 4; rep 0; lambda 0.080000; train accuracy 42.877154; train loss 1.949250; test accuracy 42.788265; test loss 1.947216; reg val 22.522949; time 2019-04-28 22:15:06.362567\n",
      "Epoch 5; rep 0; lambda 0.080000; train accuracy 44.548181; train loss 1.882266; test accuracy 44.522959; test loss 1.880758; reg val 24.393879; time 2019-04-28 22:15:53.466273\n",
      "Epoch 6; rep 0; lambda 0.080000; train accuracy 46.077856; train loss 1.825913; test accuracy 46.088010; test loss 1.825954; reg val 26.235743; time 2019-04-28 22:16:40.474392\n",
      "Epoch 7; rep 0; lambda 0.080000; train accuracy 47.425654; train loss 1.778386; test accuracy 47.258929; test loss 1.782533; reg val 27.603762; time 2019-04-28 22:17:27.697577\n",
      "Epoch 8; rep 0; lambda 0.080000; train accuracy 48.493937; train loss 1.737654; test accuracy 48.183673; test loss 1.747041; reg val 28.856049; time 2019-04-28 22:18:14.686597\n",
      "Epoch 9; rep 0; lambda 0.080000; train accuracy 49.470325; train loss 1.701749; test accuracy 49.122449; test loss 1.715723; reg val 30.131981; time 2019-04-28 22:19:01.780499\n",
      "Epoch 10; rep 0; lambda 0.080000; train accuracy 50.353861; train loss 1.669455; test accuracy 50.029337; test loss 1.683703; reg val 31.380405; time 2019-04-28 22:19:48.859387\n",
      "Epoch 11; rep 0; lambda 0.080000; train accuracy 51.158583; train loss 1.640181; test accuracy 50.954082; test loss 1.652831; reg val 32.626244; time 2019-04-28 22:20:35.882996\n",
      "Epoch 12; rep 0; lambda 0.080000; train accuracy 51.914167; train loss 1.613543; test accuracy 51.604592; test loss 1.626955; reg val 34.014065; time 2019-04-28 22:21:22.969941\n",
      "Epoch 13; rep 0; lambda 0.080000; train accuracy 52.581685; train loss 1.589284; test accuracy 52.197704; test loss 1.605184; reg val 35.454636; time 2019-04-28 22:22:09.976605\n",
      "Epoch 14; rep 0; lambda 0.080000; train accuracy 53.201978; train loss 1.567134; test accuracy 52.735969; test loss 1.586005; reg val 36.739105; time 2019-04-28 22:22:56.883599\n",
      "Epoch 15; rep 0; lambda 0.080000; train accuracy 53.712508; train loss 1.546823; test accuracy 53.260204; test loss 1.568268; reg val 37.865509; time 2019-04-28 22:23:44.048972\n",
      "Epoch 16; rep 0; lambda 0.080000; train accuracy 54.194320; train loss 1.528123; test accuracy 53.714286; test loss 1.551336; reg val 38.926636; time 2019-04-28 22:24:31.038862\n",
      "Epoch 17; rep 0; lambda 0.080000; train accuracy 54.680600; train loss 1.510857; test accuracy 54.117347; test loss 1.535288; reg val 40.000458; time 2019-04-28 22:25:18.311407\n",
      "Epoch 18; rep 0; lambda 0.080000; train accuracy 55.089343; train loss 1.494889; test accuracy 54.511480; test loss 1.520485; reg val 41.043327; time 2019-04-28 22:26:05.244944\n",
      "Epoch 19; rep 0; lambda 0.080000; train accuracy 55.487875; train loss 1.480103; test accuracy 54.900510; test loss 1.506969; reg val 42.094418; time 2019-04-28 22:26:52.244475\n",
      "Epoch 20; rep 0; lambda 0.080000; train accuracy 55.853861; train loss 1.466388; test accuracy 55.200255; test loss 1.494552; reg val 43.143875; time 2019-04-28 22:27:39.330820\n",
      "Epoch 21; rep 0; lambda 0.080000; train accuracy 56.175814; train loss 1.453625; test accuracy 55.446429; test loss 1.483108; reg val 44.174320; time 2019-04-28 22:28:26.421493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22; rep 0; lambda 0.080000; train accuracy 56.506063; train loss 1.441703; test accuracy 55.681122; test loss 1.472582; reg val 45.147125; time 2019-04-28 22:29:13.521417\n",
      "Epoch 23; rep 0; lambda 0.080000; train accuracy 56.791959; train loss 1.430525; test accuracy 55.918367; test loss 1.462954; reg val 46.071625; time 2019-04-28 22:30:00.703905\n",
      "Epoch 24; rep 0; lambda 0.080000; train accuracy 57.076579; train loss 1.420011; test accuracy 56.082908; test loss 1.454146; reg val 46.976856; time 2019-04-28 22:30:47.840946\n",
      "Epoch 25; rep 0; lambda 0.080000; train accuracy 57.343969; train loss 1.410093; test accuracy 56.232143; test loss 1.446071; reg val 47.869095; time 2019-04-28 22:31:34.928199\n",
      "Epoch 26; rep 0; lambda 0.080000; train accuracy 57.590619; train loss 1.400716; test accuracy 56.392857; test loss 1.438541; reg val 48.719894; time 2019-04-28 22:32:21.832790\n",
      "Epoch 27; rep 0; lambda 0.080000; train accuracy 57.834078; train loss 1.391830; test accuracy 56.558673; test loss 1.431348; reg val 49.501793; time 2019-04-28 22:33:09.039512\n",
      "Epoch 28; rep 0; lambda 0.080000; train accuracy 58.059349; train loss 1.383398; test accuracy 56.770408; test loss 1.424328; reg val 50.258842; time 2019-04-28 22:33:55.862975\n",
      "Epoch 29; rep 0; lambda 0.080000; train accuracy 58.298022; train loss 1.375386; test accuracy 56.951531; test loss 1.417359; reg val 50.992874; time 2019-04-28 22:34:43.109608\n",
      "Epoch 30; rep 0; lambda 0.080000; train accuracy 58.526165; train loss 1.367765; test accuracy 57.127551; test loss 1.410388; reg val 51.718479; time 2019-04-28 22:35:30.296286\n",
      "Epoch 31; rep 0; lambda 0.080000; train accuracy 58.723995; train loss 1.360509; test accuracy 57.299745; test loss 1.403413; reg val 52.416340; time 2019-04-28 22:36:17.521768\n",
      "Epoch 32; rep 0; lambda 0.080000; train accuracy 58.895341; train loss 1.353592; test accuracy 57.491071; test loss 1.396490; reg val 53.077736; time 2019-04-28 22:37:04.630590\n",
      "Epoch 33; rep 0; lambda 0.080000; train accuracy 59.086790; train loss 1.346992; test accuracy 57.695153; test loss 1.389674; reg val 53.697327; time 2019-04-28 22:37:51.681886\n",
      "Epoch 34; rep 0; lambda 0.080000; train accuracy 59.254308; train loss 1.340686; test accuracy 57.844388; test loss 1.383041; reg val 54.298393; time 2019-04-28 22:38:38.588848\n",
      "Epoch 35; rep 0; lambda 0.080000; train accuracy 59.419272; train loss 1.334653; test accuracy 58.065051; test loss 1.376652; reg val 54.882874; time 2019-04-28 22:39:25.558807\n",
      "Epoch 36; rep 0; lambda 0.080000; train accuracy 59.572112; train loss 1.328874; test accuracy 58.234694; test loss 1.370537; reg val 55.444588; time 2019-04-28 22:40:12.595923\n",
      "Epoch 37; rep 0; lambda 0.080000; train accuracy 59.728462; train loss 1.323329; test accuracy 58.395408; test loss 1.364716; reg val 55.979889; time 2019-04-28 22:40:59.765038\n",
      "Epoch 38; rep 0; lambda 0.080000; train accuracy 59.891832; train loss 1.318001; test accuracy 58.531888; test loss 1.359184; reg val 56.491695; time 2019-04-28 22:41:46.874112\n",
      "Epoch 39; rep 0; lambda 0.080000; train accuracy 60.021059; train loss 1.312875; test accuracy 58.654337; test loss 1.353937; reg val 56.989601; time 2019-04-28 22:42:33.987989\n",
      "Epoch 40; rep 0; lambda 0.080000; train accuracy 60.143267; train loss 1.307937; test accuracy 58.765306; test loss 1.348955; reg val 57.476585; time 2019-04-28 22:43:21.235581\n",
      "Epoch 41; rep 0; lambda 0.080000; train accuracy 60.277920; train loss 1.303173; test accuracy 58.900510; test loss 1.344209; reg val 57.960930; time 2019-04-28 22:44:08.261821\n",
      "Epoch 42; rep 0; lambda 0.080000; train accuracy 60.392151; train loss 1.298571; test accuracy 59.035714; test loss 1.339682; reg val 58.446449; time 2019-04-28 22:44:55.259158\n",
      "Epoch 43; rep 0; lambda 0.080000; train accuracy 60.514997; train loss 1.294121; test accuracy 59.142857; test loss 1.335344; reg val 58.925323; time 2019-04-28 22:45:42.375327\n",
      "Epoch 44; rep 0; lambda 0.080000; train accuracy 60.619017; train loss 1.289812; test accuracy 59.272959; test loss 1.331168; reg val 59.382454; time 2019-04-28 22:46:29.342318\n",
      "Epoch 45; rep 0; lambda 0.080000; train accuracy 60.738992; train loss 1.285635; test accuracy 59.373724; test loss 1.327142; reg val 59.833359; time 2019-04-28 22:47:16.480760\n",
      "Epoch 46; rep 0; lambda 0.080000; train accuracy 60.850032; train loss 1.281583; test accuracy 59.496173; test loss 1.323244; reg val 60.267540; time 2019-04-28 22:48:03.531289\n",
      "Epoch 47; rep 0; lambda 0.080000; train accuracy 60.964582; train loss 1.277648; test accuracy 59.599490; test loss 1.319458; reg val 60.700275; time 2019-04-28 22:48:50.655443\n",
      "Epoch 48; rep 0; lambda 0.080000; train accuracy 61.070198; train loss 1.273825; test accuracy 59.711735; test loss 1.315786; reg val 61.115959; time 2019-04-28 22:49:38.185904\n",
      "Epoch 49; rep 0; lambda 0.080000; train accuracy 61.173899; train loss 1.270108; test accuracy 59.813776; test loss 1.312211; reg val 61.522480; time 2019-04-28 22:50:25.824350\n",
      "Epoch 0; rep 0; lambda 0.090000; train accuracy 29.912253; train loss 2.697863; test accuracy 34.881378; test loss 2.294904; reg val 51.636917; time 2019-04-28 22:51:12.569686\n",
      "Epoch 1; rep 0; lambda 0.090000; train accuracy 36.515316; train loss 2.218997; test accuracy 37.677296; test loss 2.177772; reg val 11.719257; time 2019-04-28 22:52:00.412927\n",
      "Epoch 2; rep 0; lambda 0.090000; train accuracy 38.744097; train loss 2.118294; test accuracy 39.095663; test loss 2.102612; reg val 15.455902; time 2019-04-28 22:52:47.326795\n",
      "Epoch 3; rep 0; lambda 0.090000; train accuracy 40.945437; train loss 2.027732; test accuracy 41.053571; test loss 2.014396; reg val 18.126762; time 2019-04-28 22:53:34.806235\n",
      "Epoch 4; rep 0; lambda 0.090000; train accuracy 42.922144; train loss 1.948371; test accuracy 42.733418; test loss 1.943356; reg val 19.363632; time 2019-04-28 22:54:26.415651\n",
      "Epoch 5; rep 0; lambda 0.090000; train accuracy 44.441289; train loss 1.885887; test accuracy 44.142857; test loss 1.890090; reg val 20.045927; time 2019-04-28 22:55:19.379632\n",
      "Epoch 6; rep 0; lambda 0.090000; train accuracy 45.798022; train loss 1.834176; test accuracy 45.375000; test loss 1.844148; reg val 21.348724; time 2019-04-28 22:56:15.596737\n",
      "Epoch 7; rep 0; lambda 0.090000; train accuracy 47.079451; train loss 1.787830; test accuracy 46.670918; test loss 1.801742; reg val 22.675865; time 2019-04-28 22:57:11.507653\n",
      "Epoch 8; rep 0; lambda 0.090000; train accuracy 48.237396; train loss 1.746219; test accuracy 47.781888; test loss 1.761615; reg val 23.968494; time 2019-04-28 22:58:07.484760\n",
      "Epoch 9; rep 0; lambda 0.090000; train accuracy 49.264199; train loss 1.708908; test accuracy 48.795918; test loss 1.725072; reg val 25.365690; time 2019-04-28 22:59:03.419036\n",
      "Epoch 10; rep 0; lambda 0.090000; train accuracy 50.179004; train loss 1.675334; test accuracy 49.633929; test loss 1.694748; reg val 26.883251; time 2019-04-28 22:59:59.758182\n",
      "Epoch 11; rep 0; lambda 0.090000; train accuracy 51.059987; train loss 1.644985; test accuracy 50.447704; test loss 1.668502; reg val 28.489037; time 2019-04-28 23:00:55.896320\n",
      "Epoch 12; rep 0; lambda 0.090000; train accuracy 51.847798; train loss 1.617439; test accuracy 51.225765; test loss 1.643688; reg val 30.086548; time 2019-04-28 23:01:51.164656\n",
      "Epoch 13; rep 0; lambda 0.090000; train accuracy 52.538609; train loss 1.592412; test accuracy 51.886480; test loss 1.621663; reg val 31.532125; time 2019-04-28 23:02:47.133281\n",
      "Epoch 14; rep 0; lambda 0.090000; train accuracy 53.158583; train loss 1.569668; test accuracy 52.375000; test loss 1.602559; reg val 32.924164; time 2019-04-28 23:03:42.132718\n",
      "Epoch 15; rep 0; lambda 0.090000; train accuracy 53.719209; train loss 1.548941; test accuracy 52.881378; test loss 1.585184; reg val 34.240257; time 2019-04-28 23:04:38.735117\n",
      "Epoch 16; rep 0; lambda 0.090000; train accuracy 54.211551; train loss 1.529963; test accuracy 53.228316; test loss 1.568731; reg val 35.484261; time 2019-04-28 23:05:35.506130\n",
      "Epoch 17; rep 0; lambda 0.090000; train accuracy 54.669432; train loss 1.512490; test accuracy 53.588010; test loss 1.553054; reg val 36.713860; time 2019-04-28 23:06:32.307937\n",
      "Epoch 18; rep 0; lambda 0.090000; train accuracy 55.069241; train loss 1.496314; test accuracy 53.969388; test loss 1.538141; reg val 37.884777; time 2019-04-28 23:07:28.972563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19; rep 0; lambda 0.090000; train accuracy 55.464263; train loss 1.481267; test accuracy 54.321429; test loss 1.524017; reg val 39.000793; time 2019-04-28 23:08:25.516121\n",
      "Epoch 20; rep 0; lambda 0.090000; train accuracy 55.813019; train loss 1.467230; test accuracy 54.656888; test loss 1.510640; reg val 40.041191; time 2019-04-28 23:09:22.307745\n",
      "Epoch 21; rep 0; lambda 0.090000; train accuracy 56.169113; train loss 1.454124; test accuracy 54.955357; test loss 1.497961; reg val 41.018600; time 2019-04-28 23:10:18.808904\n",
      "Epoch 22; rep 0; lambda 0.090000; train accuracy 56.492342; train loss 1.441879; test accuracy 55.234694; test loss 1.485908; reg val 41.957115; time 2019-04-28 23:11:15.493722\n",
      "Epoch 23; rep 0; lambda 0.090000; train accuracy 56.809190; train loss 1.430422; test accuracy 55.501276; test loss 1.474443; reg val 42.826622; time 2019-04-28 23:12:12.320254\n",
      "Epoch 24; rep 0; lambda 0.090000; train accuracy 57.099234; train loss 1.419682; test accuracy 55.764031; test loss 1.463633; reg val 43.637329; time 2019-04-28 23:13:09.204149\n",
      "Epoch 25; rep 0; lambda 0.090000; train accuracy 57.374920; train loss 1.409591; test accuracy 56.026786; test loss 1.453605; reg val 44.411583; time 2019-04-28 23:14:05.789792\n",
      "Epoch 26; rep 0; lambda 0.090000; train accuracy 57.627632; train loss 1.400087; test accuracy 56.233418; test loss 1.444468; reg val 45.128464; time 2019-04-28 23:15:02.286923\n",
      "Epoch 27; rep 0; lambda 0.090000; train accuracy 57.856733; train loss 1.391114; test accuracy 56.464286; test loss 1.436269; reg val 45.826744; time 2019-04-28 23:15:58.854239\n",
      "Epoch 28; rep 0; lambda 0.090000; train accuracy 58.088066; train loss 1.382623; test accuracy 56.665816; test loss 1.428888; reg val 46.498302; time 2019-04-28 23:16:55.029871\n",
      "Epoch 29; rep 0; lambda 0.090000; train accuracy 58.310147; train loss 1.374571; test accuracy 56.860969; test loss 1.422179; reg val 47.167885; time 2019-04-28 23:17:51.558741\n",
      "Epoch 30; rep 0; lambda 0.090000; train accuracy 58.530313; train loss 1.366923; test accuracy 56.980867; test loss 1.415935; reg val 47.789314; time 2019-04-28 23:18:48.013094\n",
      "Epoch 31; rep 0; lambda 0.090000; train accuracy 58.700702; train loss 1.359649; test accuracy 57.140306; test loss 1.410013; reg val 48.421761; time 2019-04-28 23:19:44.737044\n",
      "Epoch 32; rep 0; lambda 0.090000; train accuracy 58.900766; train loss 1.352722; test accuracy 57.316327; test loss 1.404321; reg val 49.034401; time 2019-04-28 23:20:41.402796\n",
      "Epoch 33; rep 0; lambda 0.090000; train accuracy 59.085514; train loss 1.346117; test accuracy 57.455357; test loss 1.398790; reg val 49.647194; time 2019-04-28 23:21:38.294739\n",
      "Epoch 34; rep 0; lambda 0.090000; train accuracy 59.269623; train loss 1.339812; test accuracy 57.613520; test loss 1.393345; reg val 50.242702; time 2019-04-28 23:22:34.910170\n",
      "Epoch 35; rep 0; lambda 0.090000; train accuracy 59.440970; train loss 1.333784; test accuracy 57.724490; test loss 1.387961; reg val 50.828148; time 2019-04-28 23:23:31.872775\n",
      "Epoch 36; rep 0; lambda 0.090000; train accuracy 59.598915; train loss 1.328013; test accuracy 57.850765; test loss 1.382620; reg val 51.371105; time 2019-04-28 23:24:28.630570\n",
      "Epoch 37; rep 0; lambda 0.090000; train accuracy 59.772176; train loss 1.322481; test accuracy 57.988520; test loss 1.377311; reg val 51.914032; time 2019-04-28 23:25:25.417368\n",
      "Epoch 38; rep 0; lambda 0.090000; train accuracy 59.906509; train loss 1.317170; test accuracy 58.084184; test loss 1.372050; reg val 52.430069; time 2019-04-28 23:26:22.309080\n",
      "Epoch 39; rep 0; lambda 0.090000; train accuracy 60.041800; train loss 1.312064; test accuracy 58.261480; test loss 1.366851; reg val 52.943832; time 2019-04-28 23:27:18.724075\n",
      "Epoch 40; rep 0; lambda 0.090000; train accuracy 60.179962; train loss 1.307150; test accuracy 58.380102; test loss 1.361738; reg val 53.438568; time 2019-04-28 23:28:15.407793\n",
      "Epoch 41; rep 0; lambda 0.090000; train accuracy 60.297703; train loss 1.302413; test accuracy 58.531888; test loss 1.356725; reg val 53.902798; time 2019-04-28 23:29:12.045294\n",
      "Epoch 42; rep 0; lambda 0.090000; train accuracy 60.424378; train loss 1.297842; test accuracy 58.635204; test loss 1.351843; reg val 54.357822; time 2019-04-28 23:30:08.542056\n",
      "Epoch 43; rep 0; lambda 0.090000; train accuracy 60.539247; train loss 1.293427; test accuracy 58.775510; test loss 1.347096; reg val 54.798882; time 2019-04-28 23:31:04.793846\n",
      "Epoch 44; rep 0; lambda 0.090000; train accuracy 60.648054; train loss 1.289158; test accuracy 58.892857; test loss 1.342508; reg val 55.220634; time 2019-04-28 23:31:59.287369\n",
      "Epoch 45; rep 0; lambda 0.090000; train accuracy 60.753989; train loss 1.285025; test accuracy 59.017857; test loss 1.338075; reg val 55.640228; time 2019-04-28 23:32:55.392391\n",
      "Epoch 46; rep 0; lambda 0.090000; train accuracy 60.888641; train loss 1.281021; test accuracy 59.142857; test loss 1.333801; reg val 56.045586; time 2019-04-28 23:33:51.382121\n",
      "Epoch 47; rep 0; lambda 0.090000; train accuracy 60.988194; train loss 1.277139; test accuracy 59.211735; test loss 1.329672; reg val 56.446301; time 2019-04-28 23:34:47.579345\n",
      "Epoch 48; rep 0; lambda 0.090000; train accuracy 61.106254; train loss 1.273370; test accuracy 59.309949; test loss 1.325689; reg val 56.823639; time 2019-04-28 23:35:43.807334\n",
      "Epoch 49; rep 0; lambda 0.090000; train accuracy 61.206765; train loss 1.269710; test accuracy 59.429847; test loss 1.321836; reg val 57.186508; time 2019-04-28 23:36:40.318067\n"
     ]
    }
   ],
   "source": [
    "# regularizing digonal blocks of the partitioned RNN\n",
    "# initialize arrays of loss values and weights over the number of epohcs, the number of lambdas we are testing, and the number of reps. \n",
    "train_loss_P = np.zeros((N_EPOCHS,N_LAMBDA,N_REPS)) \n",
    "train_acc_P = np.zeros((N_EPOCHS,N_LAMBDA,N_REPS))\n",
    "test_loss_P = np.zeros((N_EPOCHS,N_LAMBDA,N_REPS))\n",
    "test_acc_P = np.zeros((N_EPOCHS,N_LAMBDA,N_REPS))\n",
    "Phist_P = np.zeros((N_PARAMS,N_EPOCHS,N_LAMBDA,N_REPS))\n",
    "\n",
    "model_P = [None]*N_LAMBDA*N_REPS # array of models\n",
    "regval_P = [] # array of regularization values\n",
    "\n",
    "lr = 1e-2\n",
    "momentum = 0.9\n",
    "\n",
    "for r in tnrange(N_REPS): # loop over the number of reps\n",
    "    for k in tnrange(N_LAMBDA): # loop over the number of different lambda values\n",
    "        reg_lambda = lambdas[k] # set the regularization lambda\n",
    "        model_path = './models/model_'+modelkey+'_shortrun_P_rep_{}_lambda_{:d}_10.pt'.format(r,int(reg_lambda*10)) # path to which we will save the model\n",
    "        model_P[k+r*N_LAMBDA] = MLP(N_INPUTS,N_HIDDEN_NEURONS,N_OUTPUTS,device).to(device) # create the model\n",
    "        l2_reg = torch.tensor(1,device=device) # create the l2 regularization value tensor\n",
    "        optimizer = torch.optim.SGD(model_P[k+r*N_LAMBDA].parameters(), lr=lr, momentum=momentum) # set the function for SGD\n",
    "        criterion = nn.CrossEntropyLoss() # set the loss function\n",
    "        \n",
    "        # note that cross-entropy loss expects the indices of the class, not the one-hot. So, for A = [1,0,0,...] and B = [0,1,0,...], A is 0 and B is 1\n",
    "        \n",
    "        for epoch in range(N_EPOCHS): # for each training epoch\n",
    "            nps = 0\n",
    "            running_train_loss=0\n",
    "            running_train_acc=0\n",
    "            model_P[k+r*N_LAMBDA].train() \n",
    "            for p, param in enumerate(model_P[k+r*N_LAMBDA].parameters()): # go through all the model parameters\n",
    "                if param.requires_grad:\n",
    "                    plist = torch.flatten(param.data) # set the list of parameters\n",
    "                    for j in range(plist.size(0)):\n",
    "                        while nps < Phist_P.shape[0]:\n",
    "                            Phist_P[nps,epoch,k,r]=plist[j].item() # update the parameters\n",
    "                            nps+=1\n",
    "\n",
    "            for i, (x, y_tar) in enumerate(trainloader):\n",
    "                # print(i,x,y_tar)\n",
    "                l2_reg = 0\n",
    "                x, y_tar = x.to(device), y_tar.to(device) # x is the training set, y_tar is the output label\n",
    "                x = x-0.3\n",
    "                optimizer.zero_grad() # set gradients to 0\n",
    "                # print(x.shape)\n",
    "                y_pred = model_P[k+r*N_LAMBDA](x.view(x.shape[0],x.shape[1]*x.shape[2])) # compute the prediction. # size mismatch\n",
    "                \n",
    "                \n",
    "                loss = criterion(y_pred,y_tar) \n",
    "                for p,param in enumerate(model_P[k+r*N_LAMBDA].parameters()):\n",
    "                    if param.requires_grad and len(param.shape)==2:\n",
    "                        if param.shape[0]==N_HIDDEN_NEURONS and param.shape[1]==N_HIDDEN_NEURONS:\n",
    "                            l2_reg = l2_reg + param[:gidx,:gidx].norm(p=1) # update the l1 regularization constant\n",
    "                            l2_reg = l2_reg + param[gidx:,gidx:].norm(p=1)\n",
    "#                         elif param.shape[1]==N_HIDDEN_NEURONS:\n",
    "#                             l2_reg = l2_reg + param[:,gidx:].norm(p=1)\n",
    "#                         elif param.shape[0]==N_HIDDEN_NEURONS:\n",
    "#                             l2_reg = l2_reg + param[:gidx,:].norm(p=1)\n",
    "                regval_P.append(l2_reg.item()) # add the l2 regularization to  the running list\n",
    "                loss = loss + l2_reg*reg_lambda/BATCH_SIZE # compute the loss\n",
    "                loss.backward() # backpropogate the loss\n",
    "                optimizer.step() # run SGD\n",
    "                running_train_loss+=loss.item()\n",
    "                running_train_acc+=get_accuracy(y_pred, y_tar) # compute accuracy\n",
    "            \n",
    "            running_test_acc=0\n",
    "            running_test_loss=0\n",
    "            model_P[k+r*N_LAMBDA].eval()\n",
    "            for i,(x_test, y_test_tar) in enumerate(testloader):\n",
    "                x_test, y_test_tar = x_test.to(device), y_test_tar.to(device)\n",
    "                x_test = x_test - 0.3\n",
    "                y_test_pred = model_P[k+r*N_LAMBDA](x_test.view(x_test.shape[0],x_test.shape[1]*x_test.shape[2]))\n",
    "                loss = criterion(y_test_pred,y_test_tar)\n",
    "                \n",
    "                running_test_loss+=loss.item()\n",
    "                running_test_acc+=get_accuracy(y_test_pred, y_test_tar)\n",
    "                \n",
    "            train_loss_P[epoch,k,r] = running_train_loss/len(trainloader)\n",
    "            train_acc_P[epoch,k,r] = running_train_acc/len(trainloader)\n",
    "            test_loss_P[epoch,k,r] = running_test_loss/len(testloader)\n",
    "            test_acc_P[epoch,k,r] = running_test_acc/len(testloader)\n",
    "            print(\"Epoch %d; rep %d; lambda %f; train accuracy %f; train loss %f; test accuracy %f; test loss %f; reg val %f; time %s\"\n",
    "                  %(epoch,\n",
    "                    r,\n",
    "                    reg_lambda,\n",
    "                    train_acc_P[epoch,k,r],\n",
    "                    train_loss_P[epoch,k,r],\n",
    "                    test_acc_P[epoch,k,r],\n",
    "                    test_loss_P[epoch,k,r],\n",
    "                   l2_reg.item(),\n",
    "                   str(datetime.datetime.now())))\n",
    "            \n",
    "        # save the model and free the memory  \n",
    "        torch.save(model_P[k+r*N_LAMBDA].state_dict(), model_path)\n",
    "        model_P[k+r*N_LAMBDA] = [None]\n",
    "        del(l2_reg,loss,optimizer,criterion,plist,param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fd04e977a20>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# number of lambdas\n",
    "n_epochs_plot,n_lambdas_plot = np.mean(test_acc_P,2).shape\n",
    "\n",
    "fig,ax = plt.subplots(1,2)\n",
    "fig.subplots_adjust(hspace=20.0)\n",
    "\n",
    "ax1,ax2 = ax\n",
    "cm1 = plt.get_cmap('copper') # plt.get_cmap('gist_rainbow') # plt.get_cmap('gist_rainbow')\n",
    "# fig = plt.figure()\n",
    "NUM_COLORS = n_lambdas_plot\n",
    "ax1.set_prop_cycle('color', [cm1(1.*i/NUM_COLORS) for i in range(NUM_COLORS)])\n",
    "ax2.set_prop_cycle('color', [cm1(1.*i/NUM_COLORS) for i in range(NUM_COLORS)])\n",
    "\n",
    "fig.suptitle(\"Epoch %d; reps %d; min/max lambda [%f,%f]\"\n",
    "             %(epoch,\n",
    "               N_REPS,\n",
    "               min(lambdas),\n",
    "               max(lambdas)))\n",
    "\n",
    "# train accuracy %f; train loss %f; test accuracy %f; test loss %f\"\n",
    "# train_acc_P[epoch,k,r],\n",
    "#                train_loss_P[epoch,k,r],\n",
    "#                test_acc_P[epoch,k,r],\n",
    "#                test_loss_P[epoch,k,r]\n",
    "            \n",
    "# ax = fig.add_subplot(111)\n",
    "for i in enumerate(lambdas):\n",
    "    ax1.plot(np.mean(test_acc_P,2)[:,i[0]],label=i[1],)\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Test Accuracy\")\n",
    "    \n",
    "# ax2 = fig.add_subplot(011)\n",
    "for i in enumerate(lambdas):\n",
    "    ax2.plot(np.mean(test_loss_P,2)[:,i[0]],label=i[1],)\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(\"Test Loss\")\n",
    "ax2.legend(loc=\"upper right\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd04e5b3ac8>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.plot(regval_P)\n",
    "regval_P_tensor = torch.tensor(regval_P).view(N_EPOCHS,len(lambdas),len(trainloader))\n",
    "plt.plot(regval_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # create colormap\n",
    "\n",
    "\n",
    "\n",
    "# #plt.imshow(x[0,:,:])\n",
    "# #plt.plot(y_pred.detach().numpy()[0,:])\n",
    "# #torch.max(y_pred,1)\n",
    "# plt.figure(1)\n",
    "# plt.plot(np.mean(test_acc_P,2))\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Test accuracy\")\n",
    "# plt.plot()\n",
    "\n",
    "# plt.figure(2)\n",
    "# plt.plot(np.mean(test_loss_P,2))\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Test loss\")\n",
    "# plt.plot()\n",
    "\n",
    "\n",
    "# for i,j in enumerate(zip(np.mean(test_acc_P,2),np.mean(test_loss_P,2))):\n",
    "#     # plt.figure(i+3) # Here's the part I need, but numbering starts at 1!\n",
    "#     fig,axs = plt.subplots(1,2)\n",
    "#     fig.suptitle(\"Epoch %d; reps %d; lambda %f; train accuracy %f; train loss %f; test accuracy %f; test loss %f\"\n",
    "#                   %(epoch,\n",
    "#                     REPS,\n",
    "#                     k,\n",
    "#                     train_acc_P[epoch,k,r],\n",
    "#                     train_loss_P[epoch,k,r],\n",
    "#                     test_acc_P[epoch,k,r],\n",
    "#                     test_loss_P[epoch,k,r]))\n",
    "#     axs[0].plot(lambdas,j[0])\n",
    "#     axs[0].set_xlabel(\"Lambda\")\n",
    "#     axs[0].set_ylabel(\"Test accuracy\")\n",
    "#     # axs[0].title(\"Epoch %d\"%i)\n",
    "#     axs[1].plot(lambdas,j[1])\n",
    "#     axs[1].set_xlabel(\"Lambda\")\n",
    "#     axs[1].set_ylabel(\"Test loss\")\n",
    "#     # axs[1].title(\"Epoch %d\"%i)\n",
    "\n",
    "\n",
    "# # plt.plot(np.mean(test_acc_P,1))\n",
    "# # plt.plot()\n",
    "\n",
    "# # plt.plot(np.mean(test_acc_P,2))\n",
    "# # plt.plot()\n",
    "\n",
    "# # plt.plot(np.mean(test_acc_P,3))\n",
    "# # plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def readtxt(txt_name = 'anna.txt'):\n",
    "#     dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "#     txt_file = os.path.join(dir_path,txt_name)\n",
    "#     # load the whole book\n",
    "#     file = open(self.txt_file)\n",
    "#     alltxt = file.read()\n",
    "#     # remove newline formmating\n",
    "#     alltxt = alltxt.replace(\"\\n\\n\", \"&\").replace(\"\\n\", \" \").replace(\"&\", \"\\n\")\n",
    "#     # define categories\n",
    "#     categories = list(sorted(set(alltxt)))\n",
    "#     # integer encode\n",
    "#     label_encoder = LabelEncoder()\n",
    "#     label_encoder.fit(categories)\n",
    "#     integer_encoded = torch.LongTensor(label_encoder.transform(list(alltxt)))\n",
    "#     return integer_encoded, categories\n",
    "\n",
    "# # def onehotencode(integer_encoded_batch,n_cat):\n",
    "    \n",
    "# # def get_next_batch(dat,batch_size):\n",
    "# #     x_int = \n",
    "# #     y_int = \n",
    "# #     x_hot = onehotencode(x_int): \n",
    "# #     return x_hot, y_int \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# BATCH_SIZE = 500 # how many batches we are running\n",
    "# N_STEPS = 10 # How many characters are we inputting into the list at a time\n",
    "# N_HIDDEN_NEURONS = 512 # how many neurons per hidden layer\n",
    "# N_INPUTS = 77*N_STEPS\n",
    "# N_OUTPUTS = 77\n",
    "# N_LAYERS = 2 # 2 hidden layers\n",
    "# N_EPOCHS = 11 # how many training epocs\n",
    "# learning_rates = np.asarray([2]) # learning rates\n",
    "# N_REPS = 3 # len(learning_rates) # the number of learning repetitions\n",
    "# N_PARAMS = nparam_MLP(N_INPUTS,N_HIDDEN_NEURONS,N_OUTPUTS)\n",
    "# gidx = int(N_HIDDEN_NEURONS/2)\n",
    "\n",
    "\n",
    "# train_loss_P = np.zeros((N_EPOCHS,N_LAMBDA,N_REPS)) \n",
    "# train_acc_P = np.zeros((N_EPOCHS,N_LAMBDA,N_REPS))\n",
    "# test_loss_P = np.zeros((N_EPOCHS,N_LAMBDA,N_REPS))\n",
    "# test_acc_P = np.zeros((N_EPOCHS,N_LAMBDA,N_REPS))\n",
    "# Phist_P = np.zeros((N_PARAMS,N_EPOCHS,N_LAMBDA,N_REPS))\n",
    "\n",
    "# model_P = [None]*N_LAMBDA*N_REPS # array of models\n",
    "# regval_P = [] # array of regularization values\n",
    "\n",
    "\n",
    "\n",
    "pickle.dump([lambdas,N_EPOCHS,N_REPS,N_HIDDEN_NEURONS,learning_rates,N_REPS,N_PARAMS,\n",
    "             model_P,regval_P,\n",
    "             train_loss_P,train_acc_P,\n",
    "             test_loss_P,test_acc_P,\n",
    "             Phist_P], \n",
    "            open(modelkey+\"_mlp_ak_quickset.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(test_acc_P.shape)\n",
    "# plt.plot(np.mean(test_acc_P,2))\n",
    "# plt.plot()\n",
    "# print(np.mean(test_acc_P,1))\n",
    "# print(np.mean(test_acc_P,1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of tensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
