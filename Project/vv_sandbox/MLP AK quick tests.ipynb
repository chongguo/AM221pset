{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from tqdm import tnrange\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "%matplotlib inline  \n",
    "import datetime\n",
    "\n",
    "# alphabet\n",
    "import string\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(logit, target):\n",
    "    batch_size = len(target)\n",
    "    corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
    "    accuracy = 100.0 * corrects/batch_size\n",
    "    return accuracy.item()\n",
    "\n",
    "def nparam(ninputs,nhidden,noutputs):\n",
    "    return ninputs*(nhidden+1) + nhidden*(nhidden+1)+nhidden*(noutputs+1)\n",
    "\n",
    "# define the nnumber of parameters we need\n",
    "def nparam_MLP(N_INPUTS,N_HIDDEN,N_OUTPUTS):\n",
    "    input_to_hidden1 = (N_INPUTS+1)*N_HIDDEN #+1 for bias\n",
    "    hidden1_to_hidden2 = (N_HIDDEN + 1)*N_HIDDEN\n",
    "    hidden2_to_output = (N_OUTPUTS)*(N_HIDDEN+1)\n",
    "    return(sum([input_to_hidden1,hidden1_to_hidden2,hidden2_to_output]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a prototype MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden_neurons, n_output,  device):\n",
    "        super(MLP, self).__init__()\n",
    "        self.n_inputs = n_inputs # set the number of neurons in the input layer\n",
    "        self.n_hidden_neurons = n_hidden_neurons # how many neurons are in each hidden layer\n",
    "        self.n_output = n_output # set the number of neurons in the output layer\n",
    "        self.sig = nn.Sigmoid() # set the activation function \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.n_hidden = n_hidden_neurons\n",
    "        self.encoder = nn.Linear(n_inputs, n_hidden_neurons) # encode input\n",
    "        self.recurrent = nn.Linear(n_hidden_neurons,n_hidden_neurons) # recurrent connections\n",
    "        self.decoder = nn.Linear(n_hidden_neurons, n_output) # decode output\n",
    "                \n",
    "    def forward(self, x):\n",
    "        self.hidden1 = self.tanh(self.encoder(x))\n",
    "        self.hidden2 = self.tanh(self.recurrent(self.hidden1))\n",
    "        self.output = self.decoder(self.hidden2)\n",
    "        return self.output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.    0.01  0.02  0.03  0.04  0.05  0.06  0.07  0.08  0.09]\n"
     ]
    }
   ],
   "source": [
    "# Test MLP on Anna Karenina\n",
    "# Load Anna Karenina\n",
    "from torch.utils.data import DataLoader # dataloader \n",
    "import sys\n",
    "sys.path.insert(0,'../final_project/Data/')\n",
    "from AnnaDataset_MLP import AnnaDataset, InvertAnna # import AK dataset\n",
    "# from AnnaDataset import AnnaDataset, InvertAnna # import AK dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# params\n",
    "BATCH_SIZE = 500 # how many batches we are running\n",
    "N_STEPS = 10 # How many characters are we inputting into the list at a time\n",
    "N_HIDDEN_NEURONS = 512 # how many neurons per hidden layer\n",
    "N_LAYERS = 2 # 2 hidden layers\n",
    "N_EPOCHS = 15 # how many training epocs\n",
    "learning_rates = np.asarray([2]) # learning rates\n",
    "N_REPS = 1 # len(learning_rates) # the number of learning repetitions\n",
    "gidx = int(N_HIDDEN_NEURONS/2)\n",
    "\n",
    "# regularization parameters\n",
    "# lambdas = np.arange(0,1e-2,3e-3,dtype=np.float)\n",
    "lambdas = np.arange(0,1e-1,1e-2,dtype=np.float) # full sweep\n",
    "# lambdas = np.arange(0.4,1,0.45) # short sweep\n",
    "print(lambdas)\n",
    "# lambdas = np.arange(0,1,1e-1,dtype=np.float) # full sweep\n",
    "N_LAMBDA = len(lambdas)\n",
    "\n",
    "# load data\n",
    "# list all transformations\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Normalize((0,), (0.3,))])\n",
    "\n",
    "dataset = AnnaDataset(N_STEPS) # load the dataset\n",
    "\n",
    "N_INPUTS = len(dataset.categories)*N_STEPS\n",
    "N_OUTPUTS = len(dataset.categories)\n",
    "N_PARAMS = nparam_MLP(N_INPUTS,N_HIDDEN_NEURONS,N_OUTPUTS)\n",
    "\n",
    "\n",
    "# trainloader = DataLoader(dataset, batch_size=BATCH_SIZE,\n",
    "#                         shuffle=False, num_workers=4) # create a DataLoader. We want a batch of BATCH_SIZE entries\n",
    "# testloader = DataLoader(dataset, batch_size=BATCH_SIZE,\n",
    "#                         shuffle=False, num_workers=4) # create a DataLoader. We want a batch of BATCH_SIZE entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566579\n",
      "1566579\n",
      "391645\n"
     ]
    }
   ],
   "source": [
    "# test the test-train-validate sampler\n",
    "# # test_split = torch.split(dataset.onehot_encoded,20,dim=0)\n",
    "# # print(len(test_split))\n",
    "# print(dataset.onehot_encoded.shape)\n",
    "# print(test_split[0].shape)\n",
    "# print(test_split[1].shape)\n",
    "\n",
    "# train-test-split\n",
    "# train_fraction,test_fraction,valid_fraction = (0.8,0.1,0.1)\n",
    "train_fraction = 0.8\n",
    "\n",
    "random_seed = 0\n",
    "shuffle_dataset = True\n",
    "\n",
    "# from https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets \n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset.onehot_encoded)\n",
    "indices = list(range(dataset_size))\n",
    "train_split = int(np.floor(train_fraction * dataset_size))\n",
    "print(train_split)\n",
    "# valid_split = train_split + int(np.floor(valid_fraction * dataset_size))\n",
    "# test_split = valid_split + int(np.floor(test_fraction * dataset_size))\n",
    "\n",
    "# print(train_split,valid_split,test_split,dataset_size)\n",
    "\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "# train_indices, val_indices, test_indices = indices[:train_split], indices[train_split:valid_split], indices[valid_split:]\n",
    "train_indices = indices[:train_split]\n",
    "# val_indices = indices[train_split:valid_split]\n",
    "# test_indices = indices[valid_split:]\n",
    "test_indices = indices[train_split:]\n",
    "\n",
    "print(len(train_indices))\n",
    "# print(len(val_indices))\n",
    "print(len(test_indices))\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = torch.utils.data.SequentialSampler(train_indices)\n",
    "test_sampler = torch.utils.data.SequentialSampler(test_indices)\n",
    "# valid_sampler = torch.utils.data.SubsetRandomSampler(val_indices)\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "#                                            sampler=train_sampler)\n",
    "# validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "#                                                 sampler=valid_sampler)\n",
    "\n",
    "\n",
    "# dataset_train,dataset_test = train_test_split(dataset,test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test-validate split\n",
    "\n",
    "trainloader = DataLoader(dataset, batch_size=BATCH_SIZE,\n",
    "                         shuffle=False, num_workers=4,\n",
    "                        sampler = train_sampler) # create a DataLoader. We want a batch of BATCH_SIZE entries\n",
    "# validloader = DataLoader(dataset, batch_size=BATCH_SIZE,\n",
    "#                         sampler = valid_sampler) # create a DataLoader. We want a batch of BATCH_SIZE entries\n",
    "testloader = DataLoader(dataset, batch_size=BATCH_SIZE,\n",
    "                        shuffle=False, num_workers=4,\n",
    "                        sampler = test_sampler) # create a DataLoader. We want a batch of BATCH_SIZE entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-28T02:53:32.939026\n"
     ]
    }
   ],
   "source": [
    "# modelkey = ''.join([str(np.random.randint(0,9)) for i in range(10)])\n",
    "\n",
    "runnow = datetime.datetime.now()\n",
    "modelkey = str(runnow.isoformat())\n",
    "print(modelkey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f07dd5753b24d9a83b224dcd487818e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "806678f0674242d7bc8932d7681d1922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0; rep 0; lambda 0.000000; train accuracy 30.507977; train loss 2.501612; test accuracy 35.191327; test loss 2.264577; reg val 3194.038574\n",
      "Epoch 1; rep 0; lambda 0.000000; train accuracy 37.275048; train loss 2.178007; test accuracy 37.919643; test loss 2.146216; reg val 3363.051270\n",
      "Epoch 2; rep 0; lambda 0.000000; train accuracy 39.865986; train loss 2.066938; test accuracy 39.941327; test loss 2.055475; reg val 3551.477539\n",
      "Epoch 3; rep 0; lambda 0.000000; train accuracy 42.282387; train loss 1.966916; test accuracy 42.691327; test loss 1.953828; reg val 3743.566162\n",
      "Epoch 4; rep 0; lambda 0.000000; train accuracy 44.292597; train loss 1.885373; test accuracy 44.792092; test loss 1.879390; reg val 3929.897217\n",
      "Epoch 5; rep 0; lambda 0.000000; train accuracy 46.084556; train loss 1.816646; test accuracy 46.210459; test loss 1.822051; reg val 4114.816406\n",
      "Epoch 6; rep 0; lambda 0.000000; train accuracy 47.731972; train loss 1.758130; test accuracy 47.660714; test loss 1.768805; reg val 4298.642578\n",
      "Epoch 7; rep 0; lambda 0.000000; train accuracy 49.141672; train loss 1.708457; test accuracy 48.799745; test loss 1.725628; reg val 4479.700684\n",
      "Epoch 8; rep 0; lambda 0.000000; train accuracy 50.350670; train loss 1.665711; test accuracy 49.850765; test loss 1.687791; reg val 4657.918945\n",
      "Epoch 9; rep 0; lambda 0.000000; train accuracy 51.375239; train loss 1.628278; test accuracy 50.732143; test loss 1.654636; reg val 4832.378906\n",
      "Epoch 10; rep 0; lambda 0.000000; train accuracy 52.287811; train loss 1.594880; test accuracy 51.551020; test loss 1.624964; reg val 5003.364258\n",
      "Epoch 11; rep 0; lambda 0.000000; train accuracy 53.119655; train loss 1.564859; test accuracy 52.332908; test loss 1.598113; reg val 5171.077148\n",
      "Epoch 12; rep 0; lambda 0.000000; train accuracy 53.849394; train loss 1.537961; test accuracy 53.109694; test loss 1.572361; reg val 5334.903320\n",
      "Epoch 13; rep 0; lambda 0.000000; train accuracy 54.493618; train loss 1.513844; test accuracy 53.760204; test loss 1.548325; reg val 5493.767578\n",
      "Epoch 14; rep 0; lambda 0.000000; train accuracy 55.065412; train loss 1.492086; test accuracy 54.269133; test loss 1.526998; reg val 5648.400391\n",
      "Epoch 0; rep 0; lambda 0.010000; train accuracy 30.445437; train loss 2.561819; test accuracy 35.117347; test loss 2.267356; reg val 2466.943848\n",
      "Epoch 1; rep 0; lambda 0.010000; train accuracy 37.124442; train loss 2.231361; test accuracy 37.658163; test loss 2.160271; reg val 1996.302734\n",
      "Epoch 2; rep 0; lambda 0.010000; train accuracy 39.301213; train loss 2.127547; test accuracy 39.575255; test loss 2.078766; reg val 1665.668213\n",
      "Epoch 3; rep 0; lambda 0.010000; train accuracy 41.804084; train loss 2.020330; test accuracy 42.368622; test loss 1.969539; reg val 1439.875244\n",
      "Epoch 4; rep 0; lambda 0.010000; train accuracy 43.900128; train loss 1.932613; test accuracy 44.297194; test loss 1.898886; reg val 1256.504150\n",
      "Epoch 5; rep 0; lambda 0.010000; train accuracy 45.507658; train loss 1.865169; test accuracy 45.644133; test loss 1.841697; reg val 1125.561768\n",
      "Epoch 6; rep 0; lambda 0.010000; train accuracy 47.023293; train loss 1.807793; test accuracy 47.053571; test loss 1.793889; reg val 1037.456055\n",
      "Epoch 7; rep 0; lambda 0.010000; train accuracy 48.329292; train loss 1.758978; test accuracy 48.146684; test loss 1.752960; reg val 979.711304\n",
      "Epoch 8; rep 0; lambda 0.010000; train accuracy 49.485960; train loss 1.717376; test accuracy 49.119898; test loss 1.719539; reg val 942.709717\n",
      "Epoch 9; rep 0; lambda 0.010000; train accuracy 50.475112; train loss 1.681683; test accuracy 49.957908; test loss 1.692842; reg val 918.996216\n",
      "Epoch 10; rep 0; lambda 0.010000; train accuracy 51.297064; train loss 1.650556; test accuracy 50.599490; test loss 1.669274; reg val 904.230469\n",
      "Epoch 11; rep 0; lambda 0.010000; train accuracy 52.038609; train loss 1.622995; test accuracy 51.197704; test loss 1.646693; reg val 894.460327\n",
      "Epoch 12; rep 0; lambda 0.010000; train accuracy 52.708679; train loss 1.598276; test accuracy 51.806122; test loss 1.625059; reg val 887.433044\n",
      "Epoch 13; rep 0; lambda 0.010000; train accuracy 53.295788; train loss 1.575885; test accuracy 52.295918; test loss 1.604956; reg val 881.618530\n",
      "Epoch 14; rep 0; lambda 0.010000; train accuracy 53.843969; train loss 1.555492; test accuracy 52.795918; test loss 1.586657; reg val 876.149109\n",
      "Epoch 0; rep 0; lambda 0.020000; train accuracy 30.343650; train loss 2.608464; test accuracy 35.214286; test loss 2.268578; reg val 1822.959717\n",
      "Epoch 1; rep 0; lambda 0.020000; train accuracy 37.022336; train loss 2.249504; test accuracy 37.695153; test loss 2.165998; reg val 1030.514893\n",
      "Epoch 2; rep 0; lambda 0.020000; train accuracy 39.111040; train loss 2.133019; test accuracy 39.626276; test loss 2.082080; reg val 607.039429\n",
      "Epoch 3; rep 0; lambda 0.020000; train accuracy 41.509253; train loss 2.025394; test accuracy 41.973214; test loss 1.988703; reg val 416.337891\n",
      "Epoch 4; rep 0; lambda 0.020000; train accuracy 43.525846; train loss 1.939381; test accuracy 43.914541; test loss 1.915093; reg val 335.465759\n",
      "Epoch 5; rep 0; lambda 0.020000; train accuracy 45.197192; train loss 1.871130; test accuracy 45.501276; test loss 1.859466; reg val 301.700317\n",
      "Epoch 6; rep 0; lambda 0.020000; train accuracy 46.580408; train loss 1.817414; test accuracy 46.628827; test loss 1.816966; reg val 286.597961\n",
      "Epoch 7; rep 0; lambda 0.020000; train accuracy 47.791640; train loss 1.772799; test accuracy 47.538265; test loss 1.778623; reg val 280.050140\n",
      "Epoch 8; rep 0; lambda 0.020000; train accuracy 48.840779; train loss 1.733953; test accuracy 48.602041; test loss 1.739818; reg val 278.245941\n",
      "Epoch 9; rep 0; lambda 0.020000; train accuracy 49.750798; train loss 1.699216; test accuracy 49.568878; test loss 1.704516; reg val 277.864258\n",
      "Epoch 10; rep 0; lambda 0.020000; train accuracy 50.610083; train loss 1.668089; test accuracy 50.294643; test loss 1.674753; reg val 277.382263\n",
      "Epoch 11; rep 0; lambda 0.020000; train accuracy 51.331525; train loss 1.640241; test accuracy 51.014031; test loss 1.649065; reg val 275.946655\n",
      "Epoch 12; rep 0; lambda 0.020000; train accuracy 52.013082; train loss 1.615293; test accuracy 51.649235; test loss 1.626253; reg val 273.429199\n",
      "Epoch 13; rep 0; lambda 0.020000; train accuracy 52.605297; train loss 1.592805; test accuracy 52.266582; test loss 1.605618; reg val 270.459595\n",
      "Epoch 14; rep 0; lambda 0.020000; train accuracy 53.142629; train loss 1.572343; test accuracy 52.700255; test loss 1.586796; reg val 267.328369\n",
      "Epoch 0; rep 0; lambda 0.030000; train accuracy 30.194320; train loss 2.637532; test accuracy 34.900510; test loss 2.273895; reg val 1282.143066\n",
      "Epoch 1; rep 0; lambda 0.030000; train accuracy 36.856094; train loss 2.249302; test accuracy 37.515306; test loss 2.174690; reg val 455.941803\n",
      "Epoch 2; rep 0; lambda 0.030000; train accuracy 38.903318; train loss 2.129167; test accuracy 39.604592; test loss 2.089058; reg val 192.801620\n",
      "Epoch 3; rep 0; lambda 0.030000; train accuracy 41.182833; train loss 2.028055; test accuracy 41.733418; test loss 1.995437; reg val 143.605865\n",
      "Epoch 4; rep 0; lambda 0.030000; train accuracy 43.118379; train loss 1.943696; test accuracy 43.483418; test loss 1.923825; reg val 136.702789\n",
      "Epoch 5; rep 0; lambda 0.030000; train accuracy 44.792278; train loss 1.877695; test accuracy 44.984694; test loss 1.864530; reg val 135.603241\n",
      "Epoch 6; rep 0; lambda 0.030000; train accuracy 46.255265; train loss 1.824131; test accuracy 46.372449; test loss 1.815602; reg val 134.469864\n",
      "Epoch 7; rep 0; lambda 0.030000; train accuracy 47.486918; train loss 1.779407; test accuracy 47.549745; test loss 1.773852; reg val 132.603821\n",
      "Epoch 8; rep 0; lambda 0.030000; train accuracy 48.557754; train loss 1.739856; test accuracy 48.500000; test loss 1.740647; reg val 131.313202\n",
      "Epoch 9; rep 0; lambda 0.030000; train accuracy 49.521698; train loss 1.704172; test accuracy 49.362245; test loss 1.712848; reg val 130.982346\n",
      "Epoch 10; rep 0; lambda 0.030000; train accuracy 50.429483; train loss 1.672024; test accuracy 50.112245; test loss 1.685687; reg val 130.785736\n",
      "Epoch 11; rep 0; lambda 0.030000; train accuracy 51.229419; train loss 1.643184; test accuracy 50.704082; test loss 1.660727; reg val 130.554306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12; rep 0; lambda 0.030000; train accuracy 51.941927; train loss 1.617322; test accuracy 51.309949; test loss 1.638566; reg val 129.951401\n",
      "Epoch 13; rep 0; lambda 0.030000; train accuracy 52.568283; train loss 1.594012; test accuracy 51.820153; test loss 1.618551; reg val 129.073624\n",
      "Epoch 14; rep 0; lambda 0.030000; train accuracy 53.142629; train loss 1.572863; test accuracy 52.260204; test loss 1.600583; reg val 128.084656\n",
      "Epoch 0; rep 0; lambda 0.040000; train accuracy 30.195278; train loss 2.658705; test accuracy 34.996173; test loss 2.274205; reg val 864.043274\n",
      "Epoch 1; rep 0; lambda 0.040000; train accuracy 36.761327; train loss 2.239392; test accuracy 37.645408; test loss 2.180861; reg val 178.895447\n",
      "Epoch 2; rep 0; lambda 0.040000; train accuracy 38.831525; train loss 2.123020; test accuracy 39.321429; test loss 2.102046; reg val 73.201317\n",
      "Epoch 3; rep 0; lambda 0.040000; train accuracy 41.128271; train loss 2.025327; test accuracy 41.163265; test loss 2.006950; reg val 70.496597\n",
      "Epoch 4; rep 0; lambda 0.040000; train accuracy 43.001914; train loss 1.945410; test accuracy 42.850765; test loss 1.938591; reg val 72.529854\n",
      "Epoch 5; rep 0; lambda 0.040000; train accuracy 44.543395; train loss 1.882690; test accuracy 44.400510; test loss 1.881152; reg val 74.233879\n",
      "Epoch 6; rep 0; lambda 0.040000; train accuracy 45.941927; train loss 1.829956; test accuracy 45.696429; test loss 1.835795; reg val 74.580070\n",
      "Epoch 7; rep 0; lambda 0.040000; train accuracy 47.239311; train loss 1.784904; test accuracy 46.735969; test loss 1.795913; reg val 74.040878\n",
      "Epoch 8; rep 0; lambda 0.040000; train accuracy 48.340140; train loss 1.744981; test accuracy 47.781888; test loss 1.759111; reg val 73.663116\n",
      "Epoch 9; rep 0; lambda 0.040000; train accuracy 49.372368; train loss 1.708439; test accuracy 48.770408; test loss 1.725037; reg val 73.411575\n",
      "Epoch 10; rep 0; lambda 0.040000; train accuracy 50.309190; train loss 1.674810; test accuracy 49.695153; test loss 1.693804; reg val 73.538406\n",
      "Epoch 11; rep 0; lambda 0.040000; train accuracy 51.144863; train loss 1.644339; test accuracy 50.561224; test loss 1.665107; reg val 73.490250\n",
      "Epoch 12; rep 0; lambda 0.040000; train accuracy 51.869177; train loss 1.617114; test accuracy 51.248724; test loss 1.639945; reg val 73.210976\n",
      "Epoch 13; rep 0; lambda 0.040000; train accuracy 52.524888; train loss 1.592841; test accuracy 51.845663; test loss 1.618666; reg val 72.873810\n",
      "Epoch 14; rep 0; lambda 0.040000; train accuracy 53.114231; train loss 1.571052; test accuracy 52.375000; test loss 1.600056; reg val 72.739532\n",
      "Epoch 0; rep 0; lambda 0.050000; train accuracy 30.177090; train loss 2.674777; test accuracy 35.006378; test loss 2.276625; reg val 546.827637\n",
      "Epoch 1; rep 0; lambda 0.050000; train accuracy 36.671027; train loss 2.231819; test accuracy 37.536990; test loss 2.185699; reg val 69.338264\n",
      "Epoch 2; rep 0; lambda 0.050000; train accuracy 38.841417; train loss 2.118298; test accuracy 39.595663; test loss 2.091483; reg val 40.180527\n",
      "Epoch 3; rep 0; lambda 0.050000; train accuracy 41.228462; train loss 2.021795; test accuracy 41.507653; test loss 2.000602; reg val 46.002312\n",
      "Epoch 4; rep 0; lambda 0.050000; train accuracy 43.134971; train loss 1.944050; test accuracy 43.168367; test loss 1.938122; reg val 47.032036\n",
      "Epoch 5; rep 0; lambda 0.050000; train accuracy 44.679643; train loss 1.882451; test accuracy 44.905612; test loss 1.879925; reg val 47.295525\n",
      "Epoch 6; rep 0; lambda 0.050000; train accuracy 46.000319; train loss 1.830530; test accuracy 46.321429; test loss 1.831239; reg val 47.265160\n",
      "Epoch 7; rep 0; lambda 0.050000; train accuracy 47.225271; train loss 1.784903; test accuracy 47.480867; test loss 1.789275; reg val 47.401436\n",
      "Epoch 8; rep 0; lambda 0.050000; train accuracy 48.339821; train loss 1.744305; test accuracy 48.378827; test loss 1.752986; reg val 47.705452\n",
      "Epoch 9; rep 0; lambda 0.050000; train accuracy 49.320038; train loss 1.708138; test accuracy 49.228316; test loss 1.721925; reg val 48.281036\n",
      "Epoch 10; rep 0; lambda 0.050000; train accuracy 50.193044; train loss 1.675603; test accuracy 49.974490; test loss 1.694340; reg val 49.127956\n",
      "Epoch 11; rep 0; lambda 0.050000; train accuracy 50.994257; train loss 1.646206; test accuracy 50.604592; test loss 1.668687; reg val 49.987724\n",
      "Epoch 12; rep 0; lambda 0.050000; train accuracy 51.711551; train loss 1.619651; test accuracy 51.156888; test loss 1.645105; reg val 50.753120\n",
      "Epoch 13; rep 0; lambda 0.050000; train accuracy 52.346522; train loss 1.595646; test accuracy 51.720663; test loss 1.623589; reg val 51.564754\n",
      "Epoch 14; rep 0; lambda 0.050000; train accuracy 52.941608; train loss 1.573830; test accuracy 52.309949; test loss 1.603747; reg val 52.469208\n",
      "Epoch 0; rep 0; lambda 0.060000; train accuracy 30.035737; train loss 2.685579; test accuracy 34.900510; test loss 2.280365; reg val 326.931000\n",
      "Epoch 1; rep 0; lambda 0.060000; train accuracy 36.558711; train loss 2.225642; test accuracy 37.752551; test loss 2.178275; reg val 32.643574\n",
      "Epoch 2; rep 0; lambda 0.060000; train accuracy 38.776962; train loss 2.119155; test accuracy 39.373724; test loss 2.093579; reg val 28.497765\n",
      "Epoch 3; rep 0; lambda 0.060000; train accuracy 40.933950; train loss 2.028359; test accuracy 41.067602; test loss 2.018717; reg val 32.888725\n",
      "Epoch 4; rep 0; lambda 0.060000; train accuracy 42.773133; train loss 1.952327; test accuracy 42.709184; test loss 1.952547; reg val 34.959248\n",
      "Epoch 5; rep 0; lambda 0.060000; train accuracy 44.399170; train loss 1.888089; test accuracy 44.276786; test loss 1.886589; reg val 35.761894\n",
      "Epoch 6; rep 0; lambda 0.060000; train accuracy 45.825144; train loss 1.833390; test accuracy 45.785714; test loss 1.831262; reg val 36.710609\n",
      "Epoch 7; rep 0; lambda 0.060000; train accuracy 47.124761; train loss 1.786452; test accuracy 47.196429; test loss 1.786215; reg val 37.565727\n",
      "Epoch 8; rep 0; lambda 0.060000; train accuracy 48.263561; train loss 1.745238; test accuracy 48.167092; test loss 1.751688; reg val 38.703117\n",
      "Epoch 9; rep 0; lambda 0.060000; train accuracy 49.293874; train loss 1.708626; test accuracy 48.895408; test loss 1.724235; reg val 40.108208\n",
      "Epoch 10; rep 0; lambda 0.060000; train accuracy 50.215061; train loss 1.675874; test accuracy 49.506378; test loss 1.699601; reg val 41.612335\n",
      "Epoch 11; rep 0; lambda 0.060000; train accuracy 51.046267; train loss 1.646265; test accuracy 50.270408; test loss 1.675341; reg val 42.959496\n",
      "Epoch 12; rep 0; lambda 0.060000; train accuracy 51.771857; train loss 1.619301; test accuracy 51.048469; test loss 1.651188; reg val 44.156395\n",
      "Epoch 13; rep 0; lambda 0.060000; train accuracy 52.458839; train loss 1.594781; test accuracy 51.674745; test loss 1.629107; reg val 45.339478\n",
      "Epoch 14; rep 0; lambda 0.060000; train accuracy 53.060306; train loss 1.572495; test accuracy 52.219388; test loss 1.610273; reg val 46.387184\n"
     ]
    }
   ],
   "source": [
    "# regularizing digonal blocks of the partitioned RNN\n",
    "# initialize arrays of loss values and weights over the number of epohcs, the number of lambdas we are testing, and the number of reps. \n",
    "train_loss_P = np.zeros((N_EPOCHS,N_LAMBDA,N_REPS)) \n",
    "train_acc_P = np.zeros((N_EPOCHS,N_LAMBDA,N_REPS))\n",
    "test_loss_P = np.zeros((N_EPOCHS,N_LAMBDA,N_REPS))\n",
    "test_acc_P = np.zeros((N_EPOCHS,N_LAMBDA,N_REPS))\n",
    "Phist_P = np.zeros((N_PARAMS,N_EPOCHS,N_LAMBDA,N_REPS))\n",
    "\n",
    "model_P = [None]*N_LAMBDA*N_REPS # array of models\n",
    "regval_P = [] # array of regularization values\n",
    "\n",
    "lr = 1e-2\n",
    "momentum = 0.9\n",
    "\n",
    "for r in tnrange(N_REPS): # loop over the number of reps\n",
    "    for k in tnrange(N_LAMBDA): # loop over the number of different lambda values\n",
    "        reg_lambda = lambdas[k] # set the regularization lambda\n",
    "        model_path = './models/model_'+modelkey+'_shortrun_P_rep_{}_lambda_{:d}_10.pt'.format(r,int(reg_lambda*10)) # path to which we will save the model\n",
    "        model_P[k+r*N_LAMBDA] = MLP(N_INPUTS,N_HIDDEN_NEURONS,N_OUTPUTS,device).to(device) # create the model\n",
    "        l2_reg = torch.tensor(1,device=device) # create the l2 regularization value tensor\n",
    "        optimizer = torch.optim.SGD(model_P[k+r*N_LAMBDA].parameters(), lr=lr, momentum=momentum) # set the function for SGD\n",
    "        criterion = nn.CrossEntropyLoss() # set the loss function\n",
    "        \n",
    "        # note that cross-entropy loss expects the indices of the class, not the one-hot. So, for A = [1,0,0,...] and B = [0,1,0,...], A is 0 and B is 1\n",
    "        \n",
    "        for epoch in range(N_EPOCHS): # for each training epoch\n",
    "            nps = 0\n",
    "            running_train_loss=0\n",
    "            running_train_acc=0\n",
    "            model_P[k+r*N_LAMBDA].train() \n",
    "            for p, param in enumerate(model_P[k+r*N_LAMBDA].parameters()): # go through all the model parameters\n",
    "                if param.requires_grad:\n",
    "                    plist = torch.flatten(param.data) # set the list of parameters\n",
    "                    for j in range(plist.size(0)):\n",
    "                        while nps < Phist_P.shape[0]:\n",
    "                            Phist_P[nps,epoch,k,r]=plist[j].item() # update the parameters\n",
    "                            nps+=1\n",
    "\n",
    "            for i, (x, y_tar) in enumerate(trainloader):\n",
    "                # print(i,x,y_tar)\n",
    "                l2_reg = 0\n",
    "                x, y_tar = x.to(device), y_tar.to(device) # x is the training set, y_tar is the output label\n",
    "                x = x-0.3\n",
    "                optimizer.zero_grad() # set gradients to 0\n",
    "                # print(x.shape)\n",
    "                y_pred = model_P[k+r*N_LAMBDA](x.view(x.shape[0],x.shape[1]*x.shape[2])) # compute the prediction. # size mismatch\n",
    "                \n",
    "                \n",
    "                loss = criterion(y_pred,y_tar) \n",
    "                for p,param in enumerate(model_P[k+r*N_LAMBDA].parameters()):\n",
    "                    if param.requires_grad and len(param.shape)==2:\n",
    "                        if param.shape[0]==N_HIDDEN_NEURONS and param.shape[1]==N_HIDDEN_NEURONS:\n",
    "                            l2_reg = l2_reg + param[:gidx,:gidx].norm(p=1) # update the l1 regularization constant\n",
    "                            l2_reg = l2_reg + param[gidx:,gidx:].norm(p=1)\n",
    "#                         elif param.shape[1]==N_HIDDEN_NEURONS:\n",
    "#                             l2_reg = l2_reg + param[:,gidx:].norm(p=1)\n",
    "#                         elif param.shape[0]==N_HIDDEN_NEURONS:\n",
    "#                             l2_reg = l2_reg + param[:gidx,:].norm(p=1)\n",
    "                regval_P.append(l2_reg.item()) # add the l2 regularization to  the running list\n",
    "                loss = loss + l2_reg*reg_lambda/BATCH_SIZE # compute the loss\n",
    "                loss.backward() # backpropogate the loss\n",
    "                optimizer.step() # run SGD\n",
    "                running_train_loss+=loss.item()\n",
    "                running_train_acc+=get_accuracy(y_pred, y_tar) # compute accuracy\n",
    "            \n",
    "            running_test_acc=0\n",
    "            running_test_loss=0\n",
    "            model_P[k+r*N_LAMBDA].eval()\n",
    "            for i,(x_test, y_test_tar) in enumerate(testloader):\n",
    "                x_test, y_test_tar = x_test.to(device), y_test_tar.to(device)\n",
    "                x_test = x_test - 0.3\n",
    "                y_test_pred = model_P[k+r*N_LAMBDA](x_test.view(x_test.shape[0],x_test.shape[1]*x_test.shape[2]))\n",
    "                loss = criterion(y_test_pred,y_test_tar)\n",
    "                \n",
    "                running_test_loss+=loss.item()\n",
    "                running_test_acc+=get_accuracy(y_test_pred, y_test_tar)\n",
    "                \n",
    "            train_loss_P[epoch,k,r] = running_train_loss/len(trainloader)\n",
    "            train_acc_P[epoch,k,r] = running_train_acc/len(trainloader)\n",
    "            test_loss_P[epoch,k,r] = running_test_loss/len(testloader)\n",
    "            test_acc_P[epoch,k,r] = running_test_acc/len(testloader)\n",
    "            print(\"Epoch %d; rep %d; lambda %f; train accuracy %f; train loss %f; test accuracy %f; test loss %f; reg val %f\"\n",
    "                  %(epoch,\n",
    "                    r,\n",
    "                    reg_lambda,\n",
    "                    train_acc_P[epoch,k,r],\n",
    "                    train_loss_P[epoch,k,r],\n",
    "                    test_acc_P[epoch,k,r],\n",
    "                    test_loss_P[epoch,k,r],\n",
    "                   l2_reg.item()))\n",
    "            \n",
    "        # save the model and free the memory  \n",
    "        torch.save(model_P[k+r*N_LAMBDA].state_dict(), model_path)\n",
    "        model_P[k+r*N_LAMBDA] = [None]\n",
    "        del(l2_reg,loss,optimizer,criterion,plist,param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# number of lambdas\n",
    "n_epochs_plot,n_lambdas_plot = np.mean(test_acc_P,2).shape\n",
    "\n",
    "fig,ax = plt.subplots(1,2)\n",
    "fig.subplots_adjust(hspace=20.0)\n",
    "\n",
    "ax1,ax2 = ax\n",
    "cm1 = plt.get_cmap('copper') # plt.get_cmap('gist_rainbow') # plt.get_cmap('gist_rainbow')\n",
    "# fig = plt.figure()\n",
    "NUM_COLORS = n_lambdas_plot\n",
    "ax1.set_prop_cycle('color', [cm1(1.*i/NUM_COLORS) for i in range(NUM_COLORS)])\n",
    "ax2.set_prop_cycle('color', [cm1(1.*i/NUM_COLORS) for i in range(NUM_COLORS)])\n",
    "\n",
    "fig.suptitle(\"Epoch %d; reps %d; min/max lambda [%f,%f]\"\n",
    "             %(epoch,\n",
    "               N_REPS,\n",
    "               min(lambdas),\n",
    "               max(lambdas)))\n",
    "\n",
    "# train accuracy %f; train loss %f; test accuracy %f; test loss %f\"\n",
    "# train_acc_P[epoch,k,r],\n",
    "#                train_loss_P[epoch,k,r],\n",
    "#                test_acc_P[epoch,k,r],\n",
    "#                test_loss_P[epoch,k,r]\n",
    "            \n",
    "# ax = fig.add_subplot(111)\n",
    "for i in enumerate(lambdas):\n",
    "    ax1.plot(np.mean(test_acc_P,2)[:,i[0]],label=i[1],)\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Test Accuracy\")\n",
    "    \n",
    "# ax2 = fig.add_subplot(011)\n",
    "for i in enumerate(lambdas):\n",
    "    ax2.plot(np.mean(test_loss_P,2)[:,i[0]],label=i[1],)\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(\"Test Loss\")\n",
    "ax2.legend(loc=\"upper right\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(regval_P)\n",
    "regval_P_tensor = torch.tensor(regval_P).view(N_EPOCHS,len(lambdas),len(trainloader))\n",
    "plt.plot(regval_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # create colormap\n",
    "\n",
    "\n",
    "\n",
    "# #plt.imshow(x[0,:,:])\n",
    "# #plt.plot(y_pred.detach().numpy()[0,:])\n",
    "# #torch.max(y_pred,1)\n",
    "# plt.figure(1)\n",
    "# plt.plot(np.mean(test_acc_P,2))\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Test accuracy\")\n",
    "# plt.plot()\n",
    "\n",
    "# plt.figure(2)\n",
    "# plt.plot(np.mean(test_loss_P,2))\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Test loss\")\n",
    "# plt.plot()\n",
    "\n",
    "\n",
    "# for i,j in enumerate(zip(np.mean(test_acc_P,2),np.mean(test_loss_P,2))):\n",
    "#     # plt.figure(i+3) # Here's the part I need, but numbering starts at 1!\n",
    "#     fig,axs = plt.subplots(1,2)\n",
    "#     fig.suptitle(\"Epoch %d; reps %d; lambda %f; train accuracy %f; train loss %f; test accuracy %f; test loss %f\"\n",
    "#                   %(epoch,\n",
    "#                     REPS,\n",
    "#                     k,\n",
    "#                     train_acc_P[epoch,k,r],\n",
    "#                     train_loss_P[epoch,k,r],\n",
    "#                     test_acc_P[epoch,k,r],\n",
    "#                     test_loss_P[epoch,k,r]))\n",
    "#     axs[0].plot(lambdas,j[0])\n",
    "#     axs[0].set_xlabel(\"Lambda\")\n",
    "#     axs[0].set_ylabel(\"Test accuracy\")\n",
    "#     # axs[0].title(\"Epoch %d\"%i)\n",
    "#     axs[1].plot(lambdas,j[1])\n",
    "#     axs[1].set_xlabel(\"Lambda\")\n",
    "#     axs[1].set_ylabel(\"Test loss\")\n",
    "#     # axs[1].title(\"Epoch %d\"%i)\n",
    "\n",
    "\n",
    "# # plt.plot(np.mean(test_acc_P,1))\n",
    "# # plt.plot()\n",
    "\n",
    "# # plt.plot(np.mean(test_acc_P,2))\n",
    "# # plt.plot()\n",
    "\n",
    "# # plt.plot(np.mean(test_acc_P,3))\n",
    "# # plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def readtxt(txt_name = 'anna.txt'):\n",
    "#     dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "#     txt_file = os.path.join(dir_path,txt_name)\n",
    "#     # load the whole book\n",
    "#     file = open(self.txt_file)\n",
    "#     alltxt = file.read()\n",
    "#     # remove newline formmating\n",
    "#     alltxt = alltxt.replace(\"\\n\\n\", \"&\").replace(\"\\n\", \" \").replace(\"&\", \"\\n\")\n",
    "#     # define categories\n",
    "#     categories = list(sorted(set(alltxt)))\n",
    "#     # integer encode\n",
    "#     label_encoder = LabelEncoder()\n",
    "#     label_encoder.fit(categories)\n",
    "#     integer_encoded = torch.LongTensor(label_encoder.transform(list(alltxt)))\n",
    "#     return integer_encoded, categories\n",
    "\n",
    "# # def onehotencode(integer_encoded_batch,n_cat):\n",
    "    \n",
    "# # def get_next_batch(dat,batch_size):\n",
    "# #     x_int = \n",
    "# #     y_int = \n",
    "# #     x_hot = onehotencode(x_int): \n",
    "# #     return x_hot, y_int \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# BATCH_SIZE = 500 # how many batches we are running\n",
    "# N_STEPS = 10 # How many characters are we inputting into the list at a time\n",
    "# N_HIDDEN_NEURONS = 512 # how many neurons per hidden layer\n",
    "# N_INPUTS = 77*N_STEPS\n",
    "# N_OUTPUTS = 77\n",
    "# N_LAYERS = 2 # 2 hidden layers\n",
    "# N_EPOCHS = 11 # how many training epocs\n",
    "# learning_rates = np.asarray([2]) # learning rates\n",
    "# N_REPS = 3 # len(learning_rates) # the number of learning repetitions\n",
    "# N_PARAMS = nparam_MLP(N_INPUTS,N_HIDDEN_NEURONS,N_OUTPUTS)\n",
    "# gidx = int(N_HIDDEN_NEURONS/2)\n",
    "\n",
    "\n",
    "# train_loss_P = np.zeros((N_EPOCHS,N_LAMBDA,N_REPS)) \n",
    "# train_acc_P = np.zeros((N_EPOCHS,N_LAMBDA,N_REPS))\n",
    "# test_loss_P = np.zeros((N_EPOCHS,N_LAMBDA,N_REPS))\n",
    "# test_acc_P = np.zeros((N_EPOCHS,N_LAMBDA,N_REPS))\n",
    "# Phist_P = np.zeros((N_PARAMS,N_EPOCHS,N_LAMBDA,N_REPS))\n",
    "\n",
    "# model_P = [None]*N_LAMBDA*N_REPS # array of models\n",
    "# regval_P = [] # array of regularization values\n",
    "\n",
    "\n",
    "\n",
    "pickle.dump([lambdas,N_EPOCHS,N_REPS,N_HIDDEN_NEURONS,learning_rates,N_REPS,N_PARAMS,\n",
    "             model_P,regval_P,\n",
    "             train_loss_P,train_acc_P,\n",
    "             test_loss_P,test_acc_P,\n",
    "             Phist_P], \n",
    "            open(modelkey+\"_mlp_ak_quickset.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(test_acc_P.shape)\n",
    "# plt.plot(np.mean(test_acc_P,2))\n",
    "# plt.plot()\n",
    "# print(np.mean(test_acc_P,1))\n",
    "# print(np.mean(test_acc_P,1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of tensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
