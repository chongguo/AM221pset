{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from tqdm import tnrange\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "%matplotlib inline  \n",
    "\n",
    "# alphabet\n",
    "import string\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(logit, target):\n",
    "    batch_size = len(target)\n",
    "    corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
    "    accuracy = 100.0 * corrects/batch_size\n",
    "    return accuracy.item()\n",
    "\n",
    "def nparam(ninputs,nhidden,noutputs):\n",
    "    return ninputs*(nhidden+1) + nhidden*(nhidden+1)+nhidden*(noutputs+1)\n",
    "\n",
    "# define the nnumber of parameters we need\n",
    "def nparam_MLP(N_INPUTS,N_HIDDEN,N_OUTPUTS):\n",
    "    input_to_hidden1 = (N_INPUTS+1)*N_HIDDEN #+1 for bias\n",
    "    hidden1_to_hidden2 = (N_HIDDEN + 1)*N_HIDDEN\n",
    "    hidden2_to_output = (N_OUTPUTS)*(N_HIDDEN+1)\n",
    "    return(sum([input_to_hidden1,hidden1_to_hidden2,hidden2_to_output]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a prototype MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden_neurons, n_output,  device):\n",
    "        super(MLP, self).__init__()\n",
    "        self.n_inputs = n_inputs # set the number of neurons in the input layer\n",
    "        self.n_hidden_neurons = n_hidden_neurons # how many neurons are in each hidden layer\n",
    "        self.n_output = n_output # set the number of neurons in the output layer\n",
    "        self.sig = nn.Sigmoid() # set the activation function \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.n_hidden = n_hidden_neurons\n",
    "        self.encoder = nn.Linear(n_inputs, n_hidden_neurons) # encode input\n",
    "        self.recurrent = nn.Linear(n_hidden_neurons,n_hidden_neurons) # recurrent connections\n",
    "        self.decoder = nn.Linear(n_hidden_neurons, n_output) # decode output\n",
    "                \n",
    "    def forward(self, x0):\n",
    "        self.hidden1 = self.sig(self.encoder(x0))\n",
    "        self.hidden2 = self.sig(self.recurrent(self.hidden1))\n",
    "        self.output = self.decoder(self.hidden2)\n",
    "        return self.output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MLP on Anna Karenina\n",
    "# Load Anna Karenina\n",
    "from torch.utils.data import DataLoader # dataloader \n",
    "import sys\n",
    "sys.path.insert(0,'../final_project/Data/')\n",
    "from AnnaDataset_MLP import AnnaDataset, InvertAnna # import AK dataset\n",
    "\n",
    "# params\n",
    "BATCH_SIZE = 500 # how many batches we are running\n",
    "N_STEPS = 10 # How many characters are we inputting into the list at a time\n",
    "N_HIDDEN_NEURONS = 512 # how many neurons per hidden layer\n",
    "N_INPUTS = 77*N_STEPS\n",
    "N_OUTPUTS = 77\n",
    "N_LAYERS = 2 # 2 hidden layers\n",
    "N_EPOCHS = 11 # how many training epocs\n",
    "learning_rates = np.asarray([1e-2]) # learning rates\n",
    "N_REPS = 2 # len(learning_rates) # the number of learning repetitions\n",
    "N_PARAMS = nparam_MLP(N_INPUTS,N_HIDDEN_NEURONS,N_OUTPUTS)\n",
    "gidx = int(N_HIDDEN_NEURONS/2)\n",
    "\n",
    "# regularization parameters\n",
    "lambdas = [0,5]#np.arange(0,1e-2,3e-3,dtype=np.float)\n",
    "N_LAMBDA = len(lambdas)\n",
    "\n",
    "# load data\n",
    "dataset = AnnaDataset(N_STEPS) # load the dataset\n",
    "trainloader = DataLoader(dataset, batch_size=BATCH_SIZE,\n",
    "                        shuffle=False, num_workers=4) # create a DataLoader. We want a batch of BATCH_SIZE entries\n",
    "testloader = DataLoader(dataset, batch_size=BATCH_SIZE,\n",
    "                        shuffle=False, num_workers=4) # create a DataLoader. We want a batch of BATCH_SIZE entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0af1589667f44c8882b258c875455ab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ab32434856490bab813483cfae747a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "964293996e234f9b8387e3063a34f22e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-bcda25a52d05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mmodel_P\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mN_LAMBDA\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_tar\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                 \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_tar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_tar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m                 \u001b[0my_test_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_P\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mN_LAMBDA\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test_tar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# regularizing digonal blocks of the partitioned RNN\n",
    "# initialize arrays of loss values and weights over the number of epohcs, the number of lambdas we are testing, and the number of reps. \n",
    "train_loss_P = np.zeros((N_EPOCHS,N_LAMBDA,N_REPS)) \n",
    "train_acc_P = np.zeros((N_EPOCHS,N_LAMBDA,N_REPS))\n",
    "test_loss_P = np.zeros((N_EPOCHS,N_LAMBDA,N_REPS))\n",
    "test_acc_P = np.zeros((N_EPOCHS,N_LAMBDA,N_REPS))\n",
    "Phist_P = np.zeros((N_PARAMS,N_EPOCHS,N_LAMBDA,N_REPS))\n",
    "\n",
    "model_P = [None]*N_LAMBDA*N_REPS # array of models\n",
    "regval_P = [] # array of regularization values\n",
    "for r in tnrange(N_REPS): # loop over the number of reps\n",
    "    for k in tnrange(N_LAMBDA): # loop over the number of different lambda values\n",
    "        reg_lambda = lambdas[k] # set the regularization lambda\n",
    "        model_path = './model_P_rep_{}_lambda_{:d}_10.pt'.format(r,int(reg_lambda*10)) # path to which we will save the model\n",
    "        model_P[k+r*N_LAMBDA] = MLP(N_INPUTS,N_HIDDEN_NEURONS,N_OUTPUTS,device).to(device) # create the model\n",
    "        l2_reg = torch.tensor(1,device=device) # create the l2 regularization value tensor\n",
    "        optimizer = torch.optim.SGD(model_P[k+r*N_LAMBDA].parameters(), lr=1e-2, momentum=0.9) # set the function for SGD\n",
    "        criterion = nn.CrossEntropyLoss() # set the loss function\n",
    "        \n",
    "        # note that cross-entropy loss expects the indices of the class, not the one-hot. So, for A = [1,0,0,...] and B = [0,1,0,...], A is 0 and B is 1\n",
    "        \n",
    "        for epoch in tnrange(N_EPOCHS): # for each training epoch\n",
    "            nps = 0\n",
    "            running_train_loss=0\n",
    "            running_train_acc=0\n",
    "            model_P[k+r*N_LAMBDA].train() \n",
    "            for p, param in enumerate(model_P[k+r*N_LAMBDA].parameters()): # go through all the model parameters\n",
    "                if param.requires_grad:\n",
    "                    plist = torch.flatten(param.data) # set the list of parameters\n",
    "                    for j in range(plist.size(0)):\n",
    "                        while nps < Phist_P.shape[0]:\n",
    "                            Phist_P[nps,epoch,k,r]=plist[j].item() # update the parameters\n",
    "                            nps+=1\n",
    "\n",
    "            for i, (x, y_tar) in enumerate(trainloader):\n",
    "                l2_reg = 0\n",
    "                x, y_tar = x.to(device), y_tar.to(device) # x is the training set, y_tar is the output label\n",
    "                optimizer.zero_grad() # set gradients to 0\n",
    "                y_pred = model_P[k+r*N_LAMBDA](x.view(x.shape[0],x.shape[1]*x.shape[2])) # compute the prediction. \n",
    "                loss = criterion(y_pred,y_tar) \n",
    "                for p,param in enumerate(model_P[k+r*N_LAMBDA].parameters()):\n",
    "                    if param.requires_grad and len(param.shape)==2:\n",
    "                        if param.shape[0]==N_HIDDEN_NEURONS and param.shape[1]==N_HIDDEN_NEURONS:\n",
    "                            l2_reg = l2_reg + param[:gidx,:gidx].norm(p=1) # update the l2 regularization constant\n",
    "                            l2_reg = l2_reg + param[gidx:,gidx:].norm(p=1)\n",
    "                        elif param.shape[1]==N_HIDDEN_NEURONS:\n",
    "                            l2_reg = l2_reg + param[:,gidx:].norm(p=1)\n",
    "                        elif param.shape[0]==N_HIDDEN_NEURONS:\n",
    "                            l2_reg = l2_reg + param[:gidx,:].norm(p=1)\n",
    "                regval_P.append(l2_reg.item()) # add the l2 regularization to  the running list\n",
    "                loss = loss + l2_reg*reg_lambda/BATCH_SIZE # compute the loss\n",
    "                loss.backward() # backpropogate the loss\n",
    "                optimizer.step() # run SGD\n",
    "                running_train_loss+=loss.item()\n",
    "                running_train_acc+=get_accuracy(y_pred, y_tar) # compute accuracy\n",
    "            \n",
    "            running_test_acc=0\n",
    "            running_test_loss=0\n",
    "            model_P[k+r*N_LAMBDA].eval()\n",
    "            for i,(x_test, y_test_tar) in enumerate(testloader):\n",
    "                x_test, y_test_tar = x_test.to(device), y_test_tar.to(device)\n",
    "                y_test_pred = model_P[k+r*N_LAMBDA](x_test.view(x_test.shape[0],x_test.shape[1]*x_test.shape[2]))\n",
    "                loss = criterion(y_test_pred,y_test_tar)\n",
    "                \n",
    "                running_test_loss+=loss.item()\n",
    "                running_test_acc+=get_accuracy(y_test_pred, y_test_tar)\n",
    "                \n",
    "            train_loss_P[epoch,k,r] = running_train_loss/len(trainloader)\n",
    "            train_acc_P[epoch,k,r] = running_train_acc/len(trainloader)\n",
    "            test_loss_P[epoch,k,r] = running_test_loss/len(testloader)\n",
    "            test_acc_P[epoch,k,r] = running_test_acc/len(testloader)\n",
    "            \n",
    "        # save the model and free the memory  \n",
    "        torch.save(model_P[k+r*N_LAMBDA].state_dict(), model_path)\n",
    "        model_P[k+r*N_LAMBDA] = [None]\n",
    "        del(l2_reg,loss,optimizer,criterion,plist,param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.mean(train_acc_P,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_batch():"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
