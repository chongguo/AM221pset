# -*- coding: utf-8 -*-
"""MLP_hamlet

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T3x4B2-P_i0-0vZSVsxPxyrhHSzAKX-b

# Introduction
This notebook will test the performance of a 2-layer MLP (multi-layer perceptron) in performing a character-wise prediction task on 
* MNIST
* the play *Hamlet*.
"""

# imports
from tqdm import tnrange
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
from torch.autograd import Variable
import matplotlib.pyplot as plt
import numpy as np
import gc
import os
# %matplotlib inline  

# alphabet
import string

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)

"""# MLP code

## Data preparation
"""

def onehotTensor(category,n_categories):
    tensor = torch.zeros(1, n_categories,dtype=torch.long)
    tensor[0][category] = 1
    return tensor
        
def get_accuracy(logit, target):
    batch_size = len(target)
    corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()
    accuracy = 100.0 * corrects/batch_size
    return accuracy.item()

def nparam(ninputs,nhidden,noutputs):
    return ninputs*(nhidden+1) + nhidden*(nhidden+1)+nhidden*(noutputs+1)

"""## Model"""

# a prototype MLP
class MLP(nn.Module):
    def __init__(self, n_inputs, n_hidden_neurons, n_hidden_layers, n_output, dt, device):
        super(MLP, self).__init__()
        self.n_inputs = n_inputs # set the number of neurons in the input layer
        self.n_hidden_neurons = n_hidden_neurons # how many neurons are in each hidden layer
        self.n_hidden_layers = n_hidden_layers # how many hidden layers are we using in our model
        self.n_output = n_output # set the number of neurons in the output layer
        self.dt = nn.Parameter(torch.Tensor([dt])) # set the change in parameter update
        self.a = nn.Parameter(torch.Tensor([1])) # set the bias
        self.sig = nn.Sigmoid() # set the activation function 
        self.decoder = nn.Linear(n_hidden, n_output) # decode output
        self.encoder = nn.Linear(n_inputs, n_hidden) # encode input
        
    # the main operation for the MLP is to update each hidden layer with the state of the previous hidden layer
    # so, if you need to update the hidden layers, make sure you update each layer with state of the previous layer
    
    def update_hidden_layer(self,h1_prev):
      # update the neurons in the current hidden layer with the state of the inputted "previous" layer
      for i in range(self.n_hidden_layers):
        # update each node in the h1_current
        self.h1 = nn.Linear(self.h1,self.h1) # update the stored hidden layer state as many times as there are hidden layers specified
      return(h1_current)
    
    def forward(self, x0):
        x0=x0.permute(1,0,2) # permute the tensor
        # save the hidden layers as a tensor
        
        self.hlayers = torch.zeros(self.n_hidden_layers,BATCH_SIZE,self.n_hidden).to(device) # initialize the hidden layer
        # the first layer will consist of the encoded inputs
        for i in range(x0.size(0)):
            self.hlayers[0,:,:] = self.sig(self.dt)*self.a*torch.tanh(self.encoder(x0[i,:,:])) # (1-self.sig(self.dt))*self.h1+self.sig(self.dt)*self.a*torch.tanh(self.encoder(x0[i,:,:])+self.recurrent(self.h1))
        # update the additional hidden layers
        ind_hidden = 1
        while ind_hidden < self.n_hidden_layers:
          self.hlayers[ind_hidden,:,:] = self.update_hidden_layer(self.hlayers[ind_hidden - 1,:,:])
          ind_hidden += 1

        self.y1 = self.decoder(self.hlayers[-1,:,:])
        return self.y1

"""# Load data"""

# MNIST

BATCH_SIZE = 100

# list all transformations
transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.1307,), (0.3081,))])

# download and load training dataset
trainset = torchvision.datasets.MNIST(root='./data', train=True,
                                        download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,
                                          shuffle=True, num_workers=2)

# download and load testing dataset
testset = torchvision.datasets.MNIST(root='./data', train=False,
                                       download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,
                                         shuffle=False, num_workers=2)

# training