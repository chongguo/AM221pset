{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word_prediction",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "5Xblni0t5JX2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "psXr1JJz5Nf6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Introduction:\n",
        "\n",
        "This notebook will experiment with a character-based word prediction task. We will be following the tutorial here: https://machinelearningmastery.com/develop-character-based-neural-language-model-keras/\n"
      ]
    },
    {
      "metadata": {
        "id": "T8OPgOlvJr31",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline  \n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# alphabet\n",
        "import string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "keTKOL1U6xGa",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "d727eb43-f144-4b29-9ec0-692fe908491d"
      },
      "cell_type": "code",
      "source": [
        "# load the data\n",
        "import os,sys,re\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0cec9ed3-5000-43ac-b834-dd01f1712d03\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-0cec9ed3-5000-43ac-b834-dd01f1712d03\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving six_pence.txt to six_pence (1).txt\n",
            "User uploaded file \"six_pence.txt\" with length 412 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CiwASIW_80vL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3583f945-947a-4681-934f-13e4d517f456"
      },
      "cell_type": "code",
      "source": [
        "text_chunk = 'six_pence.txt'\n",
        "poem = uploaded[text_chunk].decode(\"utf-8\")\n",
        "# strip away punctuation; keep spaces!\n",
        "poem_string = [i.replace('.','') for i in poem.split('\\n') if i is not '']\n",
        "poem_string = [i.replace(',','') for i in poem_string]\n",
        "poem_string = [i.replace(';','') for i in poem_string]\n",
        "poem_string = [i.replace('\\'','') for i in poem_string]\n",
        "\n",
        "# poem_string = [i for i in poem.split('\\n') if i is not '']\n",
        "# poem_string = [i.replace(' ','') for i in poem_string]\n",
        "poem_string = ' '.join(poem_string)\n",
        "# lower case everything\n",
        "poem_string = ''.join([i.lower() for i in poem_string])\n",
        "print(poem_string)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sing a song of sixpence a pocket full of rye four and twenty blackbirds baked in a pie when the pie was opened the birds began to sing wasnt that a dainty dish to set before the king the king was in his counting house counting out his money the queen was in the parlour eating bread and honey the maid was in the garden hanging out the clothes when down came a blackbird and pecked off her nose\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7m2arGg-7hXm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# one-hot encoding of characters\n",
        "def one_hot_character(instr,addspace=True,add_punctuation=True):\n",
        "  # generate the alphabet\n",
        "  english_alpha = string.ascii_lowercase\n",
        "  # if addspace, then incorporate into the alphabet\n",
        "  if addspace:\n",
        "    english_alpha = english_alpha+' ' # add the space\n",
        "  if add_punctuation:\n",
        "    english_alpha = english_alpha+',.;\\'' # add punctuation\n",
        "  # now, generate one-hot encoding vectors...\n",
        "#   source_Vector = np.zeros(len(english_alpha))\n",
        "  one_hot = []\n",
        "  for i,j in enumerate(instr):\n",
        "    # alphabet index\n",
        "    try:\n",
        "      alpha_ind = english_alpha.index(j)\n",
        "      source_Vector = np.zeros(len(english_alpha))\n",
        "      np.put(source_Vector,alpha_ind,float(1))\n",
        "      one_hot.append(source_Vector)\n",
        "    except IOError:\n",
        "      continue\n",
        "    else:\n",
        "      continue\n",
        "  return(one_hot)\n",
        "\n",
        "def alphabet_index_encode(instr,addspace=True,add_punctuation=True):\n",
        "  # generate the alphabet\n",
        "  english_alpha = string.ascii_lowercase\n",
        "  if addspace:\n",
        "    english_alpha = english_alpha+' ' # add the space\n",
        "  if add_punctuation:\n",
        "    english_alpha = english_alpha+',.;\\'' # add punctuation\n",
        "  # now, generate one-hot encoding vectors...\n",
        "#   source_Vector = np.zeros(len(english_alpha))\n",
        "  ind_encode = []\n",
        "  for i,j in enumerate(instr):\n",
        "    # alphabet index\n",
        "    try:\n",
        "      alpha_ind = english_alpha.index(j)\n",
        "      ind_encode.append(alpha_ind)\n",
        "    except IOError:\n",
        "      continue\n",
        "    else:\n",
        "      continue\n",
        "  return(ind_encode)\n",
        "\n",
        "\n",
        "def make_string_tensor(instr,addspace,add_punctuation,N_OUTPUT=1,N_INPUT=10):\n",
        "#   N_OUTPUT = 1\n",
        "#   N_INPUT = 10\n",
        "  total_len = N_INPUT + N_OUTPUT\n",
        "\n",
        "  # one-hot encode the poem\n",
        "#   poem_string_onehot = one_hot_character(poem_string)\n",
        "  poem_string_onehot = alphabet_index_encode(poem_string,addspace=addspace,add_punctuation=add_punctuation)\n",
        "  \n",
        "  # tile the string:\n",
        "  string_tiles = []\n",
        "  for i in range(total_len,len(poem_string_onehot)+1):\n",
        "    j = poem_string_onehot[i - total_len:i]\n",
        "    # turn this into a tensor:\n",
        "    input_tensor = torch.FloatTensor(j[:-1])\n",
        "    print(j)\n",
        "    print(input_tensor)\n",
        "    output_tensor = torch.FloatTensor([j[-1]])#,dtype=torch.float64)\n",
        "    print(output_tensor)\n",
        "    string_tiles.append([input_tensor,output_tensor])\n",
        "  return(string_tiles)\n",
        "\n",
        "def make_onehot_string_tensor(instr,addspace,add_punctuation,N_OUTPUT=1,N_INPUT=10):\n",
        "#   N_OUTPUT = 1\n",
        "#   N_INPUT = 10\n",
        "  total_len = N_INPUT + N_OUTPUT\n",
        "\n",
        "  # one-hot encode the poem\n",
        "#   poem_string_onehot = one_hot_character(poem_string)\n",
        "  poem_string_onehot = one_hot_character(poem_string,addspace=addspace,add_punctuation=add_punctuation)\n",
        "  \n",
        "  # tile the string:\n",
        "  string_tiles = []\n",
        "  for i in range(total_len,len(poem_string_onehot)+1):\n",
        "    j = poem_string_onehot[i - total_len:i]\n",
        "    # turn this into a tensor:\n",
        "    input_tensor = torch.FloatTensor(j[:-1])\n",
        "#     print(j)\n",
        "#     print(input_tensor)\n",
        "    output_tensor = torch.FloatTensor([j[-1]])#,dtype=torch.float64)\n",
        "#     print(output_tensor)\n",
        "    string_tiles.append([input_tensor,output_tensor])\n",
        "  return(string_tiles)\n",
        "\n",
        "\n",
        "\n",
        "def decode_string_vec(invec,addspace=True):\n",
        "  # generate the alphabet\n",
        "  english_alpha = string.ascii_lowercase\n",
        "  # if addspace, then incorporate into the alphabet\n",
        "  if addspace:\n",
        "    english_alpha = english_alpha+' ' # add the space\n",
        "  #\n",
        "  outchar = english_alpha[list(invec).index(1)]\n",
        "  return(outchar)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mvQ0Daib--JL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## INPUT preparation\n",
        "# one-hot encode the poem string\n",
        "# break the text chunks into N_INPUT input characters and N_OUTPUT output characters\n",
        "# poem_string_input = make_string_tensor(poem_string,addspace=True,add_punctuation=False)\n",
        "# poem_string_input = make_string_tensor(poem_string)\n",
        "\n",
        "# cast as float64\n",
        "# tensor.cast(poem_string_input, tf.float64)\n",
        "# # test decoding!\n",
        "# test_decode_out = []\n",
        "# for i in poem_string_input:\n",
        "#   outvec = i[1]\n",
        "#   test_decode_out.append(decode_string_vec(outvec))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "85Z-JDhhKuB9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "79300546-f888-43e4-ef9e-2df8ca05e241"
      },
      "cell_type": "code",
      "source": [
        "print(poem_string_input[1])\n",
        "print(len(poem_string_input))"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[tensor([ 8., 13.,  6., 26.,  0., 26., 18., 14., 13.,  6.]), tensor([26.])]\n",
            "384\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_s2FOk_UJmJc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# generate trainloader and testloader sets\n",
        "BATCH_SIZE = 12\n",
        "\n",
        "# # list all transformations\n",
        "# transform = transforms.Compose(\n",
        "#     [transforms.ToTensor(),\n",
        "#      transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "# # download and load training dataset\n",
        "# trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
        "#                                         download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(poem_string_input, batch_size=BATCH_SIZE,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "# # download and load testing dataset\n",
        "# testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "#                                        download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(poem_string_input, batch_size=BATCH_SIZE,\n",
        "                                         shuffle=False, num_workers=2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xGI4rlFm-Uss",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "82d690a8-c41a-4aa8-cdcc-9f6e55f4ccd9"
      },
      "cell_type": "code",
      "source": [
        "print(trainloader)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc66e685160>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PJbX-DQ91JXq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# character/word RNN\n",
        "# include a method in the class to perform one-hot encoding of inputs and outputs.\n",
        "\n",
        "class PRNN(nn.Module):\n",
        "    def __init__(self, n_inputs, n_hidden,n_output,mr):\n",
        "        super(PRNN, self).__init__()\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_output = n_output\n",
        "        self.mr = mr\n",
        "        self.encoder = nn.Linear(n_inputs,n_hidden)\n",
        "        self.recurrent = nn.Linear(n_hidden,n_hidden)\n",
        "        self.decoder = nn.Linear(n_hidden, n_output)\n",
        "        self.rnn = nn.RNN(n_inputs, n_hidden)\n",
        "        \n",
        "    def forward(self, x0):\n",
        "        x0=x0.permute(1,0,2)\n",
        "        self.h1 = torch.zeros(1,BATCH_SIZE,self.n_hidden)\n",
        "        #self.h1 = Variable(torch.zeros(x0.size(0), self.n_hidden))\n",
        "        #for t in range(T):\n",
        "           #self.h1 = self.mr*self.h1+(1-self.mr)*torch.relu(self.encoder(x0[:,t+7,:])+self.recurrent(self.h1))\n",
        "        self.y0, self.h1 = self.rnn(x0,self.h1)\n",
        "        self.y1 = self.decoder(self.h1[0])\n",
        "        \n",
        "        return self.y1\n",
        "# can I modify this to set an arbitrary number of layers?\n",
        "        \n",
        "# RNN LSTM  \n",
        "class PRNN_LSTM(nn.Module):\n",
        "    def __init__(self, n_inputs, n_hidden,n_output,mr):\n",
        "        super(PRNN_LSTM, self).__init__()\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_output = n_output\n",
        "        self.mr = mr\n",
        "#         self.encoder = nn.Linear(n_inputs,n_hidden)\n",
        "#         self.recurrent = nn.Linear(n_hidden,n_hidden)\n",
        "        self.decoder = nn.Linear(n_hidden, n_output)\n",
        "        self.lstm = nn.LSTM(n_inputs, n_hidden)\n",
        "        \n",
        "    def forward(self, x0):\n",
        "        x0=x0.permute(1,0,2)\n",
        "        self.h1 = (torch.zeros(1,BATCH_SIZE,self.n_hidden),torch.zeros(1,BATCH_SIZE,self.n_hidden))\n",
        "        #self.h1 = Variable(torch.zeros(x0.size(0), self.n_hidden))\n",
        "        #for t in range(T):\n",
        "           #self.h1 = self.mr*self.h1+(1-self.mr)*torch.relu(self.encoder(x0[:,t+7,:])+self.recurrent(self.h1))\n",
        "        self.y0, self.h1 = self.lstm(x0,self.h1)\n",
        "        self.y1 = self.decoder(self.h1[0])\n",
        "        \n",
        "        return self.y1\n",
        "# can I modify this to set an arbitrary number of layers?\n",
        "\n",
        "# accuracy and one-hot encoding functions\n",
        "def onehotTensor(category,n_categories):\n",
        "    tensor = torch.zeros(1, n_categories,dtype=torch.long)\n",
        "    tensor[0][category] = 1\n",
        "    return tensor\n",
        "        \n",
        "def get_accuracy(logit, target, batch_size):\n",
        "  # target.size() and logit have incompatible dimensions?\n",
        "  print(torch.max(logit, 1)[1])\n",
        "  print(target.size())\n",
        "#  print(torch.max(logit, 1)[1].view(target.size()).data) # the formulation of torch.max(logit, 1)[1].view(target.size()).data has an error \n",
        "  corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum() # input size is the same as batch size. \n",
        "  accuracy = 100.0 * corrects/batch_size\n",
        "  return accuracy.item()\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pQIxqCsf0xW_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "cdcdb89f-555c-4ea2-e84a-0a021a19c03b"
      },
      "cell_type": "code",
      "source": [
        "# run Vanilla RNN\n",
        "# parameters \n",
        "N_STEPS = 10\n",
        "N_INPUTS = 1\n",
        "N_HIDDEN = 20\n",
        "N_OUTPUTS = 10\n",
        "N_EPHOCS = 50\n",
        "\n",
        "model = PRNN(N_INPUTS,N_HIDDEN,N_OUTPUTS,0.1)\n",
        "# model = MyModel()\n",
        "# model.cuda()\n",
        "\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1, momentum=0.9)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "criterion = nn.MSELoss() # this does not work\n",
        "\n",
        "train_running_loss = np.zeros(N_EPHOCS)\n",
        "train_acc = np.zeros(N_EPHOCS)\n",
        "nparams = 0\n",
        "for param in model.parameters(): \n",
        "  if param.requires_grad:\n",
        "    nparams += param.data.numpy().size\n",
        "Phist = np.zeros((nparams,N_EPHOCS))\n",
        "\n",
        "for epoch in range(N_EPHOCS):\n",
        "    nps = 0\n",
        "    running_loss=0\n",
        "    running_acc=0\n",
        "    for p,param in enumerate(model.parameters()):\n",
        "        if param.requires_grad:\n",
        "            plist = param.data.numpy().flatten()\n",
        "            for j in range(plist.size):\n",
        "                Phist[nps,epoch]=plist[j]\n",
        "                nps+=1\n",
        "  \n",
        "    for i, data in enumerate(trainloader):\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        x, y_tar = data\n",
        "#         print(x.shape)\n",
        "#         print(y_tar.shape)\n",
        "# #         y_tar = torch.tensor(y_tar, dtype=torch.long)\n",
        "\n",
        "#         print(y_tar)\n",
        "#         print(y_tar.size())\n",
        "        \n",
        "        y_pred = model(x.view(BATCH_SIZE,N_STEPS,N_INPUTS))\n",
        "#         print(y_pred.view(BATCH_SIZE,N_OUTPUTS).shape)\n",
        "        loss = criterion(y_pred.view(BATCH_SIZE,N_OUTPUTS).float(),y_tar.float())\n",
        "#         print(loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss+=loss.item()\n",
        "        running_acc+=get_accuracy(y_pred.view(BATCH_SIZE,N_OUTPUTS).float(), y_tar.long(), BATCH_SIZE)\n",
        "    train_running_loss[epoch] = running_loss\n",
        "    train_acc[epoch] = running_acc/i\n",
        "    \n",
        "    print('Epoch:  %d | Loss: %.4f | Train Accuracy: %.2f' %(epoch, train_running_loss[epoch], train_acc[epoch]))"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  0 | Loss: 3991.0631 | Train Accuracy: 5.23\n",
            "Epoch:  1 | Loss: 2864.2491 | Train Accuracy: 3.87\n",
            "Epoch:  2 | Loss: 3457.1776 | Train Accuracy: 5.45\n",
            "Epoch:  3 | Loss: 2697.7201 | Train Accuracy: 4.42\n",
            "Epoch:  4 | Loss: 2834.0418 | Train Accuracy: 3.35\n",
            "Epoch:  5 | Loss: 3142.8443 | Train Accuracy: 4.65\n",
            "Epoch:  6 | Loss: 2900.7627 | Train Accuracy: 6.45\n",
            "Epoch:  7 | Loss: 3319.8489 | Train Accuracy: 5.71\n",
            "Epoch:  8 | Loss: 3118.2663 | Train Accuracy: 4.68\n",
            "Epoch:  9 | Loss: 2824.9259 | Train Accuracy: 4.42\n",
            "Epoch:  10 | Loss: 2675.3225 | Train Accuracy: 6.32\n",
            "Epoch:  11 | Loss: 2787.0011 | Train Accuracy: 5.19\n",
            "Epoch:  12 | Loss: 3141.9943 | Train Accuracy: 5.45\n",
            "Epoch:  13 | Loss: 3166.6565 | Train Accuracy: 3.61\n",
            "Epoch:  14 | Loss: 2918.6796 | Train Accuracy: 5.48\n",
            "Epoch:  15 | Loss: 2791.3631 | Train Accuracy: 6.26\n",
            "Epoch:  16 | Loss: 2771.0171 | Train Accuracy: 2.84\n",
            "Epoch:  17 | Loss: 3029.7013 | Train Accuracy: 4.94\n",
            "Epoch:  18 | Loss: 2816.1548 | Train Accuracy: 2.58\n",
            "Epoch:  19 | Loss: 2658.4312 | Train Accuracy: 3.35\n",
            "Epoch:  20 | Loss: 2700.5322 | Train Accuracy: 3.35\n",
            "Epoch:  21 | Loss: 2630.6242 | Train Accuracy: 3.61\n",
            "Epoch:  22 | Loss: 3010.7941 | Train Accuracy: 2.84\n",
            "Epoch:  23 | Loss: 2966.6400 | Train Accuracy: 4.16\n",
            "Epoch:  24 | Loss: 3087.4032 | Train Accuracy: 1.81\n",
            "Epoch:  25 | Loss: 2982.0017 | Train Accuracy: 2.87\n",
            "Epoch:  26 | Loss: 2606.3731 | Train Accuracy: 3.13\n",
            "Epoch:  27 | Loss: 2623.7998 | Train Accuracy: 3.65\n",
            "Epoch:  28 | Loss: 2915.5846 | Train Accuracy: 3.13\n",
            "Epoch:  29 | Loss: 2958.0009 | Train Accuracy: 4.42\n",
            "Epoch:  30 | Loss: 2629.0109 | Train Accuracy: 2.06\n",
            "Epoch:  31 | Loss: 2570.3959 | Train Accuracy: 3.35\n",
            "Epoch:  32 | Loss: 3188.1396 | Train Accuracy: 4.42\n",
            "Epoch:  33 | Loss: 3301.1880 | Train Accuracy: 4.42\n",
            "Epoch:  34 | Loss: 2634.9268 | Train Accuracy: 4.68\n",
            "Epoch:  35 | Loss: 2584.1646 | Train Accuracy: 2.58\n",
            "Epoch:  36 | Loss: 2840.7427 | Train Accuracy: 3.87\n",
            "Epoch:  37 | Loss: 3079.1650 | Train Accuracy: 3.90\n",
            "Epoch:  38 | Loss: 4056.4061 | Train Accuracy: 3.61\n",
            "Epoch:  39 | Loss: 3828.0810 | Train Accuracy: 3.87\n",
            "Epoch:  40 | Loss: 2624.6422 | Train Accuracy: 3.65\n",
            "Epoch:  41 | Loss: 3501.4473 | Train Accuracy: 3.39\n",
            "Epoch:  42 | Loss: 2848.0807 | Train Accuracy: 5.45\n",
            "Epoch:  43 | Loss: 2828.0099 | Train Accuracy: 5.97\n",
            "Epoch:  44 | Loss: 3391.0890 | Train Accuracy: 3.87\n",
            "Epoch:  45 | Loss: 3494.7430 | Train Accuracy: 2.84\n",
            "Epoch:  46 | Loss: 2714.8157 | Train Accuracy: 4.39\n",
            "Epoch:  47 | Loss: 3006.5029 | Train Accuracy: 4.94\n",
            "Epoch:  48 | Loss: 3811.8476 | Train Accuracy: 3.10\n",
            "Epoch:  49 | Loss: 3066.2800 | Train Accuracy: 3.90\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8E9fRdJbHbDd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "fd17ffa2-b9d5-44ea-e237-41600a46834c"
      },
      "cell_type": "code",
      "source": [
        "# with RNN-LSTM\n",
        "# run Vanilla RNN\n",
        "# parameters \n",
        "N_STEPS = 10\n",
        "N_INPUTS = 1\n",
        "N_HIDDEN = 20\n",
        "N_OUTPUTS = 10\n",
        "N_EPHOCS = 50\n",
        "\n",
        "model = PRNN_LSTM(N_INPUTS,N_HIDDEN,N_OUTPUTS,0.1)\n",
        "# model = MyModel()\n",
        "# model.cuda()\n",
        "\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1, momentum=0.9)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "criterion = nn.MSELoss() # this does not work\n",
        "\n",
        "train_running_loss = np.zeros(N_EPHOCS)\n",
        "train_acc = np.zeros(N_EPHOCS)\n",
        "nparams = 0\n",
        "for param in model.parameters(): \n",
        "  if param.requires_grad:\n",
        "    nparams += param.data.numpy().size\n",
        "Phist = np.zeros((nparams,N_EPHOCS))\n",
        "\n",
        "for epoch in range(N_EPHOCS):\n",
        "    nps = 0\n",
        "    running_loss=0\n",
        "    running_acc=0\n",
        "    for p,param in enumerate(model.parameters()):\n",
        "        if param.requires_grad:\n",
        "            plist = param.data.numpy().flatten()\n",
        "            for j in range(plist.size):\n",
        "                Phist[nps,epoch]=plist[j]\n",
        "                nps+=1\n",
        "  \n",
        "    for i, data in enumerate(trainloader):\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        x, y_tar = data\n",
        "#         y_tar = torch.tensor(y_tar, dtype=torch.long)\n",
        "\n",
        "#         print(y_tar)\n",
        "#         print(y_tar.size())\n",
        "        \n",
        "        y_pred = model(x.view(BATCH_SIZE,N_STEPS,N_INPUTS))\n",
        "#         print(y_pred.view(BATCH_SIZE,N_OUTPUTS).shape)\n",
        "        loss = criterion(y_pred.view(BATCH_SIZE,N_OUTPUTS).float(),y_tar.float())\n",
        "#         print(loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss+=loss.item()\n",
        "        running_acc+=get_accuracy(y_pred.view(BATCH_SIZE,N_OUTPUTS).float(), y_tar.long(), BATCH_SIZE)\n",
        "    train_running_loss[epoch] = running_loss\n",
        "    train_acc[epoch] = running_acc/i\n",
        "    \n",
        "    print('Epoch:  %d | Loss: %.4f | Train Accuracy: %.2f' %(epoch, train_running_loss[epoch], train_acc[epoch]))"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  0 | Loss: 3699.7419 | Train Accuracy: 4.97\n",
            "Epoch:  1 | Loss: 2621.6779 | Train Accuracy: 2.06\n",
            "Epoch:  2 | Loss: 2694.0456 | Train Accuracy: 2.84\n",
            "Epoch:  3 | Loss: 2624.5346 | Train Accuracy: 5.74\n",
            "Epoch:  4 | Loss: 2525.7677 | Train Accuracy: 6.48\n",
            "Epoch:  5 | Loss: 2675.7388 | Train Accuracy: 5.16\n",
            "Epoch:  6 | Loss: 2663.7953 | Train Accuracy: 3.10\n",
            "Epoch:  7 | Loss: 2710.3007 | Train Accuracy: 3.87\n",
            "Epoch:  8 | Loss: 2596.5698 | Train Accuracy: 3.61\n",
            "Epoch:  9 | Loss: 2712.3929 | Train Accuracy: 3.61\n",
            "Epoch:  10 | Loss: 2602.5027 | Train Accuracy: 2.06\n",
            "Epoch:  11 | Loss: 2699.2000 | Train Accuracy: 2.06\n",
            "Epoch:  12 | Loss: 2645.8136 | Train Accuracy: 2.32\n",
            "Epoch:  13 | Loss: 3335.0647 | Train Accuracy: 4.39\n",
            "Epoch:  14 | Loss: 2860.9734 | Train Accuracy: 4.90\n",
            "Epoch:  15 | Loss: 2634.0460 | Train Accuracy: 3.10\n",
            "Epoch:  16 | Loss: 2649.8090 | Train Accuracy: 6.77\n",
            "Epoch:  17 | Loss: 2966.6416 | Train Accuracy: 5.19\n",
            "Epoch:  18 | Loss: 2583.3132 | Train Accuracy: 2.58\n",
            "Epoch:  19 | Loss: 2593.4843 | Train Accuracy: 5.71\n",
            "Epoch:  20 | Loss: 2612.2296 | Train Accuracy: 8.29\n",
            "Epoch:  21 | Loss: 2730.4835 | Train Accuracy: 6.00\n",
            "Epoch:  22 | Loss: 3003.7069 | Train Accuracy: 4.68\n",
            "Epoch:  23 | Loss: 2708.4485 | Train Accuracy: 5.81\n",
            "Epoch:  24 | Loss: 2858.9836 | Train Accuracy: 6.45\n",
            "Epoch:  25 | Loss: 2968.9923 | Train Accuracy: 7.52\n",
            "Epoch:  26 | Loss: 2605.2316 | Train Accuracy: 4.65\n",
            "Epoch:  27 | Loss: 2507.4600 | Train Accuracy: 6.48\n",
            "Epoch:  28 | Loss: 3472.0970 | Train Accuracy: 6.77\n",
            "Epoch:  29 | Loss: 2615.7063 | Train Accuracy: 6.52\n",
            "Epoch:  30 | Loss: 2585.9280 | Train Accuracy: 2.32\n",
            "Epoch:  31 | Loss: 2603.6561 | Train Accuracy: 2.06\n",
            "Epoch:  32 | Loss: 2761.8510 | Train Accuracy: 2.58\n",
            "Epoch:  33 | Loss: 2530.0336 | Train Accuracy: 4.13\n",
            "Epoch:  34 | Loss: 2632.3074 | Train Accuracy: 2.84\n",
            "Epoch:  35 | Loss: 2679.3463 | Train Accuracy: 3.61\n",
            "Epoch:  36 | Loss: 2703.7192 | Train Accuracy: 3.87\n",
            "Epoch:  37 | Loss: 2550.1656 | Train Accuracy: 3.61\n",
            "Epoch:  38 | Loss: 2716.9445 | Train Accuracy: 5.74\n",
            "Epoch:  39 | Loss: 2643.4325 | Train Accuracy: 1.03\n",
            "Epoch:  40 | Loss: 2650.3559 | Train Accuracy: 3.68\n",
            "Epoch:  41 | Loss: 2616.8211 | Train Accuracy: 4.13\n",
            "Epoch:  42 | Loss: 2646.4924 | Train Accuracy: 3.65\n",
            "Epoch:  43 | Loss: 2942.6874 | Train Accuracy: 5.45\n",
            "Epoch:  44 | Loss: 2874.7561 | Train Accuracy: 4.68\n",
            "Epoch:  45 | Loss: 2531.3343 | Train Accuracy: 3.87\n",
            "Epoch:  46 | Loss: 2600.4152 | Train Accuracy: 3.65\n",
            "Epoch:  47 | Loss: 3048.9763 | Train Accuracy: 4.71\n",
            "Epoch:  48 | Loss: 2618.5271 | Train Accuracy: 4.16\n",
            "Epoch:  49 | Loss: 2868.3539 | Train Accuracy: 4.65\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "I62LOimvQyUW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Both aren't working very well. Let's try the following:\n",
        "\n",
        "\n",
        "1.   Use one-hot encoding and go back to CrossEntropy Loss Criterion\n",
        "2.   Expand the number of input/output characters. \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "PX4OHdmLjF_A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# switch to one-hot encodings\n",
        "poem_string_onehot_input = make_onehot_string_tensor(poem_string,addspace=True,add_punctuation=False)\n",
        "\n",
        "# train/test loader\n",
        "# generate trainloader and testloader sets\n",
        "BATCH_SIZE = 96\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(poem_string_onehot_input, batch_size=BATCH_SIZE,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "# # download and load testing dataset\n",
        "# testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "#                                        download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(poem_string_onehot_input, batch_size=BATCH_SIZE,\n",
        "                                         shuffle=False, num_workers=2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PTx6IpzvlSZr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "outputId": "7568c3ad-50da-45a0-9756-7a896a838ad2"
      },
      "cell_type": "code",
      "source": [
        "# run Vanilla RNN\n",
        "# parameters \n",
        "N_STEPS = 27\n",
        "N_INPUTS = 10\n",
        "N_HIDDEN = 10\n",
        "N_OUTPUTS = 27\n",
        "N_EPHOCS = 50\n",
        "\n",
        "# N_STEPS = 28\n",
        "# N_INPUTS = 28\n",
        "# N_HIDDEN = 100\n",
        "# N_OUTPUTS = 10\n",
        "# N_EPHOCS = 10\n",
        "\n",
        "\n",
        "model = PRNN(N_INPUTS,N_HIDDEN,N_OUTPUTS,0.1)\n",
        "# model = MyModel()\n",
        "# model.cuda()\n",
        "\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1, momentum=0.9)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "criterion = nn.MSELoss() # this does not work\n",
        "\n",
        "train_running_loss = np.zeros(N_EPHOCS)\n",
        "train_acc = np.zeros(N_EPHOCS)\n",
        "nparams = 0\n",
        "for param in model.parameters(): \n",
        "  if param.requires_grad:\n",
        "    nparams += param.data.numpy().size\n",
        "Phist = np.zeros((nparams,N_EPHOCS))\n",
        "\n",
        "for epoch in range(N_EPHOCS):\n",
        "    nps = 0\n",
        "    running_loss=0\n",
        "    running_acc=0\n",
        "    for p,param in enumerate(model.parameters()):\n",
        "        if param.requires_grad:\n",
        "            plist = param.data.numpy().flatten()\n",
        "            for j in range(plist.size):\n",
        "                Phist[nps,epoch]=plist[j]\n",
        "                nps+=1\n",
        "  \n",
        "    for i, data in enumerate(trainloader):\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "#         print(y_tar)\n",
        "        x, y_tar = data\n",
        "                \n",
        "        y_pred = model(x.view(BATCH_SIZE,N_STEPS,N_INPUTS))\n",
        "        \n",
        "#        print(y_tar.shape)\n",
        "#        print(y_pred.shape) # prediction\n",
        "\n",
        "        loss = criterion(y_pred.view(BATCH_SIZE,N_OUTPUTS),Variable(y_tar))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss+=loss.item()\n",
        "        print(y_pred.view(BATCH_SIZE,N_OUTPUTS))\n",
        "        # torch.Tensor.view() returns a new tensor with the same data as the self tensor but of a different shape.\n",
        "        # The returned tensor shares the same data and must have the same number of elements, but may have a different size.        \n",
        "        # need to fix the .view function!\n",
        "        running_acc+=get_accuracy(y_pred.view(BATCH_SIZE,N_OUTPUTS).float(), y_tar.long(), BATCH_SIZE)\n",
        "        \n",
        "        \n",
        "    train_running_loss[epoch] = running_loss\n",
        "    train_acc[epoch] = running_acc/i\n",
        "    \n",
        "    print('Epoch:  %d | Loss: %.4f | Train Accuracy: %.2f' %(epoch, train_running_loss[epoch], train_acc[epoch]))"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.1964, -0.0417, -0.2840,  ..., -0.2828, -0.2611, -0.0805],\n",
            "        [ 0.2436,  0.0244, -0.3702,  ..., -0.2195, -0.3622, -0.0878],\n",
            "        [ 0.1410, -0.1064, -0.3047,  ..., -0.3420, -0.2597, -0.0349],\n",
            "        ...,\n",
            "        [ 0.2193, -0.1628, -0.2732,  ..., -0.3593, -0.3492, -0.1824],\n",
            "        [ 0.2271, -0.0628, -0.2515,  ..., -0.2513, -0.2797, -0.1809],\n",
            "        [ 0.2051, -0.0773, -0.2816,  ..., -0.2578, -0.2570, -0.1178]],\n",
            "       grad_fn=<ViewBackward>)\n",
            "tensor([5, 5, 7, 5, 5, 5, 5, 7, 5, 5, 5, 5, 5, 5, 7, 5, 5, 5, 5, 5, 5, 5, 7, 5,\n",
            "        5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 5, 7, 5, 5, 5, 5, 5, 5, 7, 7, 5, 5, 5, 5,\n",
            "        5, 5, 5, 7, 7, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
            "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 5, 5, 5, 5, 5, 5, 5, 5, 5])\n",
            "torch.Size([96, 1, 27])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-193-ba6ee043bed6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mrunning_loss\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN_OUTPUTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mrunning_acc\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mget_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN_OUTPUTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-184-01da99595b23>\u001b[0m in \u001b[0;36mget_accuracy\u001b[0;34m(logit, target, batch_size)\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m#  print(torch.max(logit, 1)[1].view(target.size()).data) # the formulation of torch.max(logit, 1)[1].view(target.size()).data has an error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m   \u001b[0mcorrects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# input size is the same as batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m   \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcorrects\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[96, 1, 27]' is invalid for input of size 96"
          ]
        }
      ]
    }
  ]
}