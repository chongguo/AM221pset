{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# # imports\n",
    "# from tqdm import tnrange\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torchvision\n",
    "# import torchvision.transforms as transforms\n",
    "# from torch.autograd import Variable\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import gc\n",
    "# import os\n",
    "# %matplotlib inline  \n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # alphabet\n",
    "# import string\n",
    "\n",
    "# # date and time\n",
    "# import datetime\n",
    "\n",
    "# # set the device on which to run the experiment\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(device)\n",
    "\n",
    "\n",
    "# imports\n",
    "from tqdm import tnrange\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "%matplotlib inline  \n",
    "import datetime\n",
    "\n",
    "# alphabet\n",
    "import string\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "def get_accuracy(logit, target):\n",
    "    batch_size = len(target)\n",
    "    corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
    "    accuracy = 100.0 * corrects/batch_size\n",
    "    return accuracy.item()\n",
    "\n",
    "def nparam(ninputs,nhidden,noutputs):\n",
    "    return ninputs*(nhidden+1) + nhidden*(nhidden+1)+nhidden*(noutputs+1)\n",
    "\n",
    "# define the nnumber of parameters we need\n",
    "def nparam_MLP(N_INPUTS,N_HIDDEN,N_OUTPUTS):\n",
    "    input_to_hidden1 = (N_INPUTS+1)*N_HIDDEN #+1 for bias\n",
    "    hidden1_to_hidden2 = (N_HIDDEN + 1)*N_HIDDEN\n",
    "    hidden2_to_output = (N_OUTPUTS)*(N_HIDDEN+1)\n",
    "    return(sum([input_to_hidden1,hidden1_to_hidden2,hidden2_to_output]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a prototype 2-layer MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden_neurons, n_output,  device):\n",
    "        super(MLP, self).__init__()\n",
    "        self.n_inputs = n_inputs # set the number of neurons in the input layer\n",
    "        self.n_hidden_neurons = n_hidden_neurons # how many neurons are in each hidden layer\n",
    "        self.n_output = n_output # set the number of neurons in the output layer\n",
    "        self.sig = nn.Sigmoid() # set the activation function \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.n_hidden = n_hidden_neurons\n",
    "        self.encoder = nn.Linear(n_inputs, n_hidden_neurons) # encode input\n",
    "        self.recurrent = nn.Linear(n_hidden_neurons,n_hidden_neurons) # recurrent connections\n",
    "        self.decoder = nn.Linear(n_hidden_neurons, n_output) # decode output\n",
    "                \n",
    "    def forward(self, x):\n",
    "        self.hidden1 = self.tanh(self.encoder(x))\n",
    "        self.hidden2 = self.tanh(self.recurrent(self.hidden1))\n",
    "        self.output = self.decoder(self.hidden2)\n",
    "        return self.output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.   0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9]\n"
     ]
    }
   ],
   "source": [
    "# # Test MLP on Anna Karenina\n",
    "# # Load Anna Karenina\n",
    "# from torch.utils.data import DataLoader # dataloader \n",
    "# import sys\n",
    "# sys.path.insert(0,'../final_project/Data/')\n",
    "# # from AnnaDataset import AnnaDataset, InvertAnna # import AK dataset; has 1/100th of values\n",
    "# from AnnaDataset_MLP import AnnaDataset, InvertAnna # import AK dataset\n",
    "# import torchvision\n",
    "# import torchvision.transforms as transforms\n",
    "\n",
    "# # params\n",
    "# BATCH_SIZE = 500 # how many batches we are running\n",
    "# N_STEPS = 10 # How many characters are we inputting into the list at a time\n",
    "# N_HIDDEN_NEURONS = 512 # how many neurons per hidden layer\n",
    "# # N_LAYERS = 2 # 2 hidden layers\n",
    "# N_EPOCHS = 20 # how many training epocs\n",
    "# learning_rates = np.asarray([2]) # learning rates\n",
    "# N_REPS = 3 # len(learning_rates) # the number of learning repetitions\n",
    "# gidx = int(N_HIDDEN_NEURONS/2) # partition of the hidden neurons for block regularization\n",
    "\n",
    "# # regularization parameters\n",
    "# # lambdas = np.arange(0,1e-2,3e-3,dtype=np.float)\n",
    "# # lambdas = 10**np.arange(-5,5,1,dtype=np.float) # order-of-magnitude sweep\n",
    "# lambdas = np.arange(0,1e-1,1e-2,dtype=np.float) # full sweep\n",
    "# N_LAMBDA = len(lambdas)\n",
    "\n",
    "# # load data\n",
    "# # list all transformations\n",
    "# transform = transforms.Compose(\n",
    "#     [transforms.Normalize((0,), (0.3,))])\n",
    "\n",
    "# dataset = AnnaDataset(N_STEPS) # load the dataset\n",
    "\n",
    "# N_INPUTS = len(dataset.categories)*N_STEPS\n",
    "# N_OUTPUTS = len(dataset.categories)\n",
    "\n",
    "# N_PARAMS = nparam_MLP(N_INPUTS,N_HIDDEN_NEURONS,N_OUTPUTS)\n",
    "\n",
    "# Test MLP on Anna Karenina\n",
    "# Load Anna Karenina\n",
    "from torch.utils.data import DataLoader # dataloader \n",
    "import sys\n",
    "sys.path.insert(0,'../final_project/Data/')\n",
    "from AnnaDataset_MLP import AnnaDataset, InvertAnna # import AK dataset\n",
    "# from AnnaDataset import AnnaDataset, InvertAnna # import AK dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# params\n",
    "BATCH_SIZE = 500 # how many batches we are running\n",
    "N_STEPS = 10 # How many characters are we inputting into the list at a time\n",
    "N_HIDDEN_NEURONS = 512 # how many neurons per hidden layer\n",
    "N_LAYERS = 2 # 2 hidden layers\n",
    "N_EPOCHS = 15 # how many training epocs\n",
    "learning_rates = np.asarray([2]) # learning rates\n",
    "N_REPS = 1 # len(learning_rates) # the number of learning repetitions\n",
    "gidx = int(N_HIDDEN_NEURONS/2)\n",
    "\n",
    "# regularization parameters\n",
    "# lambdas = np.arange(0,1e-2,3e-3,dtype=np.float)\n",
    "# lambdas = np.arange(0,1e-1,1e-2,dtype=np.float) # full sweep\n",
    "lambdas = np.arange(0,1,1e-1) # short sweep\n",
    "print(lambdas)\n",
    "# lambdas = np.arange(0,1,1e-1,dtype=np.float) # full sweep\n",
    "N_LAMBDA = len(lambdas)\n",
    "\n",
    "# load data\n",
    "# list all transformations\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Normalize((0,), (0.3,))])\n",
    "\n",
    "dataset = AnnaDataset(N_STEPS) # load the dataset\n",
    "\n",
    "N_INPUTS = len(dataset.categories)*N_STEPS\n",
    "N_OUTPUTS = len(dataset.categories)\n",
    "N_PARAMS = nparam_MLP(N_INPUTS,N_HIDDEN_NEURONS,N_OUTPUTS)\n",
    "\n",
    "\n",
    "# trainloader = DataLoader(dataset, batch_size=BATCH_SIZE,\n",
    "#                         shuffle=False, num_workers=4) # create a DataLoader. We want a batch of BATCH_SIZE entries\n",
    "# testloader = DataLoader(dataset, batch_size=BATCH_SIZE,\n",
    "#                         shuffle=False, num_workers=4) # create a DataLoader. We want a batch of BATCH_SIZE entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566579\n",
      "1566579\n",
      "391645\n"
     ]
    }
   ],
   "source": [
    "# # # test_split = torch.split(dataset.onehot_encoded,20,dim=0)\n",
    "# # # print(len(test_split))\n",
    "# # print(dataset.onehot_encoded.shape)\n",
    "# # print(test_split[0].shape)\n",
    "# # print(test_split[1].shape)\n",
    "\n",
    "# # train-test-split\n",
    "# train_fraction,test_fraction,valid_fraction = (0.8,0.1,0.1)\n",
    "# random_seed = 0\n",
    "# shuffle_dataset = True\n",
    "\n",
    "# # from https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets \n",
    "# # Creating data indices for training and validation splits:\n",
    "# dataset_size = len(dataset.onehot_encoded)\n",
    "# indices = list(range(dataset_size))\n",
    "# train_split = int(np.floor(train_fraction * dataset_size))\n",
    "# print(train_split)\n",
    "# valid_split = train_split + int(np.floor(valid_fraction * dataset_size))\n",
    "# test_split = valid_split + int(np.floor(test_fraction * dataset_size))\n",
    "\n",
    "# print(train_split,valid_split,test_split,dataset_size)\n",
    "\n",
    "# if shuffle_dataset :\n",
    "#     np.random.seed(random_seed)\n",
    "#     np.random.shuffle(indices)\n",
    "# # train_indices, val_indices, test_indices = indices[:train_split], indices[train_split:valid_split], indices[valid_split:]\n",
    "# train_indices = indices[:train_split]\n",
    "# val_indices = indices[train_split:valid_split]\n",
    "# test_indices = indices[valid_split:]\n",
    "\n",
    "# print(max(train_indices))\n",
    "# print(max(val_indices))\n",
    "# print(max(test_indices))\n",
    "\n",
    "# # Creating PT data samplers and loaders:\n",
    "# train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "# test_sampler = torch.utils.data.SubsetRandomSampler(test_indices)\n",
    "# valid_sampler = torch.utils.data.SubsetRandomSampler(val_indices)\n",
    "\n",
    "# # train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "# #                                            sampler=train_sampler)\n",
    "# # validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "# #                                                 sampler=valid_sampler)\n",
    "\n",
    "\n",
    "\n",
    "# train-test-split\n",
    "train_fraction = 0.8\n",
    "\n",
    "random_seed = 0\n",
    "shuffle_dataset = True\n",
    "\n",
    "# from https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets \n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset.onehot_encoded)\n",
    "indices = list(range(dataset_size))\n",
    "train_split = int(np.floor(train_fraction * dataset_size))\n",
    "print(train_split)\n",
    "\n",
    "\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices = indices[:train_split]\n",
    "test_indices = indices[train_split:]\n",
    "\n",
    "print(len(train_indices))\n",
    "print(len(test_indices))\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = torch.utils.data.SequentialSampler(train_indices)\n",
    "test_sampler = torch.utils.data.SequentialSampler(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test-validate split\n",
    "\n",
    "trainloader = DataLoader(dataset, batch_size=BATCH_SIZE,\n",
    "                         shuffle=False, num_workers=4,\n",
    "                        sampler = train_sampler) # create a DataLoader. We want a batch of BATCH_SIZE entries\n",
    "# validloader = DataLoader(dataset, batch_size=BATCH_SIZE,\n",
    "#                         sampler = valid_sampler) # create a DataLoader. We want a batch of BATCH_SIZE entries\n",
    "testloader = DataLoader(dataset, batch_size=BATCH_SIZE,\n",
    "                        shuffle=False, num_workers=4,\n",
    "                        sampler = test_sampler) # create a DataLoader. We want a batch of BATCH_SIZE entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-28T02:53:46.934341\n"
     ]
    }
   ],
   "source": [
    "# modelkey = ''.join([str(np.random.randint(0,9)) for i in range(10)])\n",
    "\n",
    "runnow = datetime.datetime.now()\n",
    "modelkey = str(runnow.isoformat())\n",
    "print(modelkey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # regularizing digonal blocks of the partitioned RNN\n",
    "# # initialize arrays of loss values and weights over the number of epohcs, the number of lambdas we are testing, and the number of reps. \n",
    "# train_loss_P = np.zeros((N_EPOCHS,N_LAMBDA,N_REPS)) \n",
    "# train_acc_P = np.zeros((N_EPOCHS,N_LAMBDA,N_REPS))\n",
    "# test_loss_P = np.zeros((N_EPOCHS,N_LAMBDA,N_REPS))\n",
    "# test_acc_P = np.zeros((N_EPOCHS,N_LAMBDA,N_REPS))\n",
    "# Phist_P = np.zeros((N_PARAMS,N_EPOCHS,N_LAMBDA,N_REPS))\n",
    "\n",
    "# model_P = [None]*N_LAMBDA*N_REPS # array of models\n",
    "# regval_P = [] # array of regularization values\n",
    "\n",
    "# # generate a unique key for these models\n",
    "\n",
    "# for r in tnrange(N_REPS): # loop over the number of reps\n",
    "#     for k in tnrange(N_LAMBDA): # loop over the number of different lambda values\n",
    "#         reg_lambda = lambdas[k] # set the regularization lambda\n",
    "#         model_path = './models/model'+str(modelkey)+'_P_rep_{}_lambda_{:d}_10.pt'.format(r,int(reg_lambda*10)) # path to which we will save the model\n",
    "#         model_P[k+r*N_LAMBDA] = MLP(N_INPUTS,N_HIDDEN_NEURONS,N_OUTPUTS,device).to(device) # create the model\n",
    "#         l2_reg = torch.tensor(1,device=device) # create the l2 regularization value tensor\n",
    "# #        optimizer = torch.optim.SGD(model_P[k+r*N_LAMBDA].parameters(), lr=1e-1, momentum=0.9) # set the function for SGD\n",
    "#         optimizer = torch.optim.SGD(model_P[k+r*N_LAMBDA].parameters(), lr=1e-2, momentum=0.9) # set the function for SGD\n",
    "#         criterion = nn.CrossEntropyLoss() # set the loss function\n",
    "        \n",
    "#         # note that cross-entropy loss expects the indices of the class, not the one-hot. So, for A = [1,0,0,...] and B = [0,1,0,...], A is 0 and B is 1\n",
    "        \n",
    "#         for epoch in range(N_EPOCHS): # for each training epoch\n",
    "#             nps = 0\n",
    "#             running_train_loss=0\n",
    "#             running_train_acc=0\n",
    "#             model_P[k+r*N_LAMBDA].train() \n",
    "#             for p, param in enumerate(model_P[k+r*N_LAMBDA].parameters()): # go through all the model parameters\n",
    "#                 if param.requires_grad:\n",
    "#                     plist = torch.flatten(param.data) # set the list of parameters\n",
    "#                     for j in range(plist.size(0)):\n",
    "#                         while nps < Phist_P.shape[0]:\n",
    "#                             Phist_P[nps,epoch,k,r]=plist[j].item() # update the parameters\n",
    "#                             nps+=1\n",
    "\n",
    "#             for i, (x, y_tar) in enumerate(trainloader):\n",
    "#                 # print(i)\n",
    "#                 l2_reg = 0\n",
    "#                 x, y_tar = x.to(device), y_tar.to(device) # x is the training set, y_tar is the output label\n",
    "#                 x = x-0.3\n",
    "#                 optimizer.zero_grad() # set gradients to 0\n",
    "#                 y_pred = model_P[k+r*N_LAMBDA](x.view(x.shape[0],x.shape[1]*x.shape[2])) # compute the prediction. \n",
    "#                 loss = criterion(y_pred,y_tar) \n",
    "#                 for p,param in enumerate(model_P[k+r*N_LAMBDA].parameters()):\n",
    "#                     if param.requires_grad and len(param.shape)==2:\n",
    "#                         if param.shape[0]==N_HIDDEN_NEURONS and param.shape[1]==N_HIDDEN_NEURONS:\n",
    "#                             l2_reg = l2_reg + param[:gidx,:gidx].norm(p=1) # update the l2 regularization constant\n",
    "#                             l2_reg = l2_reg + param[gidx:,gidx:].norm(p=1)\n",
    "# #                         elif param.shape[1]==N_HIDDEN_NEURONS: # regularization \n",
    "# #                             l2_reg = l2_reg + param[:,gidx:].norm(p=1)\n",
    "# #                         elif param.shape[0]==N_HIDDEN_NEURONS:\n",
    "# #                             l2_reg = l2_reg + param[:gidx,:].norm(p=1)\n",
    "#                 regval_P.append(l2_reg.item()) # add the l2 regularization to  the running list\n",
    "#                 loss = loss + l2_reg*reg_lambda/BATCH_SIZE # compute the loss\n",
    "#                 loss.backward() # backpropogate the loss\n",
    "#                 optimizer.step() # run SGD\n",
    "#                 running_train_loss+=loss.item()\n",
    "#                 running_train_acc+=get_accuracy(y_pred, y_tar) # compute accuracy\n",
    "            \n",
    "#             running_test_acc=0\n",
    "#             running_test_loss=0\n",
    "#             model_P[k+r*N_LAMBDA].eval()\n",
    "#             for i,(x_test, y_test_tar) in enumerate(testloader):\n",
    "#                 x_test, y_test_tar = x_test.to(device), y_test_tar.to(device)\n",
    "#                 x_test = x_test - 0.3\n",
    "#                 y_test_pred = model_P[k+r*N_LAMBDA](x_test.view(x_test.shape[0],x_test.shape[1]*x_test.shape[2]))\n",
    "#                 loss = criterion(y_test_pred,y_test_tar)\n",
    "                \n",
    "#                 running_test_loss+=loss.item()\n",
    "#                 running_test_acc+=get_accuracy(y_test_pred, y_test_tar)\n",
    "                \n",
    "#             for i,(x_valid, y_valid_tar) in enumerate(validloader):\n",
    "#                 x_valid, y_valid_tar = x_test.to(device), y_test_tar.to(device)\n",
    "#                 x_valid = x_valid - 0.3\n",
    "#                 y_valid_pred = model_P[k+r*N_LAMBDA](x_test.view(x_test.shape[0],x_test.shape[1]*x_test.shape[2]))\n",
    "#                 loss = criterion(y_test_pred,y_test_tar)\n",
    "                \n",
    "#                 running_valid_loss+=loss.item()\n",
    "#                 running_valid_acc+=get_accuracy(y_valid_pred, y_valid_tar)\n",
    "\n",
    "                \n",
    "#             train_loss_P[epoch,k,r] = running_train_loss/len(trainloader)\n",
    "#             train_acc_P[epoch,k,r] = running_train_acc/len(trainloader)\n",
    "#             test_loss_P[epoch,k,r] = running_test_loss/len(testloader)\n",
    "#             test_acc_P[epoch,k,r] = running_test_acc/len(testloader)\n",
    "#             valid_loss_P[epoch,k,r] = running_valid_loss/len(validloader)\n",
    "#             valid_acc_P[epoch,k,r] = running_valid_acc/len(validloader)\n",
    "\n",
    "#             print(\"Epoch %d; rep %d; lambda %f; train accuracy %f; train loss %f; test accuracy %f; test loss %f; valid accuracy $f; valid loss %f; reg val %f\"\n",
    "#                   %(epoch,\n",
    "#                     r,\n",
    "#                     k,\n",
    "#                     train_acc_P[epoch,k,r],\n",
    "#                     train_loss_P[epoch,k,r],\n",
    "#                     test_acc_P[epoch,k,r],\n",
    "#                     valid_acc_P[epoch,k,r],\n",
    "#                     valid_loss_P[epoch,k,r],\n",
    "#                     test_loss_P[epoch,k,r],l2_reg.item()))\n",
    "\n",
    "#             # print(train_acc_P[epoch,k,r])\n",
    "            \n",
    "#         # save the model and free the memory  \n",
    "#         torch.save(model_P[k+r*N_LAMBDA].state_dict(), model_path)\n",
    "#         model_P[k+r*N_LAMBDA] = [None]\n",
    "#         del(l2_reg,loss,optimizer,criterion,plist,param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd4eb3742a9442c49499192052382187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd0b020eeeeb4bc9a2048a15e99bc7fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0; rep 0; lambda 0.000000; train accuracy 30.589343; train loss 2.501998; test accuracy 35.077806; test loss 2.263530; reg val 3195.826172\n",
      "Epoch 1; rep 0; lambda 0.000000; train accuracy 37.304403; train loss 2.177086; test accuracy 37.769133; test loss 2.150169; reg val 3365.591309\n",
      "Epoch 2; rep 0; lambda 0.000000; train accuracy 39.889279; train loss 2.065143; test accuracy 40.072704; test loss 2.048893; reg val 3559.140625\n",
      "Epoch 3; rep 0; lambda 0.000000; train accuracy 42.359923; train loss 1.962911; test accuracy 42.900510; test loss 1.944289; reg val 3755.304199\n",
      "Epoch 4; rep 0; lambda 0.000000; train accuracy 44.514678; train loss 1.879171; test accuracy 44.992347; test loss 1.871501; reg val 3942.714111\n",
      "Epoch 5; rep 0; lambda 0.000000; train accuracy 46.386088; train loss 1.811855; test accuracy 46.293367; test loss 1.817241; reg val 4121.539062\n",
      "Epoch 6; rep 0; lambda 0.000000; train accuracy 47.890236; train loss 1.755925; test accuracy 47.673469; test loss 1.767086; reg val 4298.656250\n",
      "Epoch 7; rep 0; lambda 0.000000; train accuracy 49.218251; train loss 1.707268; test accuracy 49.038265; test loss 1.722496; reg val 4474.840820\n",
      "Epoch 8; rep 0; lambda 0.000000; train accuracy 50.416082; train loss 1.664659; test accuracy 49.978316; test loss 1.685745; reg val 4648.747070\n",
      "Epoch 9; rep 0; lambda 0.000000; train accuracy 51.461391; train loss 1.627365; test accuracy 50.604592; test loss 1.656460; reg val 4819.978516\n",
      "Epoch 10; rep 0; lambda 0.000000; train accuracy 52.374601; train loss 1.594450; test accuracy 51.307398; test loss 1.629798; reg val 4988.176758\n",
      "Epoch 11; rep 0; lambda 0.000000; train accuracy 53.158902; train loss 1.565149; test accuracy 51.994898; test loss 1.604055; reg val 5152.438477\n",
      "Epoch 12; rep 0; lambda 0.000000; train accuracy 53.863752; train loss 1.538922; test accuracy 52.713010; test loss 1.580219; reg val 5312.097168\n",
      "Epoch 13; rep 0; lambda 0.000000; train accuracy 54.485322; train loss 1.515304; test accuracy 53.323980; test loss 1.558567; reg val 5467.429688\n",
      "Epoch 14; rep 0; lambda 0.000000; train accuracy 55.050415; train loss 1.493888; test accuracy 53.883929; test loss 1.538831; reg val 5618.518066\n",
      "Epoch 0; rep 0; lambda 0.100000; train accuracy 29.753989; train loss 2.703345; test accuracy 34.817602; test loss 2.296341; reg val 29.882038\n",
      "Epoch 1; rep 0; lambda 0.100000; train accuracy 36.521378; train loss 2.219628; test accuracy 37.556122; test loss 2.180421; reg val 10.379543\n",
      "Epoch 2; rep 0; lambda 0.100000; train accuracy 38.626356; train loss 2.124678; test accuracy 39.286990; test loss 2.104787; reg val 13.415263\n",
      "Epoch 3; rep 0; lambda 0.100000; train accuracy 40.774410; train loss 2.036615; test accuracy 41.135204; test loss 2.021607; reg val 16.981853\n",
      "Epoch 4; rep 0; lambda 0.100000; train accuracy 42.773133; train loss 1.952039; test accuracy 42.858418; test loss 1.949777; reg val 18.129074\n",
      "Epoch 5; rep 0; lambda 0.100000; train accuracy 44.354180; train loss 1.886143; test accuracy 44.413265; test loss 1.893053; reg val 19.416767\n",
      "Epoch 6; rep 0; lambda 0.100000; train accuracy 45.833440; train loss 1.831511; test accuracy 45.641582; test loss 1.842711; reg val 20.591553\n",
      "Epoch 7; rep 0; lambda 0.100000; train accuracy 47.220485; train loss 1.783812; test accuracy 46.776786; test loss 1.794573; reg val 21.826719\n",
      "Epoch 8; rep 0; lambda 0.100000; train accuracy 48.385450; train loss 1.741943; test accuracy 47.840561; test loss 1.754358; reg val 23.177933\n",
      "Epoch 9; rep 0; lambda 0.100000; train accuracy 49.404595; train loss 1.705068; test accuracy 48.711735; test loss 1.723974; reg val 24.661022\n",
      "Epoch 10; rep 0; lambda 0.100000; train accuracy 50.328653; train loss 1.672165; test accuracy 49.498724; test loss 1.696364; reg val 26.229542\n",
      "Epoch 11; rep 0; lambda 0.100000; train accuracy 51.138800; train loss 1.642658; test accuracy 50.317602; test loss 1.667756; reg val 27.806276\n",
      "Epoch 12; rep 0; lambda 0.100000; train accuracy 51.868220; train loss 1.616034; test accuracy 51.094388; test loss 1.641959; reg val 29.379372\n",
      "Epoch 13; rep 0; lambda 0.100000; train accuracy 52.537332; train loss 1.591824; test accuracy 51.733418; test loss 1.618950; reg val 30.976654\n",
      "Epoch 14; rep 0; lambda 0.100000; train accuracy 53.149649; train loss 1.569671; test accuracy 52.355867; test loss 1.597750; reg val 32.529758\n",
      "Epoch 0; rep 0; lambda 0.200000; train accuracy 29.356733; train loss 2.711560; test accuracy 34.443878; test loss 2.301085; reg val 4.828643\n",
      "Epoch 1; rep 0; lambda 0.200000; train accuracy 36.468411; train loss 2.217222; test accuracy 37.511480; test loss 2.175486; reg val 6.245701\n",
      "Epoch 2; rep 0; lambda 0.200000; train accuracy 39.001276; train loss 2.109048; test accuracy 39.187500; test loss 2.096505; reg val 7.844001\n",
      "Epoch 3; rep 0; lambda 0.200000; train accuracy 41.143586; train loss 2.019196; test accuracy 40.746173; test loss 2.026008; reg val 8.746181\n",
      "Epoch 4; rep 0; lambda 0.200000; train accuracy 42.834397; train loss 1.949136; test accuracy 42.760204; test loss 1.950916; reg val 9.671618\n",
      "Epoch 5; rep 0; lambda 0.200000; train accuracy 44.398851; train loss 1.888375; test accuracy 44.475765; test loss 1.884951; reg val 10.890043\n",
      "Epoch 6; rep 0; lambda 0.200000; train accuracy 45.913529; train loss 1.833681; test accuracy 45.700255; test loss 1.835554; reg val 12.154160\n",
      "Epoch 7; rep 0; lambda 0.200000; train accuracy 47.167837; train loss 1.787462; test accuracy 46.739796; test loss 1.796790; reg val 13.270805\n",
      "Epoch 8; rep 0; lambda 0.200000; train accuracy 48.273772; train loss 1.746646; test accuracy 47.775510; test loss 1.760545; reg val 14.338352\n",
      "Epoch 9; rep 0; lambda 0.200000; train accuracy 49.253669; train loss 1.709750; test accuracy 48.633929; test loss 1.728527; reg val 15.466745\n",
      "Epoch 10; rep 0; lambda 0.200000; train accuracy 50.134014; train loss 1.676334; test accuracy 49.540816; test loss 1.698757; reg val 16.603142\n",
      "Epoch 11; rep 0; lambda 0.200000; train accuracy 50.979579; train loss 1.646176; test accuracy 50.304847; test loss 1.668688; reg val 17.641470\n",
      "Epoch 12; rep 0; lambda 0.200000; train accuracy 51.763880; train loss 1.618966; test accuracy 51.123724; test loss 1.639396; reg val 18.666298\n",
      "Epoch 13; rep 0; lambda 0.200000; train accuracy 52.447352; train loss 1.594345; test accuracy 51.946429; test loss 1.612964; reg val 19.625324\n",
      "Epoch 14; rep 0; lambda 0.200000; train accuracy 53.073389; train loss 1.571912; test accuracy 52.528061; test loss 1.590372; reg val 20.566574\n",
      "Epoch 0; rep 0; lambda 0.300000; train accuracy 29.309828; train loss 2.712526; test accuracy 34.459184; test loss 2.299455; reg val 4.132745\n",
      "Epoch 1; rep 0; lambda 0.300000; train accuracy 36.521698; train loss 2.217840; test accuracy 37.385204; test loss 2.180765; reg val 5.042013\n",
      "Epoch 2; rep 0; lambda 0.300000; train accuracy 38.911934; train loss 2.109564; test accuracy 39.005102; test loss 2.096189; reg val 5.913541\n",
      "Epoch 3; rep 0; lambda 0.300000; train accuracy 41.117422; train loss 2.016999; test accuracy 40.581633; test loss 2.017496; reg val 7.064874\n",
      "Epoch 4; rep 0; lambda 0.300000; train accuracy 43.082323; train loss 1.939025; test accuracy 42.765306; test loss 1.941678; reg val 7.798996\n",
      "Epoch 5; rep 0; lambda 0.300000; train accuracy 44.727186; train loss 1.876501; test accuracy 44.494898; test loss 1.879059; reg val 8.476489\n",
      "Epoch 6; rep 0; lambda 0.300000; train accuracy 46.228462; train loss 1.823265; test accuracy 45.846939; test loss 1.833266; reg val 9.200491\n",
      "Epoch 7; rep 0; lambda 0.300000; train accuracy 47.567964; train loss 1.776396; test accuracy 46.756378; test loss 1.797685; reg val 9.921478\n",
      "Epoch 8; rep 0; lambda 0.300000; train accuracy 48.754627; train loss 1.734996; test accuracy 47.609694; test loss 1.765294; reg val 10.709558\n",
      "Epoch 9; rep 0; lambda 0.300000; train accuracy 49.787492; train loss 1.698471; test accuracy 48.533163; test loss 1.732686; reg val 11.460163\n",
      "Epoch 10; rep 0; lambda 0.300000; train accuracy 50.671985; train loss 1.666106; test accuracy 49.525510; test loss 1.698004; reg val 12.107704\n",
      "Epoch 11; rep 0; lambda 0.300000; train accuracy 51.462029; train loss 1.637149; test accuracy 50.473214; test loss 1.663720; reg val 12.663092\n",
      "Epoch 12; rep 0; lambda 0.300000; train accuracy 52.146139; train loss 1.611037; test accuracy 51.233418; test loss 1.634646; reg val 13.252834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13; rep 0; lambda 0.300000; train accuracy 52.785897; train loss 1.587314; test accuracy 51.815051; test loss 1.610488; reg val 13.863556\n",
      "Epoch 14; rep 0; lambda 0.300000; train accuracy 53.340779; train loss 1.565647; test accuracy 52.405612; test loss 1.589522; reg val 14.527021\n",
      "Epoch 0; rep 0; lambda 0.400000; train accuracy 29.019464; train loss 2.720655; test accuracy 34.316327; test loss 2.304558; reg val 4.339820\n",
      "Epoch 1; rep 0; lambda 0.400000; train accuracy 36.467454; train loss 2.221115; test accuracy 37.377551; test loss 2.180062; reg val 4.988402\n",
      "Epoch 2; rep 0; lambda 0.400000; train accuracy 38.687620; train loss 2.122404; test accuracy 39.304847; test loss 2.102390; reg val 5.959432\n",
      "Epoch 3; rep 0; lambda 0.400000; train accuracy 40.847479; train loss 2.032784; test accuracy 41.012755; test loss 2.017768; reg val 6.957747\n",
      "Epoch 4; rep 0; lambda 0.400000; train accuracy 42.985003; train loss 1.949159; test accuracy 42.894133; test loss 1.951497; reg val 7.829134\n",
      "Epoch 5; rep 0; lambda 0.400000; train accuracy 44.581047; train loss 1.884784; test accuracy 44.478316; test loss 1.893232; reg val 8.528358\n",
      "Epoch 6; rep 0; lambda 0.400000; train accuracy 45.926930; train loss 1.831795; test accuracy 45.899235; test loss 1.842131; reg val 8.971180\n",
      "Epoch 7; rep 0; lambda 0.400000; train accuracy 47.252712; train loss 1.784754; test accuracy 47.075255; test loss 1.796946; reg val 9.335552\n",
      "Epoch 8; rep 0; lambda 0.400000; train accuracy 48.428845; train loss 1.742910; test accuracy 48.065051; test loss 1.758878; reg val 9.785368\n",
      "Epoch 9; rep 0; lambda 0.400000; train accuracy 49.488194; train loss 1.705747; test accuracy 48.863520; test loss 1.725780; reg val 10.403395\n",
      "Epoch 10; rep 0; lambda 0.400000; train accuracy 50.414805; train loss 1.672584; test accuracy 49.533163; test loss 1.698471; reg val 11.103805\n",
      "Epoch 11; rep 0; lambda 0.400000; train accuracy 51.242821; train loss 1.642838; test accuracy 50.167092; test loss 1.676101; reg val 11.918468\n",
      "Epoch 12; rep 0; lambda 0.400000; train accuracy 52.009892; train loss 1.616019; test accuracy 50.798469; test loss 1.652767; reg val 12.622573\n",
      "Epoch 13; rep 0; lambda 0.400000; train accuracy 52.671666; train loss 1.591775; test accuracy 51.536990; test loss 1.626568; reg val 13.246518\n",
      "Epoch 14; rep 0; lambda 0.400000; train accuracy 53.248564; train loss 1.569832; test accuracy 52.280612; test loss 1.601011; reg val 13.830634\n",
      "Epoch 0; rep 0; lambda 0.500000; train accuracy 28.851627; train loss 2.731497; test accuracy 34.327806; test loss 2.300666; reg val 4.327512\n",
      "Epoch 1; rep 0; lambda 0.500000; train accuracy 36.475112; train loss 2.220994; test accuracy 37.315051; test loss 2.187024; reg val 4.986475\n",
      "Epoch 2; rep 0; lambda 0.500000; train accuracy 38.820038; train loss 2.119608; test accuracy 39.081633; test loss 2.104179; reg val 5.589440\n",
      "Epoch 3; rep 0; lambda 0.500000; train accuracy 41.270262; train loss 2.021139; test accuracy 41.132653; test loss 2.015880; reg val 6.273586\n",
      "Epoch 4; rep 0; lambda 0.500000; train accuracy 43.180281; train loss 1.941130; test accuracy 43.053571; test loss 1.944199; reg val 6.818811\n",
      "Epoch 5; rep 0; lambda 0.500000; train accuracy 44.650287; train loss 1.880760; test accuracy 44.549745; test loss 1.881134; reg val 7.394413\n",
      "Epoch 6; rep 0; lambda 0.500000; train accuracy 46.056796; train loss 1.828060; test accuracy 46.090561; test loss 1.829395; reg val 7.818357\n",
      "Epoch 7; rep 0; lambda 0.500000; train accuracy 47.338864; train loss 1.782094; test accuracy 47.213010; test loss 1.787373; reg val 8.264713\n",
      "Epoch 8; rep 0; lambda 0.500000; train accuracy 48.489470; train loss 1.741427; test accuracy 48.086735; test loss 1.752900; reg val 8.818420\n",
      "Epoch 9; rep 0; lambda 0.500000; train accuracy 49.516273; train loss 1.704939; test accuracy 48.827806; test loss 1.725691; reg val 9.419750\n",
      "Epoch 10; rep 0; lambda 0.500000; train accuracy 50.451181; train loss 1.672138; test accuracy 49.536990; test loss 1.699353; reg val 10.099339\n",
      "Epoch 11; rep 0; lambda 0.500000; train accuracy 51.279834; train loss 1.642662; test accuracy 50.387755; test loss 1.670024; reg val 10.738968\n",
      "Epoch 12; rep 0; lambda 0.500000; train accuracy 52.020421; train loss 1.616133; test accuracy 51.107143; test loss 1.641519; reg val 11.342823\n",
      "Epoch 13; rep 0; lambda 0.500000; train accuracy 52.676452; train loss 1.592146; test accuracy 51.790816; test loss 1.616544; reg val 11.923051\n",
      "Epoch 14; rep 0; lambda 0.500000; train accuracy 53.270262; train loss 1.570337; test accuracy 52.446429; test loss 1.595044; reg val 12.501135\n",
      "Epoch 0; rep 0; lambda 0.600000; train accuracy 28.932355; train loss 2.730221; test accuracy 34.373724; test loss 2.301379; reg val 4.621105\n",
      "Epoch 1; rep 0; lambda 0.600000; train accuracy 36.544990; train loss 2.221413; test accuracy 37.400510; test loss 2.176993; reg val 5.022705\n",
      "Epoch 2; rep 0; lambda 0.600000; train accuracy 38.824505; train loss 2.122106; test accuracy 39.366071; test loss 2.095851; reg val 5.614813\n",
      "Epoch 3; rep 0; lambda 0.600000; train accuracy 40.981493; train loss 2.031772; test accuracy 41.181122; test loss 2.011183; reg val 6.339469\n"
     ]
    }
   ],
   "source": [
    "# regularizing digonal blocks of the partitioned RNN\n",
    "# initialize arrays of loss values and weights over the number of epohcs, the number of lambdas we are testing, and the number of reps. \n",
    "train_loss_P = np.zeros((N_EPOCHS,N_LAMBDA,N_REPS)) \n",
    "train_acc_P = np.zeros((N_EPOCHS,N_LAMBDA,N_REPS))\n",
    "test_loss_P = np.zeros((N_EPOCHS,N_LAMBDA,N_REPS))\n",
    "test_acc_P = np.zeros((N_EPOCHS,N_LAMBDA,N_REPS))\n",
    "Phist_P = np.zeros((N_PARAMS,N_EPOCHS,N_LAMBDA,N_REPS))\n",
    "\n",
    "model_P = [None]*N_LAMBDA*N_REPS # array of models\n",
    "regval_P = [] # array of regularization values\n",
    "\n",
    "lr = 1e-2\n",
    "momentum = 0.9\n",
    "\n",
    "for r in tnrange(N_REPS): # loop over the number of reps\n",
    "    for k in tnrange(N_LAMBDA): # loop over the number of different lambda values\n",
    "        reg_lambda = lambdas[k] # set the regularization lambda\n",
    "        model_path = './models/model_'+modelkey+'_P_rep_{}_lambda_{:d}_10.pt'.format(r,int(reg_lambda*10)) # path to which we will save the model\n",
    "        model_P[k+r*N_LAMBDA] = MLP(N_INPUTS,N_HIDDEN_NEURONS,N_OUTPUTS,device).to(device) # create the model\n",
    "        l2_reg = torch.tensor(1,device=device) # create the l2 regularization value tensor\n",
    "        optimizer = torch.optim.SGD(model_P[k+r*N_LAMBDA].parameters(), lr=lr, momentum=momentum) # set the function for SGD\n",
    "        criterion = nn.CrossEntropyLoss() # set the loss function\n",
    "        \n",
    "        # note that cross-entropy loss expects the indices of the class, not the one-hot. So, for A = [1,0,0,...] and B = [0,1,0,...], A is 0 and B is 1\n",
    "        \n",
    "        for epoch in range(N_EPOCHS): # for each training epoch\n",
    "            nps = 0\n",
    "            running_train_loss=0\n",
    "            running_train_acc=0\n",
    "            model_P[k+r*N_LAMBDA].train() \n",
    "            for p, param in enumerate(model_P[k+r*N_LAMBDA].parameters()): # go through all the model parameters\n",
    "                if param.requires_grad:\n",
    "                    plist = torch.flatten(param.data) # set the list of parameters\n",
    "                    for j in range(plist.size(0)):\n",
    "                        while nps < Phist_P.shape[0]:\n",
    "                            Phist_P[nps,epoch,k,r]=plist[j].item() # update the parameters\n",
    "                            nps+=1\n",
    "\n",
    "            for i, (x, y_tar) in enumerate(trainloader):\n",
    "                # print(i,x,y_tar)\n",
    "                l2_reg = 0\n",
    "                x, y_tar = x.to(device), y_tar.to(device) # x is the training set, y_tar is the output label\n",
    "                x = x-0.3\n",
    "                optimizer.zero_grad() # set gradients to 0\n",
    "                # print(x.shape)\n",
    "                y_pred = model_P[k+r*N_LAMBDA](x.view(x.shape[0],x.shape[1]*x.shape[2])) # compute the prediction. # size mismatch\n",
    "                \n",
    "                \n",
    "                loss = criterion(y_pred,y_tar) \n",
    "                for p,param in enumerate(model_P[k+r*N_LAMBDA].parameters()):\n",
    "                    if param.requires_grad and len(param.shape)==2:\n",
    "                        if param.shape[0]==N_HIDDEN_NEURONS and param.shape[1]==N_HIDDEN_NEURONS:\n",
    "                            l2_reg = l2_reg + param[:gidx,:gidx].norm(p=1) # update the l1 regularization constant\n",
    "                            l2_reg = l2_reg + param[gidx:,gidx:].norm(p=1)\n",
    "#                         elif param.shape[1]==N_HIDDEN_NEURONS:\n",
    "#                             l2_reg = l2_reg + param[:,gidx:].norm(p=1)\n",
    "#                         elif param.shape[0]==N_HIDDEN_NEURONS:\n",
    "#                             l2_reg = l2_reg + param[:gidx,:].norm(p=1)\n",
    "                regval_P.append(l2_reg.item()) # add the l2 regularization to  the running list\n",
    "                loss = loss + l2_reg*reg_lambda/BATCH_SIZE # compute the loss\n",
    "                loss.backward() # backpropogate the loss\n",
    "                optimizer.step() # run SGD\n",
    "                running_train_loss+=loss.item()\n",
    "                running_train_acc+=get_accuracy(y_pred, y_tar) # compute accuracy\n",
    "            \n",
    "            running_test_acc=0\n",
    "            running_test_loss=0\n",
    "            model_P[k+r*N_LAMBDA].eval()\n",
    "            for i,(x_test, y_test_tar) in enumerate(testloader):\n",
    "                x_test, y_test_tar = x_test.to(device), y_test_tar.to(device)\n",
    "                x_test = x_test - 0.3\n",
    "                y_test_pred = model_P[k+r*N_LAMBDA](x_test.view(x_test.shape[0],x_test.shape[1]*x_test.shape[2]))\n",
    "                loss = criterion(y_test_pred,y_test_tar)\n",
    "                \n",
    "                running_test_loss+=loss.item()\n",
    "                running_test_acc+=get_accuracy(y_test_pred, y_test_tar)\n",
    "                \n",
    "            train_loss_P[epoch,k,r] = running_train_loss/len(trainloader)\n",
    "            train_acc_P[epoch,k,r] = running_train_acc/len(trainloader)\n",
    "            test_loss_P[epoch,k,r] = running_test_loss/len(testloader)\n",
    "            test_acc_P[epoch,k,r] = running_test_acc/len(testloader)\n",
    "            print(\"Epoch %d; rep %d; lambda %f; train accuracy %f; train loss %f; test accuracy %f; test loss %f; reg val %f\"\n",
    "                  %(epoch,\n",
    "                    r,\n",
    "                    reg_lambda,\n",
    "                    train_acc_P[epoch,k,r],\n",
    "                    train_loss_P[epoch,k,r],\n",
    "                    test_acc_P[epoch,k,r],\n",
    "                    test_loss_P[epoch,k,r],\n",
    "                   l2_reg.item()))\n",
    "            \n",
    "        # save the model and free the memory  \n",
    "        torch.save(model_P[k+r*N_LAMBDA].state_dict(), model_path)\n",
    "        model_P[k+r*N_LAMBDA] = [None]\n",
    "        del(l2_reg,loss,optimizer,criterion,plist,param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# BATCH_SIZE = 500 # how many batches we are running\n",
    "# N_STEPS = 10 # How many characters are we inputting into the list at a time\n",
    "# N_HIDDEN_NEURONS = 512 # how many neurons per hidden layer\n",
    "# N_INPUTS = 77*N_STEPS\n",
    "# N_OUTPUTS = 77\n",
    "# N_LAYERS = 2 # 2 hidden layers\n",
    "# N_EPOCHS = 11 # how many training epocs\n",
    "# learning_rates = np.asarray([2]) # learning rates\n",
    "# N_REPS = 3 # len(learning_rates) # the number of learning repetitions\n",
    "# N_PARAMS = nparam_MLP(N_INPUTS,N_HIDDEN_NEURONS,N_OUTPUTS)\n",
    "# gidx = int(N_HIDDEN_NEURONS/2)\n",
    "\n",
    "\n",
    "# train_loss_P = np.zeros((N_EPOCHS,N_LAMBDA,N_REPS)) \n",
    "# train_acc_P = np.zeros((N_EPOCHS,N_LAMBDA,N_REPS))\n",
    "# test_loss_P = np.zeros((N_EPOCHS,N_LAMBDA,N_REPS))\n",
    "# test_acc_P = np.zeros((N_EPOCHS,N_LAMBDA,N_REPS))\n",
    "# Phist_P = np.zeros((N_PARAMS,N_EPOCHS,N_LAMBDA,N_REPS))\n",
    "\n",
    "# model_P = [None]*N_LAMBDA*N_REPS # array of models\n",
    "# regval_P = [] # array of regularization values\n",
    "\n",
    "\n",
    "\n",
    "pickle.dump([lambdas,N_EPOCHS,N_REPS,N_HIDDEN_NEURONS,learning_rates,\n",
    "             N_PARAMS,N_EPOCHS,N_LAMBDA,N_REPS,\n",
    "             model_P,regval_P,\n",
    "             train_loss_P,train_acc_P,\n",
    "             test_loss_P,test_acc_P,\n",
    "             Phist_P], \n",
    "            open(modelkey+\"_longepochs_mlp_ak_quickset.pkl\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# #plt.imshow(x[0,:,:])\n",
    "# #plt.plot(y_pred.detach().numpy()[0,:])\n",
    "# #torch.max(y_pred,1)\n",
    "# plt.figure(1)\n",
    "# plt.plot(np.mean(test_acc_P,2))\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Test accuracy\")\n",
    "# plt.plot()\n",
    "\n",
    "# plt.figure(2)\n",
    "# plt.plot(np.mean(test_loss_P,2))\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Test loss\")\n",
    "# plt.plot()\n",
    "\n",
    "\n",
    "# for i,j in enumerate(zip(np.mean(test_acc_P,2),np.mean(test_loss_P,2))):\n",
    "#     # plt.figure(i+3) # Here's the part I need, but numbering starts at 1!\n",
    "#     fig,axs = plt.subplots(1,2)\n",
    "#     fig.suptitle(\"Epoch %d\"%i)\n",
    "#     axs[0].plot(lambdas,j[0])\n",
    "#     axs[0].set_xlabel(\"Lambda\")\n",
    "#     axs[0].set_ylabel(\"Test accuracy\")\n",
    "#     # axs[0].title(\"Epoch %d\"%i)\n",
    "#     axs[1].plot(lambdas,j[1])\n",
    "#     axs[1].set_xlabel(\"Lambda\")\n",
    "#     axs[1].set_ylabel(\"Test loss\")\n",
    "#     # axs[1].title(\"Epoch %d\"%i)\n",
    "    \n",
    "# plt.plot(regval_P)\n",
    "\n",
    "\n",
    "# # plt.plot(np.mean(test_acc_P,1))\n",
    "# # plt.plot()\n",
    "\n",
    "# # plt.plot(np.mean(test_acc_P,2))\n",
    "# # plt.plot()\n",
    "\n",
    "# # plt.plot(np.mean(test_acc_P,3))\n",
    "# # plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# number of lambdas\n",
    "n_epochs_plot,n_lambdas_plot = np.mean(test_acc_P,2).shape\n",
    "\n",
    "fig,ax = plt.subplots(1,2)\n",
    "fig.subplots_adjust(hspace=20.0)\n",
    "\n",
    "ax1,ax2 = ax\n",
    "cm1 = plt.get_cmap('copper') # plt.get_cmap('gist_rainbow') # plt.get_cmap('gist_rainbow')\n",
    "# fig = plt.figure()\n",
    "NUM_COLORS = n_lambdas_plot\n",
    "ax1.set_prop_cycle('color', [cm1(1.*i/NUM_COLORS) for i in range(NUM_COLORS)])\n",
    "ax2.set_prop_cycle('color', [cm1(1.*i/NUM_COLORS) for i in range(NUM_COLORS)])\n",
    "\n",
    "fig.suptitle(\"Epoch %d; reps %d; min/max lambda [%f,%f]\"\n",
    "             %(epoch,\n",
    "               N_REPS,\n",
    "               min(lambdas),\n",
    "               max(lambdas)))\n",
    "\n",
    "# train accuracy %f; train loss %f; test accuracy %f; test loss %f\"\n",
    "# train_acc_P[epoch,k,r],\n",
    "#                train_loss_P[epoch,k,r],\n",
    "#                test_acc_P[epoch,k,r],\n",
    "#                test_loss_P[epoch,k,r]\n",
    "            \n",
    "# ax = fig.add_subplot(111)\n",
    "for i in enumerate(lambdas):\n",
    "    ax1.plot(np.mean(test_acc_P,2)[:,i[0]],label=i[1],)\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Test Accuracy\")\n",
    "    \n",
    "# ax2 = fig.add_subplot(011)\n",
    "for i in enumerate(lambdas):\n",
    "    ax2.plot(np.mean(test_loss_P,2)[:,i[0]],label=i[1],)\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(\"Test Loss\")\n",
    "ax2.legend(loc=\"upper right\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(regval_P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check past model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# # picked_data = pickle.load(\"mlp_ak_set.pkl\")\n",
    "\n",
    "# with open('mlp_ak_set.pkl','rb') as pickle_file:\n",
    "#     picked_data = pickle.load(pickle_file)\n",
    "#     [lambdas,N_EPOCHS,N_REPS,N_HIDDEN_NEURONS,learning_rates,N_REPS,N_PARAMS,\n",
    "#              model_P,\n",
    "#              train_loss_P,train_acc_P,\n",
    "#              test_loss_P,test_acc_P,\n",
    "#              Phist_P] = picked_data\n",
    "#     N_HIDDEN = N_HIDDEN_NEURONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load training models\n",
    "# import glob\n",
    "# modelfiles = glob.glob(\"./**.pt\")\n",
    "# past_models = []\n",
    "# for i in modelfiles:\n",
    "#     past_models.append(torch.load(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# past_models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(14,5))\n",
    "# plt.subplot(1,2,1)\n",
    "# sidx = int((N_INPUTS+N_OUTPUTS+1)*N_HIDDEN)\n",
    "# ridx = int(N_HIDDEN*N_HIDDEN)\n",
    "# rval_P = np.asarray(regval_P)\n",
    "# rval_P = np.mean(rval_P.reshape(N_REPS,int(len(rval_P)/N_REPS)).T,axis=1)\n",
    "# rval_P = rval_P.reshape(N_LAMBDA,int(len(rval_P)/N_LAMBDA)).T\n",
    "# # rval_C = np.asarray(regval_C)\n",
    "# # rval_C = np.mean(rval_C.reshape(N_REPS,int(len(rval_C)/N_REPS)).T,axis=1)\n",
    "# # rval_C = rval_C.reshape(N_LAMBDA,int(len(rval_C)/N_LAMBDA)).T\n",
    "# # rval_R = np.asarray(regval_R)\n",
    "# # rval_R = np.mean(rval_R.reshape(N_REPS,int(len(rval_R)/N_REPS)).T,axis=1)\n",
    "# # rval_R = rval_R.reshape(N_LAMBDA,int(len(rval_R)/N_LAMBDA)).T\n",
    "# plt.plot(lambdas,rval_P[-1,],label='diagonal')\n",
    "# # plt.plot(lambdas,rval_C[-1,],label='off-diagonal')\n",
    "# # plt.plot(lambdas,rval_R[-1,],label='random')\n",
    "# plt.title('L1 norm of regularized hidden-hidden weight')\n",
    "# plt.ylabel('L1 norm')\n",
    "# plt.xlabel('lambda')\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.plot(lambdas,np.mean(np.sum(np.abs(Phist_P[sidx:(sidx+ridx),-1,:,:].squeeze())<0.0025,axis=0),axis=1)/ridx,'-',label='diagonal')\n",
    "# # plt.plot(lambdas,np.mean(np.sum(np.abs(Phist_C[sidx:(sidx+ridx),-1,:,:].squeeze())<0.0025,axis=0),axis=1)/ridx,'-',label='off-diagonal')\n",
    "# # plt.plot(lambdas,np.mean(np.sum(np.abs(Phist_R[sidx:(sidx+ridx),-1,:,:].squeeze())<0.0025,axis=0),axis=1)/ridx,'-',label='random')\n",
    "# plt.plot(lambdas,0.5*np.ones_like(lambdas),'k--')\n",
    "# plt.ylim((0,.55))\n",
    "# plt.legend()\n",
    "# plt.title('Fraction of zero weight')\n",
    "# plt.xlabel('lambda')\n",
    "# plt.ylabel('Fraction')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(Phist_P[:,-1,-1,0],label='diagonal',normed=1, histtype='step',bins=np.arange(-0.5,0.5,0.005),log=True)\n",
    "# plt.hist(Phist_C[:,-1,-1,0],label='off-diagonal',normed=1, histtype='step',bins=np.arange(-0.5,0.5,0.005),log=True)\n",
    "# plt.hist(Phist_R[:,-1,-1,0],label='random',normed=1, histtype='step',bins=np.arange(-0.5,0.5,0.005),log=True)\n",
    "# plt.xlim((-.25,.25))\n",
    "# plt.xlabel('Wedight')\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(14,5))\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.plot(lambdas,1/(1+np.exp(-np.mean(Phist_P[0,-1,:,:],axis=1).squeeze())),'-',label='diagonal')\n",
    "# plt.plot(lambdas,1/(1+np.exp(-np.mean(Phist_C[0,-1,:,:],axis=1).squeeze())),'-',label='off-diagonal')\n",
    "# plt.plot(lambdas,1/(1+np.exp(-np.mean(Phist_R[0,-1,:,:],axis=1).squeeze())),'-',label='random')\n",
    "# plt.ylim((0.5,0.8))\n",
    "# plt.legend()\n",
    "# plt.title('Size of time step vs. regularization')\n",
    "# plt.ylabel('Time step size')\n",
    "# plt.xlabel('lambda')\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.plot(lambdas,np.mean(Phist_P[1,-1,:,:],axis=1).squeeze(),'-',label='diagonal')\n",
    "# plt.plot(lambdas,np.mean(Phist_C[1,-1,:,:],axis=1).squeeze(),'-',label='off-diagonal')\n",
    "# plt.plot(lambdas,np.mean(Phist_R[1,-1,:,:],axis=1).squeeze(),'-',label='random')\n",
    "# plt.ylim((2,3.5))\n",
    "# plt.legend()\n",
    "# plt.title('Amplification vs. Regularization')\n",
    "# plt.ylabel('Amplification')\n",
    "# plt.xlabel('lambda')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting by lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # number of lambdas\n",
    "# n_epochs_plot,n_lambdas_plot = np.mean(test_acc_P,2).shape\n",
    "\n",
    "# fig,ax = plt.subplots(1,2)\n",
    "# fig.subplots_adjust(hspace=20.0)\n",
    "\n",
    "# ax1,ax2 = ax\n",
    "# cm1 = plt.get_cmap('copper') # plt.get_cmap('gist_rainbow') # plt.get_cmap('gist_rainbow')\n",
    "# # fig = plt.figure()\n",
    "# NUM_COLORS = n_lambdas_plot\n",
    "# ax1.set_prop_cycle('color', [cm1(1.*i/NUM_COLORS) for i in range(NUM_COLORS)])\n",
    "# ax2.set_prop_cycle('color', [cm1(1.*i/NUM_COLORS) for i in range(NUM_COLORS)])\n",
    "\n",
    "# fig.suptitle(\"Epoch %d; reps %d; min/max lambda [%f,%f]\"\n",
    "#              %(epoch,\n",
    "#                N_REPS,\n",
    "#                min(lambdas),\n",
    "#                max(lambdas)))\n",
    "\n",
    "# # train accuracy %f; train loss %f; test accuracy %f; test loss %f\"\n",
    "# # train_acc_P[epoch,k,r],\n",
    "# #                train_loss_P[epoch,k,r],\n",
    "# #                test_acc_P[epoch,k,r],\n",
    "# #                test_loss_P[epoch,k,r]\n",
    "            \n",
    "# # ax = fig.add_subplot(111)\n",
    "# for i in enumerate(lambdas):\n",
    "#     ax1.plot(np.mean(test_acc_P,2)[:,i[0]],label=i[1],)\n",
    "#     ax1.set_xlabel(\"Epoch\")\n",
    "#     ax1.set_ylabel(\"Test Accuracy\")\n",
    "    \n",
    "# # ax2 = fig.add_subplot(011)\n",
    "# for i in enumerate(lambdas):\n",
    "#     ax2.plot(np.mean(test_loss_P,2)[:,i[0]],label=i[1],)\n",
    "#     ax2.set_xlabel(\"Epoch\")\n",
    "#     ax2.set_ylabel(\"Test Loss\")\n",
    "# ax2.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def readtxt(txt_name = 'anna.txt'):\n",
    "#     dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "#     txt_file = os.path.join(dir_path,txt_name)\n",
    "#     # load the whole book\n",
    "#     file = open(self.txt_file)\n",
    "#     alltxt = file.read()\n",
    "#     # remove newline formmating\n",
    "#     alltxt = alltxt.replace(\"\\n\\n\", \"&\").replace(\"\\n\", \" \").replace(\"&\", \"\\n\")\n",
    "#     # define categories\n",
    "#     categories = list(sorted(set(alltxt)))\n",
    "#     # integer encode\n",
    "#     label_encoder = LabelEncoder()\n",
    "#     label_encoder.fit(categories)\n",
    "#     integer_encoded = torch.LongTensor(label_encoder.transform(list(alltxt)))\n",
    "#     return integer_encoded, categories\n",
    "\n",
    "# # def onehotencode(integer_encoded_batch,n_cat):\n",
    "    \n",
    "# # def get_next_batch(dat,batch_size):\n",
    "# #     x_int = \n",
    "# #     y_int = \n",
    "# #     x_hot = onehotencode(x_int): \n",
    "# #     return x_hot, y_int \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(test_acc_P.shape)\n",
    "# plt.plot(np.mean(test_acc_P,2))\n",
    "# plt.plot()\n",
    "# print(np.mean(test_acc_P,1))\n",
    "# print(np.mean(test_acc_P,1).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
